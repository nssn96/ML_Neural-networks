{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_nn_new.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDXbL0p/B1my5Y6kqd24i3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nssn96/ML_Neural-networks/blob/main/ML_nn_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Assignment 2 - Neural Networks\n",
        "\n",
        "\n",
        "1.   Author : Surya Narayanan Nadhamuni Suresh\n",
        "2.   UTA ID : 1001877873\n"
      ],
      "metadata": {
        "id": "H4lgkRlpp0RT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "EFa5hY2BTlrS"
      },
      "outputs": [],
      "source": [
        "#import lines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#References Used\n",
        "#https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba\n",
        "#https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
        "#https://towardsdatascience.com/an-introduction-to-neural-networks-with-implementation-from-scratch-using-python-da4b6a45c05b\n",
        "\n",
        "# This is the Layer class\n",
        "\n",
        "class Layer:\n",
        "\n",
        "  def initial_w_bias(self,nn):\n",
        "    param = {}\n",
        "    #creating the values for the w and bias matrix\n",
        "    for i in range(1,len(nn)):\n",
        "      param['w' + str(i)] = np.random.randn(nn[i], nn[i-1])*0.01\n",
        "      param['b' + str(i)] = np.random.randn(nn[i],1)*0.01\n",
        "    return param\n"
      ],
      "metadata": {
        "id": "GPAB9jOSp8Bc"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the class for the hyperbolic tangent functions\n",
        "\n",
        "#class htangent_activation(Layer):\n",
        "\n",
        "def tanh_forwardprop(z):\n",
        "  #2/(1 + e-2x) - 1\n",
        "  return (2/(1+np.exp(-2*z)) -1)\n",
        "\n",
        "def tanh_backprop(dA,z):\n",
        "  value = tanh_forwardprop(z)\n",
        "  return dA * value * (1-value)\n"
      ],
      "metadata": {
        "id": "LPpEYhXdqvWx"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the class for the sigmoid function\n",
        "\n",
        "#class sigmoid_activation(Layer):\n",
        "\n",
        "def sigmoid_forwardprop(Z):\n",
        "  return 1/(1+np.exp(-Z))\n",
        "  \n",
        "def sigmoid_backprop(dA,z):\n",
        "  value = sigmoid_forwardprop(z)\n",
        "  return dA * value * (1-value)"
      ],
      "metadata": {
        "id": "xqdDZRlrqx0c"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the class for the softmax function\n",
        "\n",
        "class softmax_activation(Layer):\n",
        "\n",
        "  def softmax_forwardprop(self,z):\n",
        "    return np.exp(z) / sum(np.exp(z))\n",
        "  \n",
        "\n",
        "  #derivative of softmax\n",
        "  def softmax_backprop(self,z):\n",
        "    exp=np.exp(z-z.max())\n",
        "    return exp/np.sum(exp,axis=0)*(1-exp/np.sum(exp,axis=0))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  #Yet to edit this--- DONT FORGET+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "  # def softmax_backprop(probs, bp_err):\n",
        "  #   dim = probs.shape[1]\n",
        "  #   output = np.empty(probs.shape)\n",
        "  #   for j in range(dim):\n",
        "  #       d_prob_over_xj = - (probs * probs[:,[j]])  # i.e. prob_k * prob_j, no matter k==j or not\n",
        "  #       d_prob_over_xj[:,j] += probs[:,j]   # i.e. when k==j, +prob_j\n",
        "  #       output[:,j] = np.sum(bp_err * d_prob_over_xj, axis=1)\n",
        "  #   return output\n",
        "  "
      ],
      "metadata": {
        "id": "EcSk6q_iq0LT"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the Linear layer class\n",
        "\n",
        "class LinearLayer(Layer):\n",
        "\n",
        "\n",
        "  def forward(self,data,param):\n",
        "    \n",
        "    #Z_mat = np.dot(w,prev_out)\n",
        "    \n",
        "    layer_num = len(param)//2\n",
        "    backup = {} # to store info required for the backward calculation\n",
        "\n",
        "    #traversing over the layers in the neural network\n",
        "    #this is the same steps that we used to assign the weights and bias for network\n",
        "    for i in range(1,layer_num+1):\n",
        "      #if it is the first iteration the input will be the input ie the train or the test data\n",
        "      w = param['w'+str(i)]\n",
        "      bias = param['b'+str(i)]\n",
        "      if i==1:\n",
        "        #Z_mat--> the input matrix calculation which will be passed in the activation function\n",
        "        backup['Z' + str(i)] = np.dot(w, data) + bias\n",
        "        backup['A' + str(i)] = tanh_forwardprop(backup['Z' + str(i)])\n",
        "      else:\n",
        "        backup['Z' + str(i)] = np.dot(w, backup['A' + str(i-1)]) + bias\n",
        "        if i==layer_num:\n",
        "          backup['A' + str(i)] = backup['Z' + str(i)]\n",
        "        else:\n",
        "          backup['A' + str(i)] = tanh_forwardprop(backup['Z' + str(i)])\n",
        "\n",
        "      #storing the values for backward pass call\n",
        "      #backup['Z' + str(i)] = Z_mat\n",
        "\n",
        "    \n",
        "    # for i in backup.keys():\n",
        "    #   print(i)\n",
        "    return backup\n",
        "  \n",
        "  def compute_loss(self,backup,target):\n",
        "    layer_num = len(backup)//2\n",
        "    y_pred = backup['A' + str(layer_num)]\n",
        "    loss = cost = 1/(2*len(target)) * np.sum(np.square(y_pred - target))\n",
        "    return loss\n",
        "  \n",
        "  def backward(self,data,target,param,backup):\n",
        "    deriv_values= {}  #dictionary to store the derivative values\n",
        "    layer_num = len(param)//2\n",
        "    n = len(target)\n",
        "     #since we need to go backward in backward propagation\n",
        "    for i in range(layer_num,0,-1):\n",
        "\n",
        "      # print(i)\n",
        "      # print(backup['A'+str(i)])\n",
        "      A_previous = backup['A'+str(i)]\n",
        "      z_current = backup['Z'+str(i)]\n",
        "      w = param['w'+str(i)]\n",
        "      bias = param['b'+str(i)]\n",
        "\n",
        "      if i == layer_num:\n",
        "        dA_current = 1/n * (A_previous - target)\n",
        "        dz = dA_current\n",
        "      else:\n",
        "        dA_current = np.dot(param['w' + str(i+1)].T, dz)\n",
        "        dz = np.multiply(dA_current, np.where(A_previous>=0, 1, 0))\n",
        "      if i==1:\n",
        "        #dw = 1/n * np.dot(dz, data.T)\n",
        "        dw = 1/n * np.dot(dz, data)\n",
        "        dbias = 1/n * np.sum(dz, axis=1, keepdims=True)\n",
        "      else:\n",
        "        dw = 1/n * np.dot(dz,backup['A' + str(i-1)].T)\n",
        "        dbias = 1/n * np.sum(dz, axis=1, keepdims=True)\n",
        "      \n",
        "      deriv_values['dw' + str(i)] = dw\n",
        "      deriv_values['db' + str(i)] = dbias\n",
        "\n",
        "    # for i in deriv_values.keys():\n",
        "    #   print(i)\n",
        "    return deriv_values\n",
        "  \n",
        "  def update_param(self,param,deriv_values,alpha):\n",
        "    layer_num = len(param)//2\n",
        "    updated_param = {}\n",
        "    for i in range(1,layer_num+1):\n",
        "      updated_param['w'+str(i)] = param['w' + str(i)] - alpha * deriv_values['dw' + str(i)]\n",
        "      updated_param['b'+str(i)] = param['b' + str(i)] - alpha * deriv_values['db' + str(i)]\n",
        "    return updated_param\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pi04VShKrvxh"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the class for entropy loss\n",
        "\n",
        "class entropy_loss(LinearLayer):\n",
        "\n",
        "  def cross_entropy_forward(self,y_pred,y):\n",
        "    return -y * np.log(y_pred)\n",
        "  \n",
        "  def cross_entropy_backward(self,y_pred,y):\n",
        "    return y-y_pred\n"
      ],
      "metadata": {
        "id": "aq7aAweb3hMI"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This part of the code is for sequential class\n",
        "\n",
        "class Sequential(LinearLayer):\n",
        "\n",
        "  def train(self,data,target,nn,steps,alpha):\n",
        "    param = self.initial_w_bias(nn)\n",
        "    for i in range(steps):\n",
        "      backup = self.forward(data.T, param)\n",
        "      loss = self.compute_loss(backup,target.T)\n",
        "      deriv_values = self.backward(data,target,param,backup)\n",
        "      param = self.update_param(param,deriv_values,alpha)\n",
        "      print('Iteration ' + str(i+1) + ' , Loss = ' + str(loss) + '\\n')\n",
        "    return param\n",
        "  \n",
        "  def accuracy_calc(self,x_train,x_test,y_train,y_test,param,nn):\n",
        "    backup_train = self.forward(x_train.T,param)\n",
        "    backup_test = self.forward(x_test.T,param)\n",
        "    accuracy_train = np.sqrt(mean_squared_error(y_train, backup_train['A' + str(len(nn)-1)].T))\n",
        "    test_acc = np.sqrt(mean_squared_error(y_test, backup_test['A' + str(len(nn)-1)].T))\n",
        "  \n",
        "  def predict(self,data,param):\n",
        "    backup = self.forward(data.T,param)\n",
        "    pred = backup['A' + str(len(backup)//2)].T\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "huOAeZ3D3mvU"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[0, 0],\n",
        "                    [0, 1],\n",
        "                    [1, 0],\n",
        "                    [1, 1]])\n",
        "target = np.array([0, 1, 1, 0])"
      ],
      "metadata": {
        "id": "mzdPGK8RE5p-"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "# target = np.array([[[0]], [[1]], [[1]], [[0]]])"
      ],
      "metadata": {
        "id": "4tFc2s37Sdz2"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# net = Sequential()\n",
        "# data = load_boston()                                                              #load dataset\n",
        "# X,Y = data[\"data\"], data[\"target\"]                                               #separate data into input and output features\n",
        "# X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size = 0.2)           #split data into train and test sets in 80-20 ratio\n",
        "# layer_sizes = [13, 5, 5, 1]                                                       #set layer sizes, do not change the size of the first and last layer \n",
        "# num_iters = 1000                                                                  #set number of iterations over the training set(also known as epochs in batch gradient descent context)\n",
        "# learning_rate = 0.03                                                              #set learning rate for gradient descent\n",
        "# params = net.train(X_train, Y_train, layer_sizes, num_iters, learning_rate)           #train the model\n",
        "# train_acc, test_acc = net.accuracy_calc(X_train, X_test, Y_train, Y_test, params)  #get training and test accuracy\n",
        "# print('Root Mean Squared Error on Training Data = ' + str(train_acc))\n",
        "# print('Root Mean Squared Error on Test Data = ' + str(test_acc))"
      ],
      "metadata": {
        "id": "CC9Sv_PaGHYb"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training part\n",
        "net = Sequential()\n",
        "layers = [2,2,2,1]                                                        \n",
        "steps = 5000                                                                 \n",
        "alpha = 0.01                                                        \n",
        "params = net.train(data, target, layers, steps, alpha)          \n",
        "# train_acc, test_acc = compute_accuracy(X_train, X_test, Y_train, Y_test, params)  \n",
        "# print('Root Mean Squared Error on Training Data = ' + str(train_acc))\n",
        "# print('Root Mean Squared Error on Test Data = ' + str(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzR2ft6gEXp9",
        "outputId": "9f496ca8-43ae-4204-939b-05ea29dde958"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 2501 , Loss = 0.12500047846593576\n",
            "\n",
            "Iteration 2502 , Loss = 0.1250004760744909\n",
            "\n",
            "Iteration 2503 , Loss = 0.12500047369499928\n",
            "\n",
            "Iteration 2504 , Loss = 0.12500047132740105\n",
            "\n",
            "Iteration 2505 , Loss = 0.12500046897163686\n",
            "\n",
            "Iteration 2506 , Loss = 0.12500046662764755\n",
            "\n",
            "Iteration 2507 , Loss = 0.1250004642953742\n",
            "\n",
            "Iteration 2508 , Loss = 0.12500046197475834\n",
            "\n",
            "Iteration 2509 , Loss = 0.12500045966574164\n",
            "\n",
            "Iteration 2510 , Loss = 0.12500045736826612\n",
            "\n",
            "Iteration 2511 , Loss = 0.12500045508227414\n",
            "\n",
            "Iteration 2512 , Loss = 0.12500045280770825\n",
            "\n",
            "Iteration 2513 , Loss = 0.1250004505445114\n",
            "\n",
            "Iteration 2514 , Loss = 0.1250004482926267\n",
            "\n",
            "Iteration 2515 , Loss = 0.12500044605199762\n",
            "\n",
            "Iteration 2516 , Loss = 0.12500044382256797\n",
            "\n",
            "Iteration 2517 , Loss = 0.12500044160428167\n",
            "\n",
            "Iteration 2518 , Loss = 0.1250004393970831\n",
            "\n",
            "Iteration 2519 , Loss = 0.12500043720091683\n",
            "\n",
            "Iteration 2520 , Loss = 0.1250004350157277\n",
            "\n",
            "Iteration 2521 , Loss = 0.1250004328414608\n",
            "\n",
            "Iteration 2522 , Loss = 0.12500043067806163\n",
            "\n",
            "Iteration 2523 , Loss = 0.1250004285254758\n",
            "\n",
            "Iteration 2524 , Loss = 0.12500042638364928\n",
            "\n",
            "Iteration 2525 , Loss = 0.12500042425252827\n",
            "\n",
            "Iteration 2526 , Loss = 0.1250004221320593\n",
            "\n",
            "Iteration 2527 , Loss = 0.12500042002218914\n",
            "\n",
            "Iteration 2528 , Loss = 0.12500041792286476\n",
            "\n",
            "Iteration 2529 , Loss = 0.12500041583403348\n",
            "\n",
            "Iteration 2530 , Loss = 0.12500041375564283\n",
            "\n",
            "Iteration 2531 , Loss = 0.1250004116876407\n",
            "\n",
            "Iteration 2532 , Loss = 0.12500040962997505\n",
            "\n",
            "Iteration 2533 , Loss = 0.1250004075825943\n",
            "\n",
            "Iteration 2534 , Loss = 0.12500040554544695\n",
            "\n",
            "Iteration 2535 , Loss = 0.12500040351848202\n",
            "\n",
            "Iteration 2536 , Loss = 0.12500040150164846\n",
            "\n",
            "Iteration 2537 , Loss = 0.1250003994948957\n",
            "\n",
            "Iteration 2538 , Loss = 0.1250003974981733\n",
            "\n",
            "Iteration 2539 , Loss = 0.12500039551143116\n",
            "\n",
            "Iteration 2540 , Loss = 0.12500039353461942\n",
            "\n",
            "Iteration 2541 , Loss = 0.1250003915676884\n",
            "\n",
            "Iteration 2542 , Loss = 0.12500038961058874\n",
            "\n",
            "Iteration 2543 , Loss = 0.1250003876632713\n",
            "\n",
            "Iteration 2544 , Loss = 0.1250003857256872\n",
            "\n",
            "Iteration 2545 , Loss = 0.12500038379778775\n",
            "\n",
            "Iteration 2546 , Loss = 0.12500038187952453\n",
            "\n",
            "Iteration 2547 , Loss = 0.12500037997084942\n",
            "\n",
            "Iteration 2548 , Loss = 0.12500037807171452\n",
            "\n",
            "Iteration 2549 , Loss = 0.12500037618207208\n",
            "\n",
            "Iteration 2550 , Loss = 0.12500037430187466\n",
            "\n",
            "Iteration 2551 , Loss = 0.1250003724310751\n",
            "\n",
            "Iteration 2552 , Loss = 0.1250003705696264\n",
            "\n",
            "Iteration 2553 , Loss = 0.12500036871748182\n",
            "\n",
            "Iteration 2554 , Loss = 0.12500036687459484\n",
            "\n",
            "Iteration 2555 , Loss = 0.12500036504091921\n",
            "\n",
            "Iteration 2556 , Loss = 0.1250003632164089\n",
            "\n",
            "Iteration 2557 , Loss = 0.12500036140101806\n",
            "\n",
            "Iteration 2558 , Loss = 0.12500035959470113\n",
            "\n",
            "Iteration 2559 , Loss = 0.12500035779741275\n",
            "\n",
            "Iteration 2560 , Loss = 0.12500035600910783\n",
            "\n",
            "Iteration 2561 , Loss = 0.1250003542297414\n",
            "\n",
            "Iteration 2562 , Loss = 0.12500035245926883\n",
            "\n",
            "Iteration 2563 , Loss = 0.12500035069764565\n",
            "\n",
            "Iteration 2564 , Loss = 0.12500034894482764\n",
            "\n",
            "Iteration 2565 , Loss = 0.1250003472007708\n",
            "\n",
            "Iteration 2566 , Loss = 0.12500034546543126\n",
            "\n",
            "Iteration 2567 , Loss = 0.1250003437387656\n",
            "\n",
            "Iteration 2568 , Loss = 0.12500034202073032\n",
            "\n",
            "Iteration 2569 , Loss = 0.12500034031128232\n",
            "\n",
            "Iteration 2570 , Loss = 0.12500033861037868\n",
            "\n",
            "Iteration 2571 , Loss = 0.12500033691797677\n",
            "\n",
            "Iteration 2572 , Loss = 0.125000335234034\n",
            "\n",
            "Iteration 2573 , Loss = 0.12500033355850812\n",
            "\n",
            "Iteration 2574 , Loss = 0.12500033189135706\n",
            "\n",
            "Iteration 2575 , Loss = 0.12500033023253898\n",
            "\n",
            "Iteration 2576 , Loss = 0.1250003285820122\n",
            "\n",
            "Iteration 2577 , Loss = 0.12500032693973528\n",
            "\n",
            "Iteration 2578 , Loss = 0.125000325305667\n",
            "\n",
            "Iteration 2579 , Loss = 0.12500032367976632\n",
            "\n",
            "Iteration 2580 , Loss = 0.12500032206199244\n",
            "\n",
            "Iteration 2581 , Loss = 0.1250003204523047\n",
            "\n",
            "Iteration 2582 , Loss = 0.1250003188506627\n",
            "\n",
            "Iteration 2583 , Loss = 0.1250003172570262\n",
            "\n",
            "Iteration 2584 , Loss = 0.12500031567135528\n",
            "\n",
            "Iteration 2585 , Loss = 0.12500031409361004\n",
            "\n",
            "Iteration 2586 , Loss = 0.12500031252375082\n",
            "\n",
            "Iteration 2587 , Loss = 0.12500031096173836\n",
            "\n",
            "Iteration 2588 , Loss = 0.1250003094075333\n",
            "\n",
            "Iteration 2589 , Loss = 0.12500030786109664\n",
            "\n",
            "Iteration 2590 , Loss = 0.12500030632238962\n",
            "\n",
            "Iteration 2591 , Loss = 0.12500030479137353\n",
            "\n",
            "Iteration 2592 , Loss = 0.12500030326800995\n",
            "\n",
            "Iteration 2593 , Loss = 0.12500030175226065\n",
            "\n",
            "Iteration 2594 , Loss = 0.1250003002440876\n",
            "\n",
            "Iteration 2595 , Loss = 0.12500029874345284\n",
            "\n",
            "Iteration 2596 , Loss = 0.12500029725031878\n",
            "\n",
            "Iteration 2597 , Loss = 0.12500029576464783\n",
            "\n",
            "Iteration 2598 , Loss = 0.1250002942864028\n",
            "\n",
            "Iteration 2599 , Loss = 0.1250002928155465\n",
            "\n",
            "Iteration 2600 , Loss = 0.12500029135204202\n",
            "\n",
            "Iteration 2601 , Loss = 0.12500028989585266\n",
            "\n",
            "Iteration 2602 , Loss = 0.12500028844694175\n",
            "\n",
            "Iteration 2603 , Loss = 0.12500028700527296\n",
            "\n",
            "Iteration 2604 , Loss = 0.1250002855708102\n",
            "\n",
            "Iteration 2605 , Loss = 0.12500028414351724\n",
            "\n",
            "Iteration 2606 , Loss = 0.1250002827233584\n",
            "\n",
            "Iteration 2607 , Loss = 0.125000281310298\n",
            "\n",
            "Iteration 2608 , Loss = 0.12500027990430052\n",
            "\n",
            "Iteration 2609 , Loss = 0.1250002785053307\n",
            "\n",
            "Iteration 2610 , Loss = 0.12500027711335338\n",
            "\n",
            "Iteration 2611 , Loss = 0.12500027572833358\n",
            "\n",
            "Iteration 2612 , Loss = 0.12500027435023658\n",
            "\n",
            "Iteration 2613 , Loss = 0.12500027297902777\n",
            "\n",
            "Iteration 2614 , Loss = 0.12500027161467273\n",
            "\n",
            "Iteration 2615 , Loss = 0.12500027025713717\n",
            "\n",
            "Iteration 2616 , Loss = 0.125000268906387\n",
            "\n",
            "Iteration 2617 , Loss = 0.12500026756238836\n",
            "\n",
            "Iteration 2618 , Loss = 0.12500026622510743\n",
            "\n",
            "Iteration 2619 , Loss = 0.1250002648945107\n",
            "\n",
            "Iteration 2620 , Loss = 0.1250002635705647\n",
            "\n",
            "Iteration 2621 , Loss = 0.12500026225323624\n",
            "\n",
            "Iteration 2622 , Loss = 0.12500026094249223\n",
            "\n",
            "Iteration 2623 , Loss = 0.12500025963829972\n",
            "\n",
            "Iteration 2624 , Loss = 0.125000258340626\n",
            "\n",
            "Iteration 2625 , Loss = 0.1250002570494385\n",
            "\n",
            "Iteration 2626 , Loss = 0.12500025576470475\n",
            "\n",
            "Iteration 2627 , Loss = 0.12500025448639254\n",
            "\n",
            "Iteration 2628 , Loss = 0.12500025321446975\n",
            "\n",
            "Iteration 2629 , Loss = 0.12500025194890446\n",
            "\n",
            "Iteration 2630 , Loss = 0.1250002506896649\n",
            "\n",
            "Iteration 2631 , Loss = 0.1250002494367194\n",
            "\n",
            "Iteration 2632 , Loss = 0.12500024819003655\n",
            "\n",
            "Iteration 2633 , Loss = 0.12500024694958503\n",
            "\n",
            "Iteration 2634 , Loss = 0.1250002457153337\n",
            "\n",
            "Iteration 2635 , Loss = 0.12500024448725156\n",
            "\n",
            "Iteration 2636 , Loss = 0.12500024326530776\n",
            "\n",
            "Iteration 2637 , Loss = 0.12500024204947166\n",
            "\n",
            "Iteration 2638 , Loss = 0.12500024083971273\n",
            "\n",
            "Iteration 2639 , Loss = 0.12500023963600054\n",
            "\n",
            "Iteration 2640 , Loss = 0.1250002384383049\n",
            "\n",
            "Iteration 2641 , Loss = 0.12500023724659579\n",
            "\n",
            "Iteration 2642 , Loss = 0.1250002360608432\n",
            "\n",
            "Iteration 2643 , Loss = 0.1250002348810174\n",
            "\n",
            "Iteration 2644 , Loss = 0.12500023370708876\n",
            "\n",
            "Iteration 2645 , Loss = 0.12500023253902778\n",
            "\n",
            "Iteration 2646 , Loss = 0.12500023137680522\n",
            "\n",
            "Iteration 2647 , Loss = 0.12500023022039178\n",
            "\n",
            "Iteration 2648 , Loss = 0.12500022906975847\n",
            "\n",
            "Iteration 2649 , Loss = 0.12500022792487644\n",
            "\n",
            "Iteration 2650 , Loss = 0.1250002267857169\n",
            "\n",
            "Iteration 2651 , Loss = 0.12500022565225122\n",
            "\n",
            "Iteration 2652 , Loss = 0.12500022452445106\n",
            "\n",
            "Iteration 2653 , Loss = 0.12500022340228795\n",
            "\n",
            "Iteration 2654 , Loss = 0.12500022228573382\n",
            "\n",
            "Iteration 2655 , Loss = 0.12500022117476056\n",
            "\n",
            "Iteration 2656 , Loss = 0.12500022006934033\n",
            "\n",
            "Iteration 2657 , Loss = 0.1250002189694454\n",
            "\n",
            "Iteration 2658 , Loss = 0.12500021787504806\n",
            "\n",
            "Iteration 2659 , Loss = 0.12500021678612092\n",
            "\n",
            "Iteration 2660 , Loss = 0.12500021570263656\n",
            "\n",
            "Iteration 2661 , Loss = 0.12500021462456784\n",
            "\n",
            "Iteration 2662 , Loss = 0.12500021355188767\n",
            "\n",
            "Iteration 2663 , Loss = 0.1250002124845691\n",
            "\n",
            "Iteration 2664 , Loss = 0.12500021142258538\n",
            "\n",
            "Iteration 2665 , Loss = 0.12500021036590975\n",
            "\n",
            "Iteration 2666 , Loss = 0.12500020931451578\n",
            "\n",
            "Iteration 2667 , Loss = 0.12500020826837702\n",
            "\n",
            "Iteration 2668 , Loss = 0.12500020722746719\n",
            "\n",
            "Iteration 2669 , Loss = 0.12500020619176022\n",
            "\n",
            "Iteration 2670 , Loss = 0.12500020516122998\n",
            "\n",
            "Iteration 2671 , Loss = 0.12500020413585075\n",
            "\n",
            "Iteration 2672 , Loss = 0.12500020311559668\n",
            "\n",
            "Iteration 2673 , Loss = 0.12500020210044216\n",
            "\n",
            "Iteration 2674 , Loss = 0.12500020109036175\n",
            "\n",
            "Iteration 2675 , Loss = 0.12500020008533008\n",
            "\n",
            "Iteration 2676 , Loss = 0.12500019908532187\n",
            "\n",
            "Iteration 2677 , Loss = 0.12500019809031204\n",
            "\n",
            "Iteration 2678 , Loss = 0.1250001971002756\n",
            "\n",
            "Iteration 2679 , Loss = 0.12500019611518767\n",
            "\n",
            "Iteration 2680 , Loss = 0.12500019513502356\n",
            "\n",
            "Iteration 2681 , Loss = 0.12500019415975866\n",
            "\n",
            "Iteration 2682 , Loss = 0.12500019318936845\n",
            "\n",
            "Iteration 2683 , Loss = 0.12500019222382855\n",
            "\n",
            "Iteration 2684 , Loss = 0.12500019126311476\n",
            "\n",
            "Iteration 2685 , Loss = 0.12500019030720297\n",
            "\n",
            "Iteration 2686 , Loss = 0.1250001893560691\n",
            "\n",
            "Iteration 2687 , Loss = 0.12500018840968932\n",
            "\n",
            "Iteration 2688 , Loss = 0.1250001874680399\n",
            "\n",
            "Iteration 2689 , Loss = 0.12500018653109712\n",
            "\n",
            "Iteration 2690 , Loss = 0.12500018559883752\n",
            "\n",
            "Iteration 2691 , Loss = 0.12500018467123766\n",
            "\n",
            "Iteration 2692 , Loss = 0.1250001837482743\n",
            "\n",
            "Iteration 2693 , Loss = 0.12500018282992414\n",
            "\n",
            "Iteration 2694 , Loss = 0.12500018191616424\n",
            "\n",
            "Iteration 2695 , Loss = 0.12500018100697166\n",
            "\n",
            "Iteration 2696 , Loss = 0.12500018010232347\n",
            "\n",
            "Iteration 2697 , Loss = 0.12500017920219703\n",
            "\n",
            "Iteration 2698 , Loss = 0.12500017830656973\n",
            "\n",
            "Iteration 2699 , Loss = 0.12500017741541913\n",
            "\n",
            "Iteration 2700 , Loss = 0.12500017652872272\n",
            "\n",
            "Iteration 2701 , Loss = 0.12500017564645835\n",
            "\n",
            "Iteration 2702 , Loss = 0.12500017476860387\n",
            "\n",
            "Iteration 2703 , Loss = 0.12500017389513715\n",
            "\n",
            "Iteration 2704 , Loss = 0.12500017302603633\n",
            "\n",
            "Iteration 2705 , Loss = 0.12500017216127962\n",
            "\n",
            "Iteration 2706 , Loss = 0.1250001713008452\n",
            "\n",
            "Iteration 2707 , Loss = 0.1250001704447115\n",
            "\n",
            "Iteration 2708 , Loss = 0.1250001695928571\n",
            "\n",
            "Iteration 2709 , Loss = 0.12500016874526054\n",
            "\n",
            "Iteration 2710 , Loss = 0.12500016790190055\n",
            "\n",
            "Iteration 2711 , Loss = 0.12500016706275593\n",
            "\n",
            "Iteration 2712 , Loss = 0.1250001662278057\n",
            "\n",
            "Iteration 2713 , Loss = 0.12500016539702882\n",
            "\n",
            "Iteration 2714 , Loss = 0.12500016457040442\n",
            "\n",
            "Iteration 2715 , Loss = 0.12500016374791176\n",
            "\n",
            "Iteration 2716 , Loss = 0.1250001629295302\n",
            "\n",
            "Iteration 2717 , Loss = 0.12500016211523923\n",
            "\n",
            "Iteration 2718 , Loss = 0.12500016130501834\n",
            "\n",
            "Iteration 2719 , Loss = 0.1250001604988472\n",
            "\n",
            "Iteration 2720 , Loss = 0.1250001596967056\n",
            "\n",
            "Iteration 2721 , Loss = 0.12500015889857335\n",
            "\n",
            "Iteration 2722 , Loss = 0.12500015810443044\n",
            "\n",
            "Iteration 2723 , Loss = 0.12500015731425695\n",
            "\n",
            "Iteration 2724 , Loss = 0.12500015652803298\n",
            "\n",
            "Iteration 2725 , Loss = 0.1250001557457389\n",
            "\n",
            "Iteration 2726 , Loss = 0.12500015496735492\n",
            "\n",
            "Iteration 2727 , Loss = 0.12500015419286156\n",
            "\n",
            "Iteration 2728 , Loss = 0.12500015342223944\n",
            "\n",
            "Iteration 2729 , Loss = 0.1250001526554691\n",
            "\n",
            "Iteration 2730 , Loss = 0.12500015189253136\n",
            "\n",
            "Iteration 2731 , Loss = 0.12500015113340707\n",
            "\n",
            "Iteration 2732 , Loss = 0.1250001503780771\n",
            "\n",
            "Iteration 2733 , Loss = 0.12500014962652256\n",
            "\n",
            "Iteration 2734 , Loss = 0.12500014887872452\n",
            "\n",
            "Iteration 2735 , Loss = 0.12500014813466426\n",
            "\n",
            "Iteration 2736 , Loss = 0.12500014739432305\n",
            "\n",
            "Iteration 2737 , Loss = 0.1250001466576823\n",
            "\n",
            "Iteration 2738 , Loss = 0.12500014592472355\n",
            "\n",
            "Iteration 2739 , Loss = 0.12500014519542835\n",
            "\n",
            "Iteration 2740 , Loss = 0.12500014446977847\n",
            "\n",
            "Iteration 2741 , Loss = 0.12500014374775564\n",
            "\n",
            "Iteration 2742 , Loss = 0.12500014302934168\n",
            "\n",
            "Iteration 2743 , Loss = 0.12500014231451864\n",
            "\n",
            "Iteration 2744 , Loss = 0.1250001416032685\n",
            "\n",
            "Iteration 2745 , Loss = 0.12500014089557346\n",
            "\n",
            "Iteration 2746 , Loss = 0.1250001401914157\n",
            "\n",
            "Iteration 2747 , Loss = 0.12500013949077754\n",
            "\n",
            "Iteration 2748 , Loss = 0.12500013879364147\n",
            "\n",
            "Iteration 2749 , Loss = 0.1250001380999899\n",
            "\n",
            "Iteration 2750 , Loss = 0.12500013740980542\n",
            "\n",
            "Iteration 2751 , Loss = 0.12500013672307073\n",
            "\n",
            "Iteration 2752 , Loss = 0.12500013603976856\n",
            "\n",
            "Iteration 2753 , Loss = 0.1250001353598818\n",
            "\n",
            "Iteration 2754 , Loss = 0.12500013468339335\n",
            "\n",
            "Iteration 2755 , Loss = 0.12500013401028623\n",
            "\n",
            "Iteration 2756 , Loss = 0.1250001333405435\n",
            "\n",
            "Iteration 2757 , Loss = 0.1250001326741484\n",
            "\n",
            "Iteration 2758 , Loss = 0.12500013201108415\n",
            "\n",
            "Iteration 2759 , Loss = 0.12500013135133412\n",
            "\n",
            "Iteration 2760 , Loss = 0.12500013069488178\n",
            "\n",
            "Iteration 2761 , Loss = 0.1250001300417106\n",
            "\n",
            "Iteration 2762 , Loss = 0.1250001293918042\n",
            "\n",
            "Iteration 2763 , Loss = 0.12500012874514624\n",
            "\n",
            "Iteration 2764 , Loss = 0.1250001281017205\n",
            "\n",
            "Iteration 2765 , Loss = 0.12500012746151085\n",
            "\n",
            "Iteration 2766 , Loss = 0.12500012682450118\n",
            "\n",
            "Iteration 2767 , Loss = 0.12500012619067546\n",
            "\n",
            "Iteration 2768 , Loss = 0.12500012556001788\n",
            "\n",
            "Iteration 2769 , Loss = 0.12500012493251253\n",
            "\n",
            "Iteration 2770 , Loss = 0.12500012430814367\n",
            "\n",
            "Iteration 2771 , Loss = 0.12500012368689561\n",
            "\n",
            "Iteration 2772 , Loss = 0.12500012306875274\n",
            "\n",
            "Iteration 2773 , Loss = 0.12500012245369962\n",
            "\n",
            "Iteration 2774 , Loss = 0.1250001218417207\n",
            "\n",
            "Iteration 2775 , Loss = 0.1250001212328007\n",
            "\n",
            "Iteration 2776 , Loss = 0.1250001206269243\n",
            "\n",
            "Iteration 2777 , Loss = 0.12500012002407626\n",
            "\n",
            "Iteration 2778 , Loss = 0.12500011942424147\n",
            "\n",
            "Iteration 2779 , Loss = 0.12500011882740486\n",
            "\n",
            "Iteration 2780 , Loss = 0.12500011823355145\n",
            "\n",
            "Iteration 2781 , Loss = 0.12500011764266633\n",
            "\n",
            "Iteration 2782 , Loss = 0.12500011705473463\n",
            "\n",
            "Iteration 2783 , Loss = 0.12500011646974163\n",
            "\n",
            "Iteration 2784 , Loss = 0.12500011588767262\n",
            "\n",
            "Iteration 2785 , Loss = 0.12500011530851302\n",
            "\n",
            "Iteration 2786 , Loss = 0.12500011473224826\n",
            "\n",
            "Iteration 2787 , Loss = 0.1250001141588639\n",
            "\n",
            "Iteration 2788 , Loss = 0.12500011358834542\n",
            "\n",
            "Iteration 2789 , Loss = 0.12500011302067868\n",
            "\n",
            "Iteration 2790 , Loss = 0.1250001124558493\n",
            "\n",
            "Iteration 2791 , Loss = 0.12500011189384316\n",
            "\n",
            "Iteration 2792 , Loss = 0.12500011133464609\n",
            "\n",
            "Iteration 2793 , Loss = 0.1250001107782441\n",
            "\n",
            "Iteration 2794 , Loss = 0.1250001102246232\n",
            "\n",
            "Iteration 2795 , Loss = 0.1250001096737695\n",
            "\n",
            "Iteration 2796 , Loss = 0.12500010912566917\n",
            "\n",
            "Iteration 2797 , Loss = 0.12500010858030838\n",
            "\n",
            "Iteration 2798 , Loss = 0.12500010803767356\n",
            "\n",
            "Iteration 2799 , Loss = 0.12500010749775098\n",
            "\n",
            "Iteration 2800 , Loss = 0.12500010696052716\n",
            "\n",
            "Iteration 2801 , Loss = 0.12500010642598855\n",
            "\n",
            "Iteration 2802 , Loss = 0.12500010589412172\n",
            "\n",
            "Iteration 2803 , Loss = 0.1250001053649134\n",
            "\n",
            "Iteration 2804 , Loss = 0.12500010483835022\n",
            "\n",
            "Iteration 2805 , Loss = 0.125000104314419\n",
            "\n",
            "Iteration 2806 , Loss = 0.12500010379310655\n",
            "\n",
            "Iteration 2807 , Loss = 0.1250001032743998\n",
            "\n",
            "Iteration 2808 , Loss = 0.1250001027582858\n",
            "\n",
            "Iteration 2809 , Loss = 0.1250001022447515\n",
            "\n",
            "Iteration 2810 , Loss = 0.12500010173378395\n",
            "\n",
            "Iteration 2811 , Loss = 0.12500010122537047\n",
            "\n",
            "Iteration 2812 , Loss = 0.12500010071949816\n",
            "\n",
            "Iteration 2813 , Loss = 0.12500010021615443\n",
            "\n",
            "Iteration 2814 , Loss = 0.12500009971532658\n",
            "\n",
            "Iteration 2815 , Loss = 0.12500009921700203\n",
            "\n",
            "Iteration 2816 , Loss = 0.1250000987211683\n",
            "\n",
            "Iteration 2817 , Loss = 0.12500009822781288\n",
            "\n",
            "Iteration 2818 , Loss = 0.12500009773692347\n",
            "\n",
            "Iteration 2819 , Loss = 0.12500009724848768\n",
            "\n",
            "Iteration 2820 , Loss = 0.12500009676249327\n",
            "\n",
            "Iteration 2821 , Loss = 0.12500009627892802\n",
            "\n",
            "Iteration 2822 , Loss = 0.12500009579777982\n",
            "\n",
            "Iteration 2823 , Loss = 0.12500009531903658\n",
            "\n",
            "Iteration 2824 , Loss = 0.12500009484268623\n",
            "\n",
            "Iteration 2825 , Loss = 0.12500009436871687\n",
            "\n",
            "Iteration 2826 , Loss = 0.12500009389711658\n",
            "\n",
            "Iteration 2827 , Loss = 0.12500009342787352\n",
            "\n",
            "Iteration 2828 , Loss = 0.1250000929609759\n",
            "\n",
            "Iteration 2829 , Loss = 0.12500009249641197\n",
            "\n",
            "Iteration 2830 , Loss = 0.12500009203417015\n",
            "\n",
            "Iteration 2831 , Loss = 0.12500009157423875\n",
            "\n",
            "Iteration 2832 , Loss = 0.12500009111660623\n",
            "\n",
            "Iteration 2833 , Loss = 0.12500009066126117\n",
            "\n",
            "Iteration 2834 , Loss = 0.12500009020819203\n",
            "\n",
            "Iteration 2835 , Loss = 0.12500008975738752\n",
            "\n",
            "Iteration 2836 , Loss = 0.1250000893088363\n",
            "\n",
            "Iteration 2837 , Loss = 0.1250000888625271\n",
            "\n",
            "Iteration 2838 , Loss = 0.12500008841844867\n",
            "\n",
            "Iteration 2839 , Loss = 0.12500008797658999\n",
            "\n",
            "Iteration 2840 , Loss = 0.12500008753693978\n",
            "\n",
            "Iteration 2841 , Loss = 0.12500008709948715\n",
            "\n",
            "Iteration 2842 , Loss = 0.12500008666422108\n",
            "\n",
            "Iteration 2843 , Loss = 0.1250000862311306\n",
            "\n",
            "Iteration 2844 , Loss = 0.12500008580020486\n",
            "\n",
            "Iteration 2845 , Loss = 0.12500008537143306\n",
            "\n",
            "Iteration 2846 , Loss = 0.12500008494480438\n",
            "\n",
            "Iteration 2847 , Loss = 0.12500008452030817\n",
            "\n",
            "Iteration 2848 , Loss = 0.12500008409793373\n",
            "\n",
            "Iteration 2849 , Loss = 0.1250000836776705\n",
            "\n",
            "Iteration 2850 , Loss = 0.12500008325950784\n",
            "\n",
            "Iteration 2851 , Loss = 0.1250000828434354\n",
            "\n",
            "Iteration 2852 , Loss = 0.12500008242944255\n",
            "\n",
            "Iteration 2853 , Loss = 0.125000082017519\n",
            "\n",
            "Iteration 2854 , Loss = 0.1250000816076544\n",
            "\n",
            "Iteration 2855 , Loss = 0.12500008119983846\n",
            "\n",
            "Iteration 2856 , Loss = 0.12500008079406094\n",
            "\n",
            "Iteration 2857 , Loss = 0.12500008039031163\n",
            "\n",
            "Iteration 2858 , Loss = 0.12500007998858043\n",
            "\n",
            "Iteration 2859 , Loss = 0.1250000795888572\n",
            "\n",
            "Iteration 2860 , Loss = 0.1250000791911319\n",
            "\n",
            "Iteration 2861 , Loss = 0.12500007879539463\n",
            "\n",
            "Iteration 2862 , Loss = 0.1250000784016354\n",
            "\n",
            "Iteration 2863 , Loss = 0.12500007800984428\n",
            "\n",
            "Iteration 2864 , Loss = 0.1250000776200115\n",
            "\n",
            "Iteration 2865 , Loss = 0.12500007723212722\n",
            "\n",
            "Iteration 2866 , Loss = 0.12500007684618175\n",
            "\n",
            "Iteration 2867 , Loss = 0.12500007646216538\n",
            "\n",
            "Iteration 2868 , Loss = 0.12500007608006847\n",
            "\n",
            "Iteration 2869 , Loss = 0.12500007569988136\n",
            "\n",
            "Iteration 2870 , Loss = 0.12500007532159463\n",
            "\n",
            "Iteration 2871 , Loss = 0.12500007494519869\n",
            "\n",
            "Iteration 2872 , Loss = 0.1250000745706841\n",
            "\n",
            "Iteration 2873 , Loss = 0.1250000741980415\n",
            "\n",
            "Iteration 2874 , Loss = 0.1250000738272615\n",
            "\n",
            "Iteration 2875 , Loss = 0.12500007345833478\n",
            "\n",
            "Iteration 2876 , Loss = 0.12500007309125208\n",
            "\n",
            "Iteration 2877 , Loss = 0.12500007272600422\n",
            "\n",
            "Iteration 2878 , Loss = 0.12500007236258195\n",
            "\n",
            "Iteration 2879 , Loss = 0.12500007200097624\n",
            "\n",
            "Iteration 2880 , Loss = 0.12500007164117793\n",
            "\n",
            "Iteration 2881 , Loss = 0.12500007128317808\n",
            "\n",
            "Iteration 2882 , Loss = 0.12500007092696763\n",
            "\n",
            "Iteration 2883 , Loss = 0.12500007057253762\n",
            "\n",
            "Iteration 2884 , Loss = 0.1250000702198792\n",
            "\n",
            "Iteration 2885 , Loss = 0.1250000698689835\n",
            "\n",
            "Iteration 2886 , Loss = 0.1250000695198417\n",
            "\n",
            "Iteration 2887 , Loss = 0.12500006917244502\n",
            "\n",
            "Iteration 2888 , Loss = 0.12500006882678477\n",
            "\n",
            "Iteration 2889 , Loss = 0.12500006848285225\n",
            "\n",
            "Iteration 2890 , Loss = 0.12500006814063885\n",
            "\n",
            "Iteration 2891 , Loss = 0.12500006780013595\n",
            "\n",
            "Iteration 2892 , Loss = 0.125000067461335\n",
            "\n",
            "Iteration 2893 , Loss = 0.1250000671242275\n",
            "\n",
            "Iteration 2894 , Loss = 0.125000066788805\n",
            "\n",
            "Iteration 2895 , Loss = 0.12500006645505904\n",
            "\n",
            "Iteration 2896 , Loss = 0.12500006612298128\n",
            "\n",
            "Iteration 2897 , Loss = 0.1250000657925634\n",
            "\n",
            "Iteration 2898 , Loss = 0.12500006546379702\n",
            "\n",
            "Iteration 2899 , Loss = 0.12500006513667394\n",
            "\n",
            "Iteration 2900 , Loss = 0.12500006481118595\n",
            "\n",
            "Iteration 2901 , Loss = 0.1250000644873249\n",
            "\n",
            "Iteration 2902 , Loss = 0.1250000641650826\n",
            "\n",
            "Iteration 2903 , Loss = 0.125000063844451\n",
            "\n",
            "Iteration 2904 , Loss = 0.125000063525422\n",
            "\n",
            "Iteration 2905 , Loss = 0.12500006320798765\n",
            "\n",
            "Iteration 2906 , Loss = 0.12500006289213994\n",
            "\n",
            "Iteration 2907 , Loss = 0.125000062577871\n",
            "\n",
            "Iteration 2908 , Loss = 0.12500006226517285\n",
            "\n",
            "Iteration 2909 , Loss = 0.12500006195403768\n",
            "\n",
            "Iteration 2910 , Loss = 0.1250000616444577\n",
            "\n",
            "Iteration 2911 , Loss = 0.12500006133642508\n",
            "\n",
            "Iteration 2912 , Loss = 0.12500006102993216\n",
            "\n",
            "Iteration 2913 , Loss = 0.12500006072497116\n",
            "\n",
            "Iteration 2914 , Loss = 0.12500006042153453\n",
            "\n",
            "Iteration 2915 , Loss = 0.12500006011961454\n",
            "\n",
            "Iteration 2916 , Loss = 0.1250000598192037\n",
            "\n",
            "Iteration 2917 , Loss = 0.1250000595202944\n",
            "\n",
            "Iteration 2918 , Loss = 0.12500005922287916\n",
            "\n",
            "Iteration 2919 , Loss = 0.12500005892695049\n",
            "\n",
            "Iteration 2920 , Loss = 0.12500005863250102\n",
            "\n",
            "Iteration 2921 , Loss = 0.1250000583395233\n",
            "\n",
            "Iteration 2922 , Loss = 0.12500005804801\n",
            "\n",
            "Iteration 2923 , Loss = 0.12500005775795378\n",
            "\n",
            "Iteration 2924 , Loss = 0.12500005746934736\n",
            "\n",
            "Iteration 2925 , Loss = 0.12500005718218352\n",
            "\n",
            "Iteration 2926 , Loss = 0.12500005689645505\n",
            "\n",
            "Iteration 2927 , Loss = 0.12500005661215474\n",
            "\n",
            "Iteration 2928 , Loss = 0.12500005632927547\n",
            "\n",
            "Iteration 2929 , Loss = 0.12500005604781014\n",
            "\n",
            "Iteration 2930 , Loss = 0.12500005576775167\n",
            "\n",
            "Iteration 2931 , Loss = 0.12500005548909304\n",
            "\n",
            "Iteration 2932 , Loss = 0.12500005521182725\n",
            "\n",
            "Iteration 2933 , Loss = 0.12500005493594732\n",
            "\n",
            "Iteration 2934 , Loss = 0.12500005466144637\n",
            "\n",
            "Iteration 2935 , Loss = 0.12500005438831746\n",
            "\n",
            "Iteration 2936 , Loss = 0.12500005411655377\n",
            "\n",
            "Iteration 2937 , Loss = 0.12500005384614846\n",
            "\n",
            "Iteration 2938 , Loss = 0.1250000535770947\n",
            "\n",
            "Iteration 2939 , Loss = 0.1250000533093858\n",
            "\n",
            "Iteration 2940 , Loss = 0.125000053043015\n",
            "\n",
            "Iteration 2941 , Loss = 0.12500005277797563\n",
            "\n",
            "Iteration 2942 , Loss = 0.12500005251426102\n",
            "\n",
            "Iteration 2943 , Loss = 0.12500005225186459\n",
            "\n",
            "Iteration 2944 , Loss = 0.12500005199077968\n",
            "\n",
            "Iteration 2945 , Loss = 0.12500005173099976\n",
            "\n",
            "Iteration 2946 , Loss = 0.12500005147251836\n",
            "\n",
            "Iteration 2947 , Loss = 0.12500005121532892\n",
            "\n",
            "Iteration 2948 , Loss = 0.125000050959425\n",
            "\n",
            "Iteration 2949 , Loss = 0.1250000507048002\n",
            "\n",
            "Iteration 2950 , Loss = 0.1250000504514481\n",
            "\n",
            "Iteration 2951 , Loss = 0.1250000501993624\n",
            "\n",
            "Iteration 2952 , Loss = 0.12500004994853667\n",
            "\n",
            "Iteration 2953 , Loss = 0.12500004969896464\n",
            "\n",
            "Iteration 2954 , Loss = 0.12500004945064014\n",
            "\n",
            "Iteration 2955 , Loss = 0.12500004920355678\n",
            "\n",
            "Iteration 2956 , Loss = 0.12500004895770847\n",
            "\n",
            "Iteration 2957 , Loss = 0.125000048713089\n",
            "\n",
            "Iteration 2958 , Loss = 0.12500004846969223\n",
            "\n",
            "Iteration 2959 , Loss = 0.12500004822751207\n",
            "\n",
            "Iteration 2960 , Loss = 0.12500004798654243\n",
            "\n",
            "Iteration 2961 , Loss = 0.12500004774677723\n",
            "\n",
            "Iteration 2962 , Loss = 0.12500004750821048\n",
            "\n",
            "Iteration 2963 , Loss = 0.12500004727083613\n",
            "\n",
            "Iteration 2964 , Loss = 0.12500004703464832\n",
            "\n",
            "Iteration 2965 , Loss = 0.12500004679964105\n",
            "\n",
            "Iteration 2966 , Loss = 0.1250000465658084\n",
            "\n",
            "Iteration 2967 , Loss = 0.12500004633314454\n",
            "\n",
            "Iteration 2968 , Loss = 0.12500004610164367\n",
            "\n",
            "Iteration 2969 , Loss = 0.12500004587129993\n",
            "\n",
            "Iteration 2970 , Loss = 0.1250000456421075\n",
            "\n",
            "Iteration 2971 , Loss = 0.12500004541406068\n",
            "\n",
            "Iteration 2972 , Loss = 0.1250000451871537\n",
            "\n",
            "Iteration 2973 , Loss = 0.12500004496138095\n",
            "\n",
            "Iteration 2974 , Loss = 0.12500004473673665\n",
            "\n",
            "Iteration 2975 , Loss = 0.12500004451321522\n",
            "\n",
            "Iteration 2976 , Loss = 0.12500004429081105\n",
            "\n",
            "Iteration 2977 , Loss = 0.12500004406951848\n",
            "\n",
            "Iteration 2978 , Loss = 0.12500004384933208\n",
            "\n",
            "Iteration 2979 , Loss = 0.12500004363024625\n",
            "\n",
            "Iteration 2980 , Loss = 0.12500004341225548\n",
            "\n",
            "Iteration 2981 , Loss = 0.12500004319535432\n",
            "\n",
            "Iteration 2982 , Loss = 0.12500004297953732\n",
            "\n",
            "Iteration 2983 , Loss = 0.12500004276479904\n",
            "\n",
            "Iteration 2984 , Loss = 0.12500004255113412\n",
            "\n",
            "Iteration 2985 , Loss = 0.12500004233853715\n",
            "\n",
            "Iteration 2986 , Loss = 0.12500004212700286\n",
            "\n",
            "Iteration 2987 , Loss = 0.1250000419165259\n",
            "\n",
            "Iteration 2988 , Loss = 0.12500004170710094\n",
            "\n",
            "Iteration 2989 , Loss = 0.12500004149872282\n",
            "\n",
            "Iteration 2990 , Loss = 0.12500004129138625\n",
            "\n",
            "Iteration 2991 , Loss = 0.125000041085086\n",
            "\n",
            "Iteration 2992 , Loss = 0.12500004087981695\n",
            "\n",
            "Iteration 2993 , Loss = 0.12500004067557385\n",
            "\n",
            "Iteration 2994 , Loss = 0.1250000404723517\n",
            "\n",
            "Iteration 2995 , Loss = 0.12500004027014533\n",
            "\n",
            "Iteration 2996 , Loss = 0.12500004006894963\n",
            "\n",
            "Iteration 2997 , Loss = 0.12500003986875963\n",
            "\n",
            "Iteration 2998 , Loss = 0.12500003966957024\n",
            "\n",
            "Iteration 2999 , Loss = 0.12500003947137645\n",
            "\n",
            "Iteration 3000 , Loss = 0.12500003927417336\n",
            "\n",
            "Iteration 3001 , Loss = 0.12500003907795593\n",
            "\n",
            "Iteration 3002 , Loss = 0.1250000388827193\n",
            "\n",
            "Iteration 3003 , Loss = 0.1250000386884585\n",
            "\n",
            "Iteration 3004 , Loss = 0.12500003849516866\n",
            "\n",
            "Iteration 3005 , Loss = 0.12500003830284506\n",
            "\n",
            "Iteration 3006 , Loss = 0.12500003811148272\n",
            "\n",
            "Iteration 3007 , Loss = 0.12500003792107686\n",
            "\n",
            "Iteration 3008 , Loss = 0.12500003773162274\n",
            "\n",
            "Iteration 3009 , Loss = 0.12500003754311562\n",
            "\n",
            "Iteration 3010 , Loss = 0.12500003735555068\n",
            "\n",
            "Iteration 3011 , Loss = 0.1250000371689233\n",
            "\n",
            "Iteration 3012 , Loss = 0.12500003698322873\n",
            "\n",
            "Iteration 3013 , Loss = 0.12500003679846233\n",
            "\n",
            "Iteration 3014 , Loss = 0.1250000366146195\n",
            "\n",
            "Iteration 3015 , Loss = 0.12500003643169558\n",
            "\n",
            "Iteration 3016 , Loss = 0.12500003624968598\n",
            "\n",
            "Iteration 3017 , Loss = 0.12500003606858615\n",
            "\n",
            "Iteration 3018 , Loss = 0.1250000358883915\n",
            "\n",
            "Iteration 3019 , Loss = 0.12500003570909757\n",
            "\n",
            "Iteration 3020 , Loss = 0.12500003553069977\n",
            "\n",
            "Iteration 3021 , Loss = 0.12500003535319373\n",
            "\n",
            "Iteration 3022 , Loss = 0.1250000351765749\n",
            "\n",
            "Iteration 3023 , Loss = 0.1250000350008389\n",
            "\n",
            "Iteration 3024 , Loss = 0.12500003482598127\n",
            "\n",
            "Iteration 3025 , Loss = 0.12500003465199766\n",
            "\n",
            "Iteration 3026 , Loss = 0.12500003447888372\n",
            "\n",
            "Iteration 3027 , Loss = 0.12500003430663506\n",
            "\n",
            "Iteration 3028 , Loss = 0.12500003413524735\n",
            "\n",
            "Iteration 3029 , Loss = 0.12500003396471632\n",
            "\n",
            "Iteration 3030 , Loss = 0.12500003379503763\n",
            "\n",
            "Iteration 3031 , Loss = 0.1250000336262071\n",
            "\n",
            "Iteration 3032 , Loss = 0.12500003345822044\n",
            "\n",
            "Iteration 3033 , Loss = 0.12500003329107343\n",
            "\n",
            "Iteration 3034 , Loss = 0.12500003312476193\n",
            "\n",
            "Iteration 3035 , Loss = 0.1250000329592817\n",
            "\n",
            "Iteration 3036 , Loss = 0.1250000327946286\n",
            "\n",
            "Iteration 3037 , Loss = 0.1250000326307985\n",
            "\n",
            "Iteration 3038 , Loss = 0.1250000324677873\n",
            "\n",
            "Iteration 3039 , Loss = 0.1250000323055909\n",
            "\n",
            "Iteration 3040 , Loss = 0.1250000321442052\n",
            "\n",
            "Iteration 3041 , Loss = 0.1250000319836262\n",
            "\n",
            "Iteration 3042 , Loss = 0.1250000318238498\n",
            "\n",
            "Iteration 3043 , Loss = 0.12500003166487209\n",
            "\n",
            "Iteration 3044 , Loss = 0.12500003150668895\n",
            "\n",
            "Iteration 3045 , Loss = 0.1250000313492965\n",
            "\n",
            "Iteration 3046 , Loss = 0.12500003119269076\n",
            "\n",
            "Iteration 3047 , Loss = 0.1250000310368678\n",
            "\n",
            "Iteration 3048 , Loss = 0.12500003088182368\n",
            "\n",
            "Iteration 3049 , Loss = 0.12500003072755458\n",
            "\n",
            "Iteration 3050 , Loss = 0.12500003057405654\n",
            "\n",
            "Iteration 3051 , Loss = 0.1250000304213258\n",
            "\n",
            "Iteration 3052 , Loss = 0.1250000302693584\n",
            "\n",
            "Iteration 3053 , Loss = 0.1250000301181506\n",
            "\n",
            "Iteration 3054 , Loss = 0.12500002996769866\n",
            "\n",
            "Iteration 3055 , Loss = 0.12500002981799868\n",
            "\n",
            "Iteration 3056 , Loss = 0.125000029669047\n",
            "\n",
            "Iteration 3057 , Loss = 0.12500002952083983\n",
            "\n",
            "Iteration 3058 , Loss = 0.12500002937337346\n",
            "\n",
            "Iteration 3059 , Loss = 0.12500002922664416\n",
            "\n",
            "Iteration 3060 , Loss = 0.12500002908064833\n",
            "\n",
            "Iteration 3061 , Loss = 0.1250000289353822\n",
            "\n",
            "Iteration 3062 , Loss = 0.12500002879084215\n",
            "\n",
            "Iteration 3063 , Loss = 0.12500002864702464\n",
            "\n",
            "Iteration 3064 , Loss = 0.12500002850392594\n",
            "\n",
            "Iteration 3065 , Loss = 0.1250000283615425\n",
            "\n",
            "Iteration 3066 , Loss = 0.12500002821987077\n",
            "\n",
            "Iteration 3067 , Loss = 0.1250000280789072\n",
            "\n",
            "Iteration 3068 , Loss = 0.12500002793864817\n",
            "\n",
            "Iteration 3069 , Loss = 0.12500002779909025\n",
            "\n",
            "Iteration 3070 , Loss = 0.12500002766022988\n",
            "\n",
            "Iteration 3071 , Loss = 0.1250000275220636\n",
            "\n",
            "Iteration 3072 , Loss = 0.12500002738458793\n",
            "\n",
            "Iteration 3073 , Loss = 0.1250000272477994\n",
            "\n",
            "Iteration 3074 , Loss = 0.1250000271116946\n",
            "\n",
            "Iteration 3075 , Loss = 0.12500002697627013\n",
            "\n",
            "Iteration 3076 , Loss = 0.1250000268415226\n",
            "\n",
            "Iteration 3077 , Loss = 0.1250000267074485\n",
            "\n",
            "Iteration 3078 , Loss = 0.12500002657404463\n",
            "\n",
            "Iteration 3079 , Loss = 0.12500002644130753\n",
            "\n",
            "Iteration 3080 , Loss = 0.1250000263092339\n",
            "\n",
            "Iteration 3081 , Loss = 0.1250000261778205\n",
            "\n",
            "Iteration 3082 , Loss = 0.1250000260470639\n",
            "\n",
            "Iteration 3083 , Loss = 0.12500002591696088\n",
            "\n",
            "Iteration 3084 , Loss = 0.12500002578750818\n",
            "\n",
            "Iteration 3085 , Loss = 0.12500002565870252\n",
            "\n",
            "Iteration 3086 , Loss = 0.12500002553054068\n",
            "\n",
            "Iteration 3087 , Loss = 0.1250000254030195\n",
            "\n",
            "Iteration 3088 , Loss = 0.12500002527613566\n",
            "\n",
            "Iteration 3089 , Loss = 0.1250000251498861\n",
            "\n",
            "Iteration 3090 , Loss = 0.12500002502426752\n",
            "\n",
            "Iteration 3091 , Loss = 0.12500002489927686\n",
            "\n",
            "Iteration 3092 , Loss = 0.12500002477491096\n",
            "\n",
            "Iteration 3093 , Loss = 0.1250000246511667\n",
            "\n",
            "Iteration 3094 , Loss = 0.12500002452804096\n",
            "\n",
            "Iteration 3095 , Loss = 0.12500002440553062\n",
            "\n",
            "Iteration 3096 , Loss = 0.12500002428363272\n",
            "\n",
            "Iteration 3097 , Loss = 0.12500002416234407\n",
            "\n",
            "Iteration 3098 , Loss = 0.12500002404166166\n",
            "\n",
            "Iteration 3099 , Loss = 0.1250000239215825\n",
            "\n",
            "Iteration 3100 , Loss = 0.1250000238021035\n",
            "\n",
            "Iteration 3101 , Loss = 0.12500002368322172\n",
            "\n",
            "Iteration 3102 , Loss = 0.1250000235649342\n",
            "\n",
            "Iteration 3103 , Loss = 0.1250000234472379\n",
            "\n",
            "Iteration 3104 , Loss = 0.1250000233301299\n",
            "\n",
            "Iteration 3105 , Loss = 0.12500002321360726\n",
            "\n",
            "Iteration 3106 , Loss = 0.12500002309766706\n",
            "\n",
            "Iteration 3107 , Loss = 0.12500002298230636\n",
            "\n",
            "Iteration 3108 , Loss = 0.1250000228675223\n",
            "\n",
            "Iteration 3109 , Loss = 0.12500002275331196\n",
            "\n",
            "Iteration 3110 , Loss = 0.12500002263967247\n",
            "\n",
            "Iteration 3111 , Loss = 0.12500002252660106\n",
            "\n",
            "Iteration 3112 , Loss = 0.12500002241409477\n",
            "\n",
            "Iteration 3113 , Loss = 0.12500002230215088\n",
            "\n",
            "Iteration 3114 , Loss = 0.1250000221907665\n",
            "\n",
            "Iteration 3115 , Loss = 0.12500002207993888\n",
            "\n",
            "Iteration 3116 , Loss = 0.12500002196966523\n",
            "\n",
            "Iteration 3117 , Loss = 0.12500002185994277\n",
            "\n",
            "Iteration 3118 , Loss = 0.12500002175076874\n",
            "\n",
            "Iteration 3119 , Loss = 0.1250000216421404\n",
            "\n",
            "Iteration 3120 , Loss = 0.12500002153405504\n",
            "\n",
            "Iteration 3121 , Loss = 0.12500002142650996\n",
            "\n",
            "Iteration 3122 , Loss = 0.12500002131950244\n",
            "\n",
            "Iteration 3123 , Loss = 0.12500002121302975\n",
            "\n",
            "Iteration 3124 , Loss = 0.1250000211070893\n",
            "\n",
            "Iteration 3125 , Loss = 0.12500002100167834\n",
            "\n",
            "Iteration 3126 , Loss = 0.1250000208967943\n",
            "\n",
            "Iteration 3127 , Loss = 0.1250000207924345\n",
            "\n",
            "Iteration 3128 , Loss = 0.12500002068859634\n",
            "\n",
            "Iteration 3129 , Loss = 0.1250000205852772\n",
            "\n",
            "Iteration 3130 , Loss = 0.1250000204824745\n",
            "\n",
            "Iteration 3131 , Loss = 0.12500002038018565\n",
            "\n",
            "Iteration 3132 , Loss = 0.1250000202784081\n",
            "\n",
            "Iteration 3133 , Loss = 0.12500002017713926\n",
            "\n",
            "Iteration 3134 , Loss = 0.12500002007637662\n",
            "\n",
            "Iteration 3135 , Loss = 0.12500001997611762\n",
            "\n",
            "Iteration 3136 , Loss = 0.12500001987635975\n",
            "\n",
            "Iteration 3137 , Loss = 0.12500001977710054\n",
            "\n",
            "Iteration 3138 , Loss = 0.12500001967833746\n",
            "\n",
            "Iteration 3139 , Loss = 0.12500001958006804\n",
            "\n",
            "Iteration 3140 , Loss = 0.12500001948228984\n",
            "\n",
            "Iteration 3141 , Loss = 0.12500001938500033\n",
            "\n",
            "Iteration 3142 , Loss = 0.12500001928819712\n",
            "\n",
            "Iteration 3143 , Loss = 0.1250000191918778\n",
            "\n",
            "Iteration 3144 , Loss = 0.12500001909603992\n",
            "\n",
            "Iteration 3145 , Loss = 0.12500001900068108\n",
            "\n",
            "Iteration 3146 , Loss = 0.12500001890579887\n",
            "\n",
            "Iteration 3147 , Loss = 0.12500001881139094\n",
            "\n",
            "Iteration 3148 , Loss = 0.1250000187174549\n",
            "\n",
            "Iteration 3149 , Loss = 0.12500001862398835\n",
            "\n",
            "Iteration 3150 , Loss = 0.12500001853098905\n",
            "\n",
            "Iteration 3151 , Loss = 0.1250000184384546\n",
            "\n",
            "Iteration 3152 , Loss = 0.12500001834638266\n",
            "\n",
            "Iteration 3153 , Loss = 0.12500001825477092\n",
            "\n",
            "Iteration 3154 , Loss = 0.1250000181636171\n",
            "\n",
            "Iteration 3155 , Loss = 0.1250000180729189\n",
            "\n",
            "Iteration 3156 , Loss = 0.12500001798267407\n",
            "\n",
            "Iteration 3157 , Loss = 0.12500001789288034\n",
            "\n",
            "Iteration 3158 , Loss = 0.1250000178035354\n",
            "\n",
            "Iteration 3159 , Loss = 0.12500001771463706\n",
            "\n",
            "Iteration 3160 , Loss = 0.12500001762618307\n",
            "\n",
            "Iteration 3161 , Loss = 0.12500001753817125\n",
            "\n",
            "Iteration 3162 , Loss = 0.1250000174505993\n",
            "\n",
            "Iteration 3163 , Loss = 0.12500001736346508\n",
            "\n",
            "Iteration 3164 , Loss = 0.12500001727676643\n",
            "\n",
            "Iteration 3165 , Loss = 0.12500001719050108\n",
            "\n",
            "Iteration 3166 , Loss = 0.125000017104667\n",
            "\n",
            "Iteration 3167 , Loss = 0.12500001701926192\n",
            "\n",
            "Iteration 3168 , Loss = 0.1250000169342837\n",
            "\n",
            "Iteration 3169 , Loss = 0.12500001684973028\n",
            "\n",
            "Iteration 3170 , Loss = 0.12500001676559946\n",
            "\n",
            "Iteration 3171 , Loss = 0.1250000166818892\n",
            "\n",
            "Iteration 3172 , Loss = 0.12500001659859736\n",
            "\n",
            "Iteration 3173 , Loss = 0.12500001651572185\n",
            "\n",
            "Iteration 3174 , Loss = 0.12500001643326053\n",
            "\n",
            "Iteration 3175 , Loss = 0.12500001635121147\n",
            "\n",
            "Iteration 3176 , Loss = 0.1250000162695725\n",
            "\n",
            "Iteration 3177 , Loss = 0.12500001618834158\n",
            "\n",
            "Iteration 3178 , Loss = 0.1250000161075167\n",
            "\n",
            "Iteration 3179 , Loss = 0.12500001602709582\n",
            "\n",
            "Iteration 3180 , Loss = 0.1250000159470769\n",
            "\n",
            "Iteration 3181 , Loss = 0.12500001586745801\n",
            "\n",
            "Iteration 3182 , Loss = 0.12500001578823705\n",
            "\n",
            "Iteration 3183 , Loss = 0.12500001570941208\n",
            "\n",
            "Iteration 3184 , Loss = 0.12500001563098107\n",
            "\n",
            "Iteration 3185 , Loss = 0.12500001555294216\n",
            "\n",
            "Iteration 3186 , Loss = 0.12500001547529327\n",
            "\n",
            "Iteration 3187 , Loss = 0.12500001539803254\n",
            "\n",
            "Iteration 3188 , Loss = 0.12500001532115798\n",
            "\n",
            "Iteration 3189 , Loss = 0.12500001524466767\n",
            "\n",
            "Iteration 3190 , Loss = 0.12500001516855969\n",
            "\n",
            "Iteration 3191 , Loss = 0.12500001509283215\n",
            "\n",
            "Iteration 3192 , Loss = 0.12500001501748315\n",
            "\n",
            "Iteration 3193 , Loss = 0.12500001494251073\n",
            "\n",
            "Iteration 3194 , Loss = 0.12500001486791307\n",
            "\n",
            "Iteration 3195 , Loss = 0.1250000147936883\n",
            "\n",
            "Iteration 3196 , Loss = 0.12500001471983455\n",
            "\n",
            "Iteration 3197 , Loss = 0.12500001464634994\n",
            "\n",
            "Iteration 3198 , Loss = 0.12500001457323262\n",
            "\n",
            "Iteration 3199 , Loss = 0.1250000145004808\n",
            "\n",
            "Iteration 3200 , Loss = 0.1250000144280926\n",
            "\n",
            "Iteration 3201 , Loss = 0.12500001435606625\n",
            "\n",
            "Iteration 3202 , Loss = 0.1250000142843999\n",
            "\n",
            "Iteration 3203 , Loss = 0.1250000142130918\n",
            "\n",
            "Iteration 3204 , Loss = 0.12500001414214013\n",
            "\n",
            "Iteration 3205 , Loss = 0.1250000140715431\n",
            "\n",
            "Iteration 3206 , Loss = 0.12500001400129895\n",
            "\n",
            "Iteration 3207 , Loss = 0.12500001393140592\n",
            "\n",
            "Iteration 3208 , Loss = 0.12500001386186219\n",
            "\n",
            "Iteration 3209 , Loss = 0.12500001379266612\n",
            "\n",
            "Iteration 3210 , Loss = 0.12500001372381594\n",
            "\n",
            "Iteration 3211 , Loss = 0.12500001365530988\n",
            "\n",
            "Iteration 3212 , Loss = 0.12500001358714627\n",
            "\n",
            "Iteration 3213 , Loss = 0.12500001351932336\n",
            "\n",
            "Iteration 3214 , Loss = 0.12500001345183945\n",
            "\n",
            "Iteration 3215 , Loss = 0.12500001338469285\n",
            "\n",
            "Iteration 3216 , Loss = 0.1250000133178819\n",
            "\n",
            "Iteration 3217 , Loss = 0.1250000132514049\n",
            "\n",
            "Iteration 3218 , Loss = 0.1250000131852602\n",
            "\n",
            "Iteration 3219 , Loss = 0.12500001311944609\n",
            "\n",
            "Iteration 3220 , Loss = 0.12500001305396097\n",
            "\n",
            "Iteration 3221 , Loss = 0.12500001298880314\n",
            "\n",
            "Iteration 3222 , Loss = 0.125000012923971\n",
            "\n",
            "Iteration 3223 , Loss = 0.125000012859463\n",
            "\n",
            "Iteration 3224 , Loss = 0.12500001279527737\n",
            "\n",
            "Iteration 3225 , Loss = 0.1250000127314126\n",
            "\n",
            "Iteration 3226 , Loss = 0.12500001266786703\n",
            "\n",
            "Iteration 3227 , Loss = 0.1250000126046391\n",
            "\n",
            "Iteration 3228 , Loss = 0.1250000125417272\n",
            "\n",
            "Iteration 3229 , Loss = 0.12500001247912979\n",
            "\n",
            "Iteration 3230 , Loss = 0.12500001241684527\n",
            "\n",
            "Iteration 3231 , Loss = 0.1250000123548721\n",
            "\n",
            "Iteration 3232 , Loss = 0.12500001229320867\n",
            "\n",
            "Iteration 3233 , Loss = 0.12500001223185345\n",
            "\n",
            "Iteration 3234 , Loss = 0.12500001217080492\n",
            "\n",
            "Iteration 3235 , Loss = 0.12500001211006154\n",
            "\n",
            "Iteration 3236 , Loss = 0.1250000120496218\n",
            "\n",
            "Iteration 3237 , Loss = 0.12500001198948416\n",
            "\n",
            "Iteration 3238 , Loss = 0.12500001192964713\n",
            "\n",
            "Iteration 3239 , Loss = 0.12500001187010915\n",
            "\n",
            "Iteration 3240 , Loss = 0.1250000118108688\n",
            "\n",
            "Iteration 3241 , Loss = 0.1250000117519246\n",
            "\n",
            "Iteration 3242 , Loss = 0.125000011693275\n",
            "\n",
            "Iteration 3243 , Loss = 0.12500001163491853\n",
            "\n",
            "Iteration 3244 , Loss = 0.12500001157685375\n",
            "\n",
            "Iteration 3245 , Loss = 0.12500001151907925\n",
            "\n",
            "Iteration 3246 , Loss = 0.1250000114615935\n",
            "\n",
            "Iteration 3247 , Loss = 0.12500001140439512\n",
            "\n",
            "Iteration 3248 , Loss = 0.12500001134748262\n",
            "\n",
            "Iteration 3249 , Loss = 0.1250000112908546\n",
            "\n",
            "Iteration 3250 , Loss = 0.12500001123450966\n",
            "\n",
            "Iteration 3251 , Loss = 0.12500001117844634\n",
            "\n",
            "Iteration 3252 , Loss = 0.12500001112266323\n",
            "\n",
            "Iteration 3253 , Loss = 0.12500001106715902\n",
            "\n",
            "Iteration 3254 , Loss = 0.12500001101193217\n",
            "\n",
            "Iteration 3255 , Loss = 0.1250000109569814\n",
            "\n",
            "Iteration 3256 , Loss = 0.1250000109023053\n",
            "\n",
            "Iteration 3257 , Loss = 0.12500001084790252\n",
            "\n",
            "Iteration 3258 , Loss = 0.12500001079377165\n",
            "\n",
            "Iteration 3259 , Loss = 0.12500001073991135\n",
            "\n",
            "Iteration 3260 , Loss = 0.1250000106863203\n",
            "\n",
            "Iteration 3261 , Loss = 0.1250000106329971\n",
            "\n",
            "Iteration 3262 , Loss = 0.1250000105799404\n",
            "\n",
            "Iteration 3263 , Loss = 0.12500001052714896\n",
            "\n",
            "Iteration 3264 , Loss = 0.12500001047462136\n",
            "\n",
            "Iteration 3265 , Loss = 0.12500001042235634\n",
            "\n",
            "Iteration 3266 , Loss = 0.12500001037035255\n",
            "\n",
            "Iteration 3267 , Loss = 0.12500001031860875\n",
            "\n",
            "Iteration 3268 , Loss = 0.12500001026712354\n",
            "\n",
            "Iteration 3269 , Loss = 0.12500001021589568\n",
            "\n",
            "Iteration 3270 , Loss = 0.1250000101649239\n",
            "\n",
            "Iteration 3271 , Loss = 0.1250000101142069\n",
            "\n",
            "Iteration 3272 , Loss = 0.1250000100637434\n",
            "\n",
            "Iteration 3273 , Loss = 0.12500001001353214\n",
            "\n",
            "Iteration 3274 , Loss = 0.12500000996357186\n",
            "\n",
            "Iteration 3275 , Loss = 0.12500000991386132\n",
            "\n",
            "Iteration 3276 , Loss = 0.12500000986439927\n",
            "\n",
            "Iteration 3277 , Loss = 0.12500000981518442\n",
            "\n",
            "Iteration 3278 , Loss = 0.12500000976621561\n",
            "\n",
            "Iteration 3279 , Loss = 0.12500000971749153\n",
            "\n",
            "Iteration 3280 , Loss = 0.12500000966901104\n",
            "\n",
            "Iteration 3281 , Loss = 0.12500000962077285\n",
            "\n",
            "Iteration 3282 , Loss = 0.1250000095727758\n",
            "\n",
            "Iteration 3283 , Loss = 0.1250000095250186\n",
            "\n",
            "Iteration 3284 , Loss = 0.12500000947750017\n",
            "\n",
            "Iteration 3285 , Loss = 0.12500000943021927\n",
            "\n",
            "Iteration 3286 , Loss = 0.12500000938317468\n",
            "\n",
            "Iteration 3287 , Loss = 0.12500000933636524\n",
            "\n",
            "Iteration 3288 , Loss = 0.1250000092897898\n",
            "\n",
            "Iteration 3289 , Loss = 0.12500000924344717\n",
            "\n",
            "Iteration 3290 , Loss = 0.12500000919733617\n",
            "\n",
            "Iteration 3291 , Loss = 0.12500000915145565\n",
            "\n",
            "Iteration 3292 , Loss = 0.12500000910580444\n",
            "\n",
            "Iteration 3293 , Loss = 0.12500000906038145\n",
            "\n",
            "Iteration 3294 , Loss = 0.12500000901518554\n",
            "\n",
            "Iteration 3295 , Loss = 0.12500000897021551\n",
            "\n",
            "Iteration 3296 , Loss = 0.12500000892547025\n",
            "\n",
            "Iteration 3297 , Loss = 0.12500000888094867\n",
            "\n",
            "Iteration 3298 , Loss = 0.1250000088366496\n",
            "\n",
            "Iteration 3299 , Loss = 0.12500000879257198\n",
            "\n",
            "Iteration 3300 , Loss = 0.1250000087487147\n",
            "\n",
            "Iteration 3301 , Loss = 0.1250000087050766\n",
            "\n",
            "Iteration 3302 , Loss = 0.12500000866165667\n",
            "\n",
            "Iteration 3303 , Loss = 0.12500000861845376\n",
            "\n",
            "Iteration 3304 , Loss = 0.12500000857546678\n",
            "\n",
            "Iteration 3305 , Loss = 0.1250000085326947\n",
            "\n",
            "Iteration 3306 , Loss = 0.12500000849013643\n",
            "\n",
            "Iteration 3307 , Loss = 0.12500000844779083\n",
            "\n",
            "Iteration 3308 , Loss = 0.12500000840565695\n",
            "\n",
            "Iteration 3309 , Loss = 0.12500000836373365\n",
            "\n",
            "Iteration 3310 , Loss = 0.1250000083220199\n",
            "\n",
            "Iteration 3311 , Loss = 0.1250000082805147\n",
            "\n",
            "Iteration 3312 , Loss = 0.12500000823921692\n",
            "\n",
            "Iteration 3313 , Loss = 0.12500000819812557\n",
            "\n",
            "Iteration 3314 , Loss = 0.12500000815723963\n",
            "\n",
            "Iteration 3315 , Loss = 0.12500000811655804\n",
            "\n",
            "Iteration 3316 , Loss = 0.1250000080760798\n",
            "\n",
            "Iteration 3317 , Loss = 0.1250000080358039\n",
            "\n",
            "Iteration 3318 , Loss = 0.12500000799572933\n",
            "\n",
            "Iteration 3319 , Loss = 0.12500000795585506\n",
            "\n",
            "Iteration 3320 , Loss = 0.12500000791618013\n",
            "\n",
            "Iteration 3321 , Loss = 0.12500000787670348\n",
            "\n",
            "Iteration 3322 , Loss = 0.12500000783742418\n",
            "\n",
            "Iteration 3323 , Loss = 0.12500000779834122\n",
            "\n",
            "Iteration 3324 , Loss = 0.1250000077594536\n",
            "\n",
            "Iteration 3325 , Loss = 0.12500000772076036\n",
            "\n",
            "Iteration 3326 , Loss = 0.12500000768226052\n",
            "\n",
            "Iteration 3327 , Loss = 0.12500000764395316\n",
            "\n",
            "Iteration 3328 , Loss = 0.12500000760583727\n",
            "\n",
            "Iteration 3329 , Loss = 0.1250000075679119\n",
            "\n",
            "Iteration 3330 , Loss = 0.1250000075301761\n",
            "\n",
            "Iteration 3331 , Loss = 0.12500000749262893\n",
            "\n",
            "Iteration 3332 , Loss = 0.12500000745526943\n",
            "\n",
            "Iteration 3333 , Loss = 0.12500000741809664\n",
            "\n",
            "Iteration 3334 , Loss = 0.1250000073811097\n",
            "\n",
            "Iteration 3335 , Loss = 0.1250000073443076\n",
            "\n",
            "Iteration 3336 , Loss = 0.12500000730768954\n",
            "\n",
            "Iteration 3337 , Loss = 0.12500000727125446\n",
            "\n",
            "Iteration 3338 , Loss = 0.1250000072350015\n",
            "\n",
            "Iteration 3339 , Loss = 0.12500000719892976\n",
            "\n",
            "Iteration 3340 , Loss = 0.12500000716303833\n",
            "\n",
            "Iteration 3341 , Loss = 0.1250000071273263\n",
            "\n",
            "Iteration 3342 , Loss = 0.1250000070917928\n",
            "\n",
            "Iteration 3343 , Loss = 0.12500000705643685\n",
            "\n",
            "Iteration 3344 , Loss = 0.1250000070212577\n",
            "\n",
            "Iteration 3345 , Loss = 0.12500000698625438\n",
            "\n",
            "Iteration 3346 , Loss = 0.125000006951426\n",
            "\n",
            "Iteration 3347 , Loss = 0.12500000691677174\n",
            "\n",
            "Iteration 3348 , Loss = 0.12500000688229065\n",
            "\n",
            "Iteration 3349 , Loss = 0.12500000684798196\n",
            "\n",
            "Iteration 3350 , Loss = 0.12500000681384477\n",
            "\n",
            "Iteration 3351 , Loss = 0.12500000677987816\n",
            "\n",
            "Iteration 3352 , Loss = 0.1250000067460814\n",
            "\n",
            "Iteration 3353 , Loss = 0.12500000671245354\n",
            "\n",
            "Iteration 3354 , Loss = 0.1250000066789938\n",
            "\n",
            "Iteration 3355 , Loss = 0.1250000066457013\n",
            "\n",
            "Iteration 3356 , Loss = 0.1250000066125752\n",
            "\n",
            "Iteration 3357 , Loss = 0.1250000065796147\n",
            "\n",
            "Iteration 3358 , Loss = 0.12500000654681892\n",
            "\n",
            "Iteration 3359 , Loss = 0.12500000651418708\n",
            "\n",
            "Iteration 3360 , Loss = 0.12500000648171836\n",
            "\n",
            "Iteration 3361 , Loss = 0.12500000644941198\n",
            "\n",
            "Iteration 3362 , Loss = 0.12500000641726705\n",
            "\n",
            "Iteration 3363 , Loss = 0.12500000638528277\n",
            "\n",
            "Iteration 3364 , Loss = 0.12500000635345837\n",
            "\n",
            "Iteration 3365 , Loss = 0.1250000063217931\n",
            "\n",
            "Iteration 3366 , Loss = 0.1250000062902861\n",
            "\n",
            "Iteration 3367 , Loss = 0.12500000625893654\n",
            "\n",
            "Iteration 3368 , Loss = 0.1250000062277437\n",
            "\n",
            "Iteration 3369 , Loss = 0.12500000619670681\n",
            "\n",
            "Iteration 3370 , Loss = 0.12500000616582502\n",
            "\n",
            "Iteration 3371 , Loss = 0.12500000613509762\n",
            "\n",
            "Iteration 3372 , Loss = 0.1250000061045238\n",
            "\n",
            "Iteration 3373 , Loss = 0.12500000607410283\n",
            "\n",
            "Iteration 3374 , Loss = 0.1250000060438339\n",
            "\n",
            "Iteration 3375 , Loss = 0.12500000601371627\n",
            "\n",
            "Iteration 3376 , Loss = 0.1250000059837492\n",
            "\n",
            "Iteration 3377 , Loss = 0.1250000059539319\n",
            "\n",
            "Iteration 3378 , Loss = 0.1250000059242637\n",
            "\n",
            "Iteration 3379 , Loss = 0.12500000589474372\n",
            "\n",
            "Iteration 3380 , Loss = 0.12500000586537133\n",
            "\n",
            "Iteration 3381 , Loss = 0.12500000583614576\n",
            "\n",
            "Iteration 3382 , Loss = 0.12500000580706627\n",
            "\n",
            "Iteration 3383 , Loss = 0.12500000577813214\n",
            "\n",
            "Iteration 3384 , Loss = 0.12500000574934264\n",
            "\n",
            "Iteration 3385 , Loss = 0.12500000572069708\n",
            "\n",
            "Iteration 3386 , Loss = 0.12500000569219463\n",
            "\n",
            "Iteration 3387 , Loss = 0.1250000056638347\n",
            "\n",
            "Iteration 3388 , Loss = 0.12500000563561656\n",
            "\n",
            "Iteration 3389 , Loss = 0.12500000560753943\n",
            "\n",
            "Iteration 3390 , Loss = 0.12500000557960267\n",
            "\n",
            "Iteration 3391 , Loss = 0.12500000555180552\n",
            "\n",
            "Iteration 3392 , Loss = 0.12500000552414736\n",
            "\n",
            "Iteration 3393 , Loss = 0.1250000054966274\n",
            "\n",
            "Iteration 3394 , Loss = 0.12500000546924503\n",
            "\n",
            "Iteration 3395 , Loss = 0.12500000544199955\n",
            "\n",
            "Iteration 3396 , Loss = 0.12500000541489023\n",
            "\n",
            "Iteration 3397 , Loss = 0.12500000538791642\n",
            "\n",
            "Iteration 3398 , Loss = 0.12500000536107744\n",
            "\n",
            "Iteration 3399 , Loss = 0.12500000533437264\n",
            "\n",
            "Iteration 3400 , Loss = 0.1250000053078013\n",
            "\n",
            "Iteration 3401 , Loss = 0.1250000052813628\n",
            "\n",
            "Iteration 3402 , Loss = 0.12500000525505645\n",
            "\n",
            "Iteration 3403 , Loss = 0.12500000522888158\n",
            "\n",
            "Iteration 3404 , Loss = 0.12500000520283755\n",
            "\n",
            "Iteration 3405 , Loss = 0.1250000051769237\n",
            "\n",
            "Iteration 3406 , Loss = 0.12500000515113943\n",
            "\n",
            "Iteration 3407 , Loss = 0.12500000512548398\n",
            "\n",
            "Iteration 3408 , Loss = 0.1250000050999568\n",
            "\n",
            "Iteration 3409 , Loss = 0.12500000507455722\n",
            "\n",
            "Iteration 3410 , Loss = 0.1250000050492846\n",
            "\n",
            "Iteration 3411 , Loss = 0.12500000502413833\n",
            "\n",
            "Iteration 3412 , Loss = 0.12500000499911773\n",
            "\n",
            "Iteration 3413 , Loss = 0.1250000049742222\n",
            "\n",
            "Iteration 3414 , Loss = 0.12500000494945113\n",
            "\n",
            "Iteration 3415 , Loss = 0.12500000492480387\n",
            "\n",
            "Iteration 3416 , Loss = 0.1250000049002798\n",
            "\n",
            "Iteration 3417 , Loss = 0.1250000048758783\n",
            "\n",
            "Iteration 3418 , Loss = 0.12500000485159884\n",
            "\n",
            "Iteration 3419 , Loss = 0.1250000048274407\n",
            "\n",
            "Iteration 3420 , Loss = 0.12500000480340331\n",
            "\n",
            "Iteration 3421 , Loss = 0.12500000477948608\n",
            "\n",
            "Iteration 3422 , Loss = 0.1250000047556884\n",
            "\n",
            "Iteration 3423 , Loss = 0.12500000473200967\n",
            "\n",
            "Iteration 3424 , Loss = 0.12500000470844932\n",
            "\n",
            "Iteration 3425 , Loss = 0.12500000468500674\n",
            "\n",
            "Iteration 3426 , Loss = 0.12500000466168132\n",
            "\n",
            "Iteration 3427 , Loss = 0.12500000463847252\n",
            "\n",
            "Iteration 3428 , Loss = 0.12500000461537972\n",
            "\n",
            "Iteration 3429 , Loss = 0.12500000459240235\n",
            "\n",
            "Iteration 3430 , Loss = 0.1250000045695398\n",
            "\n",
            "Iteration 3431 , Loss = 0.12500000454679158\n",
            "\n",
            "Iteration 3432 , Loss = 0.12500000452415708\n",
            "\n",
            "Iteration 3433 , Loss = 0.12500000450163568\n",
            "\n",
            "Iteration 3434 , Loss = 0.12500000447922688\n",
            "\n",
            "Iteration 3435 , Loss = 0.1250000044569301\n",
            "\n",
            "Iteration 3436 , Loss = 0.12500000443474477\n",
            "\n",
            "Iteration 3437 , Loss = 0.1250000044126703\n",
            "\n",
            "Iteration 3438 , Loss = 0.1250000043907062\n",
            "\n",
            "Iteration 3439 , Loss = 0.1250000043688519\n",
            "\n",
            "Iteration 3440 , Loss = 0.12500000434710679\n",
            "\n",
            "Iteration 3441 , Loss = 0.12500000432547043\n",
            "\n",
            "Iteration 3442 , Loss = 0.1250000043039422\n",
            "\n",
            "Iteration 3443 , Loss = 0.12500000428252162\n",
            "\n",
            "Iteration 3444 , Loss = 0.12500000426120805\n",
            "\n",
            "Iteration 3445 , Loss = 0.12500000424000102\n",
            "\n",
            "Iteration 3446 , Loss = 0.12500000421890006\n",
            "\n",
            "Iteration 3447 , Loss = 0.12500000419790455\n",
            "\n",
            "Iteration 3448 , Loss = 0.12500000417701398\n",
            "\n",
            "Iteration 3449 , Loss = 0.12500000415622783\n",
            "\n",
            "Iteration 3450 , Loss = 0.1250000041355456\n",
            "\n",
            "Iteration 3451 , Loss = 0.12500000411496676\n",
            "\n",
            "Iteration 3452 , Loss = 0.12500000409449072\n",
            "\n",
            "Iteration 3453 , Loss = 0.1250000040741171\n",
            "\n",
            "Iteration 3454 , Loss = 0.1250000040538453\n",
            "\n",
            "Iteration 3455 , Loss = 0.12500000403367484\n",
            "\n",
            "Iteration 3456 , Loss = 0.12500000401360517\n",
            "\n",
            "Iteration 3457 , Loss = 0.12500000399363587\n",
            "\n",
            "Iteration 3458 , Loss = 0.12500000397376634\n",
            "\n",
            "Iteration 3459 , Loss = 0.12500000395399619\n",
            "\n",
            "Iteration 3460 , Loss = 0.1250000039343248\n",
            "\n",
            "Iteration 3461 , Loss = 0.12500000391475177\n",
            "\n",
            "Iteration 3462 , Loss = 0.12500000389527657\n",
            "\n",
            "Iteration 3463 , Loss = 0.12500000387589877\n",
            "\n",
            "Iteration 3464 , Loss = 0.12500000385661778\n",
            "\n",
            "Iteration 3465 , Loss = 0.12500000383743318\n",
            "\n",
            "Iteration 3466 , Loss = 0.12500000381834447\n",
            "\n",
            "Iteration 3467 , Loss = 0.1250000037993512\n",
            "\n",
            "Iteration 3468 , Loss = 0.12500000378045284\n",
            "\n",
            "Iteration 3469 , Loss = 0.12500000376164896\n",
            "\n",
            "Iteration 3470 , Loss = 0.1250000037429391\n",
            "\n",
            "Iteration 3471 , Loss = 0.1250000037243227\n",
            "\n",
            "Iteration 3472 , Loss = 0.12500000370579942\n",
            "\n",
            "Iteration 3473 , Loss = 0.12500000368736872\n",
            "\n",
            "Iteration 3474 , Loss = 0.1250000036690301\n",
            "\n",
            "Iteration 3475 , Loss = 0.1250000036507832\n",
            "\n",
            "Iteration 3476 , Loss = 0.1250000036326275\n",
            "\n",
            "Iteration 3477 , Loss = 0.12500000361456257\n",
            "\n",
            "Iteration 3478 , Loss = 0.12500000359658792\n",
            "\n",
            "Iteration 3479 , Loss = 0.12500000357870314\n",
            "\n",
            "Iteration 3480 , Loss = 0.12500000356090774\n",
            "\n",
            "Iteration 3481 , Loss = 0.1250000035432013\n",
            "\n",
            "Iteration 3482 , Loss = 0.12500000352558333\n",
            "\n",
            "Iteration 3483 , Loss = 0.12500000350805346\n",
            "\n",
            "Iteration 3484 , Loss = 0.12500000349061124\n",
            "\n",
            "Iteration 3485 , Loss = 0.12500000347325615\n",
            "\n",
            "Iteration 3486 , Loss = 0.12500000345598788\n",
            "\n",
            "Iteration 3487 , Loss = 0.12500000343880588\n",
            "\n",
            "Iteration 3488 , Loss = 0.1250000034217098\n",
            "\n",
            "Iteration 3489 , Loss = 0.12500000340469913\n",
            "\n",
            "Iteration 3490 , Loss = 0.12500000338777353\n",
            "\n",
            "Iteration 3491 , Loss = 0.12500000337093253\n",
            "\n",
            "Iteration 3492 , Loss = 0.12500000335417571\n",
            "\n",
            "Iteration 3493 , Loss = 0.12500000333750266\n",
            "\n",
            "Iteration 3494 , Loss = 0.12500000332091293\n",
            "\n",
            "Iteration 3495 , Loss = 0.12500000330440614\n",
            "\n",
            "Iteration 3496 , Loss = 0.1250000032879819\n",
            "\n",
            "Iteration 3497 , Loss = 0.12500000327163968\n",
            "\n",
            "Iteration 3498 , Loss = 0.12500000325537922\n",
            "\n",
            "Iteration 3499 , Loss = 0.12500000323920002\n",
            "\n",
            "Iteration 3500 , Loss = 0.12500000322310167\n",
            "\n",
            "Iteration 3501 , Loss = 0.1250000032070838\n",
            "\n",
            "Iteration 3502 , Loss = 0.12500000319114601\n",
            "\n",
            "Iteration 3503 , Loss = 0.12500000317528792\n",
            "\n",
            "Iteration 3504 , Loss = 0.12500000315950904\n",
            "\n",
            "Iteration 3505 , Loss = 0.12500000314380905\n",
            "\n",
            "Iteration 3506 , Loss = 0.12500000312818754\n",
            "\n",
            "Iteration 3507 , Loss = 0.12500000311264414\n",
            "\n",
            "Iteration 3508 , Loss = 0.12500000309717846\n",
            "\n",
            "Iteration 3509 , Loss = 0.12500000308179002\n",
            "\n",
            "Iteration 3510 , Loss = 0.12500000306647852\n",
            "\n",
            "Iteration 3511 , Loss = 0.1250000030512436\n",
            "\n",
            "Iteration 3512 , Loss = 0.12500000303608477\n",
            "\n",
            "Iteration 3513 , Loss = 0.12500000302100178\n",
            "\n",
            "Iteration 3514 , Loss = 0.12500000300599412\n",
            "\n",
            "Iteration 3515 , Loss = 0.12500000299106154\n",
            "\n",
            "Iteration 3516 , Loss = 0.12500000297620356\n",
            "\n",
            "Iteration 3517 , Loss = 0.1250000029614199\n",
            "\n",
            "Iteration 3518 , Loss = 0.1250000029467101\n",
            "\n",
            "Iteration 3519 , Loss = 0.12500000293207383\n",
            "\n",
            "Iteration 3520 , Loss = 0.12500000291751073\n",
            "\n",
            "Iteration 3521 , Loss = 0.12500000290302044\n",
            "\n",
            "Iteration 3522 , Loss = 0.12500000288860258\n",
            "\n",
            "Iteration 3523 , Loss = 0.12500000287425675\n",
            "\n",
            "Iteration 3524 , Loss = 0.12500000285998267\n",
            "\n",
            "Iteration 3525 , Loss = 0.12500000284577995\n",
            "\n",
            "Iteration 3526 , Loss = 0.1250000028316482\n",
            "\n",
            "Iteration 3527 , Loss = 0.1250000028175871\n",
            "\n",
            "Iteration 3528 , Loss = 0.12500000280359627\n",
            "\n",
            "Iteration 3529 , Loss = 0.1250000027896754\n",
            "\n",
            "Iteration 3530 , Loss = 0.12500000277582415\n",
            "\n",
            "Iteration 3531 , Loss = 0.12500000276204207\n",
            "\n",
            "Iteration 3532 , Loss = 0.12500000274832893\n",
            "\n",
            "Iteration 3533 , Loss = 0.1250000027346843\n",
            "\n",
            "Iteration 3534 , Loss = 0.12500000272110792\n",
            "\n",
            "Iteration 3535 , Loss = 0.12500000270759937\n",
            "\n",
            "Iteration 3536 , Loss = 0.12500000269415837\n",
            "\n",
            "Iteration 3537 , Loss = 0.1250000026807845\n",
            "\n",
            "Iteration 3538 , Loss = 0.12500000266747757\n",
            "\n",
            "Iteration 3539 , Loss = 0.1250000026542371\n",
            "\n",
            "Iteration 3540 , Loss = 0.12500000264106284\n",
            "\n",
            "Iteration 3541 , Loss = 0.12500000262795444\n",
            "\n",
            "Iteration 3542 , Loss = 0.12500000261491154\n",
            "\n",
            "Iteration 3543 , Loss = 0.1250000026019339\n",
            "\n",
            "Iteration 3544 , Loss = 0.12500000258902105\n",
            "\n",
            "Iteration 3545 , Loss = 0.12500000257617278\n",
            "\n",
            "Iteration 3546 , Loss = 0.12500000256338872\n",
            "\n",
            "Iteration 3547 , Loss = 0.1250000025506686\n",
            "\n",
            "Iteration 3548 , Loss = 0.12500000253801202\n",
            "\n",
            "Iteration 3549 , Loss = 0.12500000252541876\n",
            "\n",
            "Iteration 3550 , Loss = 0.12500000251288843\n",
            "\n",
            "Iteration 3551 , Loss = 0.1250000025004207\n",
            "\n",
            "Iteration 3552 , Loss = 0.12500000248801532\n",
            "\n",
            "Iteration 3553 , Loss = 0.12500000247567197\n",
            "\n",
            "Iteration 3554 , Loss = 0.1250000024633903\n",
            "\n",
            "Iteration 3555 , Loss = 0.12500000245117004\n",
            "\n",
            "Iteration 3556 , Loss = 0.12500000243901083\n",
            "\n",
            "Iteration 3557 , Loss = 0.12500000242691242\n",
            "\n",
            "Iteration 3558 , Loss = 0.1250000024148745\n",
            "\n",
            "Iteration 3559 , Loss = 0.12500000240289674\n",
            "\n",
            "Iteration 3560 , Loss = 0.1250000023909789\n",
            "\n",
            "Iteration 3561 , Loss = 0.12500000237912057\n",
            "\n",
            "Iteration 3562 , Loss = 0.12500000236732153\n",
            "\n",
            "Iteration 3563 , Loss = 0.12500000235558148\n",
            "\n",
            "Iteration 3564 , Loss = 0.12500000234390013\n",
            "\n",
            "Iteration 3565 , Loss = 0.12500000233227718\n",
            "\n",
            "Iteration 3566 , Loss = 0.12500000232071232\n",
            "\n",
            "Iteration 3567 , Loss = 0.12500000230920524\n",
            "\n",
            "Iteration 3568 , Loss = 0.1250000022977557\n",
            "\n",
            "Iteration 3569 , Loss = 0.12500000228636343\n",
            "\n",
            "Iteration 3570 , Loss = 0.12500000227502808\n",
            "\n",
            "Iteration 3571 , Loss = 0.12500000226374935\n",
            "\n",
            "Iteration 3572 , Loss = 0.12500000225252703\n",
            "\n",
            "Iteration 3573 , Loss = 0.12500000224136082\n",
            "\n",
            "Iteration 3574 , Loss = 0.12500000223025046\n",
            "\n",
            "Iteration 3575 , Loss = 0.1250000022191956\n",
            "\n",
            "Iteration 3576 , Loss = 0.12500000220819601\n",
            "\n",
            "Iteration 3577 , Loss = 0.12500000219725138\n",
            "\n",
            "Iteration 3578 , Loss = 0.12500000218636145\n",
            "\n",
            "Iteration 3579 , Loss = 0.12500000217552604\n",
            "\n",
            "Iteration 3580 , Loss = 0.12500000216474472\n",
            "\n",
            "Iteration 3581 , Loss = 0.1250000021540173\n",
            "\n",
            "Iteration 3582 , Loss = 0.12500000214334353\n",
            "\n",
            "Iteration 3583 , Loss = 0.12500000213272308\n",
            "\n",
            "Iteration 3584 , Loss = 0.12500000212215576\n",
            "\n",
            "Iteration 3585 , Loss = 0.12500000211164125\n",
            "\n",
            "Iteration 3586 , Loss = 0.1250000021011793\n",
            "\n",
            "Iteration 3587 , Loss = 0.12500000209076964\n",
            "\n",
            "Iteration 3588 , Loss = 0.12500000208041204\n",
            "\n",
            "Iteration 3589 , Loss = 0.12500000207010617\n",
            "\n",
            "Iteration 3590 , Loss = 0.12500000205985184\n",
            "\n",
            "Iteration 3591 , Loss = 0.12500000204964878\n",
            "\n",
            "Iteration 3592 , Loss = 0.1250000020394967\n",
            "\n",
            "Iteration 3593 , Loss = 0.12500000202939537\n",
            "\n",
            "Iteration 3594 , Loss = 0.1250000020193446\n",
            "\n",
            "Iteration 3595 , Loss = 0.125000002009344\n",
            "\n",
            "Iteration 3596 , Loss = 0.12500000199939343\n",
            "\n",
            "Iteration 3597 , Loss = 0.12500000198949257\n",
            "\n",
            "Iteration 3598 , Loss = 0.12500000197964123\n",
            "\n",
            "Iteration 3599 , Loss = 0.12500000196983913\n",
            "\n",
            "Iteration 3600 , Loss = 0.125000001960086\n",
            "\n",
            "Iteration 3601 , Loss = 0.12500000195038163\n",
            "\n",
            "Iteration 3602 , Loss = 0.1250000019407258\n",
            "\n",
            "Iteration 3603 , Loss = 0.12500000193111824\n",
            "\n",
            "Iteration 3604 , Loss = 0.12500000192155866\n",
            "\n",
            "Iteration 3605 , Loss = 0.1250000019120469\n",
            "\n",
            "Iteration 3606 , Loss = 0.1250000019025827\n",
            "\n",
            "Iteration 3607 , Loss = 0.1250000018931658\n",
            "\n",
            "Iteration 3608 , Loss = 0.12500000188379595\n",
            "\n",
            "Iteration 3609 , Loss = 0.12500000187447297\n",
            "\n",
            "Iteration 3610 , Loss = 0.12500000186519658\n",
            "\n",
            "Iteration 3611 , Loss = 0.12500000185596655\n",
            "\n",
            "Iteration 3612 , Loss = 0.1250000018467827\n",
            "\n",
            "Iteration 3613 , Loss = 0.1250000018376447\n",
            "\n",
            "Iteration 3614 , Loss = 0.12500000182855245\n",
            "\n",
            "Iteration 3615 , Loss = 0.1250000018195056\n",
            "\n",
            "Iteration 3616 , Loss = 0.12500000181050397\n",
            "\n",
            "Iteration 3617 , Loss = 0.12500000180154736\n",
            "\n",
            "Iteration 3618 , Loss = 0.1250000017926355\n",
            "\n",
            "Iteration 3619 , Loss = 0.12500000178376822\n",
            "\n",
            "Iteration 3620 , Loss = 0.12500000177494527\n",
            "\n",
            "Iteration 3621 , Loss = 0.1250000017661664\n",
            "\n",
            "Iteration 3622 , Loss = 0.12500000175743142\n",
            "\n",
            "Iteration 3623 , Loss = 0.1250000017487401\n",
            "\n",
            "Iteration 3624 , Loss = 0.12500000174009224\n",
            "\n",
            "Iteration 3625 , Loss = 0.1250000017314876\n",
            "\n",
            "Iteration 3626 , Loss = 0.12500000172292597\n",
            "\n",
            "Iteration 3627 , Loss = 0.12500000171440717\n",
            "\n",
            "Iteration 3628 , Loss = 0.1250000017059309\n",
            "\n",
            "Iteration 3629 , Loss = 0.12500000169749706\n",
            "\n",
            "Iteration 3630 , Loss = 0.12500000168910536\n",
            "\n",
            "Iteration 3631 , Loss = 0.1250000016807556\n",
            "\n",
            "Iteration 3632 , Loss = 0.12500000167244754\n",
            "\n",
            "Iteration 3633 , Loss = 0.12500000166418107\n",
            "\n",
            "Iteration 3634 , Loss = 0.1250000016559559\n",
            "\n",
            "Iteration 3635 , Loss = 0.12500000164777186\n",
            "\n",
            "Iteration 3636 , Loss = 0.1250000016396287\n",
            "\n",
            "Iteration 3637 , Loss = 0.1250000016315263\n",
            "\n",
            "Iteration 3638 , Loss = 0.12500000162346436\n",
            "\n",
            "Iteration 3639 , Loss = 0.12500000161544272\n",
            "\n",
            "Iteration 3640 , Loss = 0.12500000160746122\n",
            "\n",
            "Iteration 3641 , Loss = 0.12500000159951957\n",
            "\n",
            "Iteration 3642 , Loss = 0.12500000159161767\n",
            "\n",
            "Iteration 3643 , Loss = 0.12500000158375527\n",
            "\n",
            "Iteration 3644 , Loss = 0.1250000015759321\n",
            "\n",
            "Iteration 3645 , Loss = 0.1250000015681481\n",
            "\n",
            "Iteration 3646 , Loss = 0.125000001560403\n",
            "\n",
            "Iteration 3647 , Loss = 0.12500000155269664\n",
            "\n",
            "Iteration 3648 , Loss = 0.12500000154502874\n",
            "\n",
            "Iteration 3649 , Loss = 0.12500000153739924\n",
            "\n",
            "Iteration 3650 , Loss = 0.1250000015298078\n",
            "\n",
            "Iteration 3651 , Loss = 0.12500000152225438\n",
            "\n",
            "Iteration 3652 , Loss = 0.1250000015147387\n",
            "\n",
            "Iteration 3653 , Loss = 0.1250000015072606\n",
            "\n",
            "Iteration 3654 , Loss = 0.12500000149981982\n",
            "\n",
            "Iteration 3655 , Loss = 0.1250000014924163\n",
            "\n",
            "Iteration 3656 , Loss = 0.1250000014850498\n",
            "\n",
            "Iteration 3657 , Loss = 0.12500000147772006\n",
            "\n",
            "Iteration 3658 , Loss = 0.12500000147042703\n",
            "\n",
            "Iteration 3659 , Loss = 0.12500000146317045\n",
            "\n",
            "Iteration 3660 , Loss = 0.12500000145595008\n",
            "\n",
            "Iteration 3661 , Loss = 0.12500000144876586\n",
            "\n",
            "Iteration 3662 , Loss = 0.12500000144161755\n",
            "\n",
            "Iteration 3663 , Loss = 0.12500000143450496\n",
            "\n",
            "Iteration 3664 , Loss = 0.12500000142742793\n",
            "\n",
            "Iteration 3665 , Loss = 0.12500000142038628\n",
            "\n",
            "Iteration 3666 , Loss = 0.1250000014133798\n",
            "\n",
            "Iteration 3667 , Loss = 0.1250000014064084\n",
            "\n",
            "Iteration 3668 , Loss = 0.12500000139947182\n",
            "\n",
            "Iteration 3669 , Loss = 0.12500000139256992\n",
            "\n",
            "Iteration 3670 , Loss = 0.12500000138570252\n",
            "\n",
            "Iteration 3671 , Loss = 0.12500000137886946\n",
            "\n",
            "Iteration 3672 , Loss = 0.12500000137207057\n",
            "\n",
            "Iteration 3673 , Loss = 0.12500000136530562\n",
            "\n",
            "Iteration 3674 , Loss = 0.12500000135857453\n",
            "\n",
            "Iteration 3675 , Loss = 0.12500000135187708\n",
            "\n",
            "Iteration 3676 , Loss = 0.1250000013452131\n",
            "\n",
            "Iteration 3677 , Loss = 0.12500000133858244\n",
            "\n",
            "Iteration 3678 , Loss = 0.12500000133198494\n",
            "\n",
            "Iteration 3679 , Loss = 0.12500000132542038\n",
            "\n",
            "Iteration 3680 , Loss = 0.12500000131888866\n",
            "\n",
            "Iteration 3681 , Loss = 0.1250000013123896\n",
            "\n",
            "Iteration 3682 , Loss = 0.12500000130592304\n",
            "\n",
            "Iteration 3683 , Loss = 0.12500000129948877\n",
            "\n",
            "Iteration 3684 , Loss = 0.1250000012930867\n",
            "\n",
            "Iteration 3685 , Loss = 0.1250000012867166\n",
            "\n",
            "Iteration 3686 , Loss = 0.12500000128037836\n",
            "\n",
            "Iteration 3687 , Loss = 0.1250000012740718\n",
            "\n",
            "Iteration 3688 , Loss = 0.1250000012677968\n",
            "\n",
            "Iteration 3689 , Loss = 0.12500000126155314\n",
            "\n",
            "Iteration 3690 , Loss = 0.1250000012553407\n",
            "\n",
            "Iteration 3691 , Loss = 0.12500000124915928\n",
            "\n",
            "Iteration 3692 , Loss = 0.1250000012430088\n",
            "\n",
            "Iteration 3693 , Loss = 0.12500000123688906\n",
            "\n",
            "Iteration 3694 , Loss = 0.12500000123079993\n",
            "\n",
            "Iteration 3695 , Loss = 0.12500000122474123\n",
            "\n",
            "Iteration 3696 , Loss = 0.1250000012187128\n",
            "\n",
            "Iteration 3697 , Loss = 0.12500000121271448\n",
            "\n",
            "Iteration 3698 , Loss = 0.1250000012067462\n",
            "\n",
            "Iteration 3699 , Loss = 0.12500000120080776\n",
            "\n",
            "Iteration 3700 , Loss = 0.12500000119489896\n",
            "\n",
            "Iteration 3701 , Loss = 0.12500000118901972\n",
            "\n",
            "Iteration 3702 , Loss = 0.1250000011831699\n",
            "\n",
            "Iteration 3703 , Loss = 0.1250000011773493\n",
            "\n",
            "Iteration 3704 , Loss = 0.12500000117155777\n",
            "\n",
            "Iteration 3705 , Loss = 0.12500000116579524\n",
            "\n",
            "Iteration 3706 , Loss = 0.12500000116006146\n",
            "\n",
            "Iteration 3707 , Loss = 0.1250000011543564\n",
            "\n",
            "Iteration 3708 , Loss = 0.12500000114867982\n",
            "\n",
            "Iteration 3709 , Loss = 0.12500000114303164\n",
            "\n",
            "Iteration 3710 , Loss = 0.1250000011374117\n",
            "\n",
            "Iteration 3711 , Loss = 0.12500000113181983\n",
            "\n",
            "Iteration 3712 , Loss = 0.12500000112625595\n",
            "\n",
            "Iteration 3713 , Loss = 0.12500000112071985\n",
            "\n",
            "Iteration 3714 , Loss = 0.12500000111521142\n",
            "\n",
            "Iteration 3715 , Loss = 0.12500000110973056\n",
            "\n",
            "Iteration 3716 , Loss = 0.12500000110427706\n",
            "\n",
            "Iteration 3717 , Loss = 0.12500000109885087\n",
            "\n",
            "Iteration 3718 , Loss = 0.12500000109345177\n",
            "\n",
            "Iteration 3719 , Loss = 0.12500000108807968\n",
            "\n",
            "Iteration 3720 , Loss = 0.12500000108273446\n",
            "\n",
            "Iteration 3721 , Loss = 0.12500000107741593\n",
            "\n",
            "Iteration 3722 , Loss = 0.125000001072124\n",
            "\n",
            "Iteration 3723 , Loss = 0.12500000106685852\n",
            "\n",
            "Iteration 3724 , Loss = 0.12500000106161935\n",
            "\n",
            "Iteration 3725 , Loss = 0.1250000010564064\n",
            "\n",
            "Iteration 3726 , Loss = 0.12500000105121947\n",
            "\n",
            "Iteration 3727 , Loss = 0.12500000104605852\n",
            "\n",
            "Iteration 3728 , Loss = 0.12500000104092335\n",
            "\n",
            "Iteration 3729 , Loss = 0.12500000103581385\n",
            "\n",
            "Iteration 3730 , Loss = 0.1250000010307299\n",
            "\n",
            "Iteration 3731 , Loss = 0.12500000102567135\n",
            "\n",
            "Iteration 3732 , Loss = 0.1250000010206381\n",
            "\n",
            "Iteration 3733 , Loss = 0.12500000101563\n",
            "\n",
            "Iteration 3734 , Loss = 0.12500000101064696\n",
            "\n",
            "Iteration 3735 , Loss = 0.12500000100568878\n",
            "\n",
            "Iteration 3736 , Loss = 0.12500000100075542\n",
            "\n",
            "Iteration 3737 , Loss = 0.12500000099584677\n",
            "\n",
            "Iteration 3738 , Loss = 0.1250000009909626\n",
            "\n",
            "Iteration 3739 , Loss = 0.12500000098610284\n",
            "\n",
            "Iteration 3740 , Loss = 0.1250000009812674\n",
            "\n",
            "Iteration 3741 , Loss = 0.12500000097645614\n",
            "\n",
            "Iteration 3742 , Loss = 0.1250000009716689\n",
            "\n",
            "Iteration 3743 , Loss = 0.12500000096690564\n",
            "\n",
            "Iteration 3744 , Loss = 0.12500000096216615\n",
            "\n",
            "Iteration 3745 , Loss = 0.1250000009574504\n",
            "\n",
            "Iteration 3746 , Loss = 0.12500000095275818\n",
            "\n",
            "Iteration 3747 , Loss = 0.1250000009480894\n",
            "\n",
            "Iteration 3748 , Loss = 0.125000000943444\n",
            "\n",
            "Iteration 3749 , Loss = 0.12500000093882183\n",
            "\n",
            "Iteration 3750 , Loss = 0.12500000093422273\n",
            "\n",
            "Iteration 3751 , Loss = 0.12500000092964664\n",
            "\n",
            "Iteration 3752 , Loss = 0.12500000092509345\n",
            "\n",
            "Iteration 3753 , Loss = 0.125000000920563\n",
            "\n",
            "Iteration 3754 , Loss = 0.1250000009160552\n",
            "\n",
            "Iteration 3755 , Loss = 0.12500000091156993\n",
            "\n",
            "Iteration 3756 , Loss = 0.1250000009071071\n",
            "\n",
            "Iteration 3757 , Loss = 0.12500000090266655\n",
            "\n",
            "Iteration 3758 , Loss = 0.12500000089824823\n",
            "\n",
            "Iteration 3759 , Loss = 0.125000000893852\n",
            "\n",
            "Iteration 3760 , Loss = 0.1250000008894777\n",
            "\n",
            "Iteration 3761 , Loss = 0.1250000008851253\n",
            "\n",
            "Iteration 3762 , Loss = 0.12500000088079466\n",
            "\n",
            "Iteration 3763 , Loss = 0.1250000008764857\n",
            "\n",
            "Iteration 3764 , Loss = 0.12500000087219823\n",
            "\n",
            "Iteration 3765 , Loss = 0.1250000008679322\n",
            "\n",
            "Iteration 3766 , Loss = 0.12500000086368754\n",
            "\n",
            "Iteration 3767 , Loss = 0.12500000085946408\n",
            "\n",
            "Iteration 3768 , Loss = 0.12500000085526172\n",
            "\n",
            "Iteration 3769 , Loss = 0.12500000085108037\n",
            "\n",
            "Iteration 3770 , Loss = 0.12500000084691995\n",
            "\n",
            "Iteration 3771 , Loss = 0.1250000008427803\n",
            "\n",
            "Iteration 3772 , Loss = 0.12500000083866136\n",
            "\n",
            "Iteration 3773 , Loss = 0.12500000083456303\n",
            "\n",
            "Iteration 3774 , Loss = 0.12500000083048513\n",
            "\n",
            "Iteration 3775 , Loss = 0.12500000082642762\n",
            "\n",
            "Iteration 3776 , Loss = 0.12500000082239043\n",
            "\n",
            "Iteration 3777 , Loss = 0.12500000081837345\n",
            "\n",
            "Iteration 3778 , Loss = 0.1250000008143765\n",
            "\n",
            "Iteration 3779 , Loss = 0.12500000081039955\n",
            "\n",
            "Iteration 3780 , Loss = 0.1250000008064425\n",
            "\n",
            "Iteration 3781 , Loss = 0.1250000008025052\n",
            "\n",
            "Iteration 3782 , Loss = 0.1250000007985876\n",
            "\n",
            "Iteration 3783 , Loss = 0.12500000079468962\n",
            "\n",
            "Iteration 3784 , Loss = 0.12500000079081103\n",
            "\n",
            "Iteration 3785 , Loss = 0.12500000078695192\n",
            "\n",
            "Iteration 3786 , Loss = 0.12500000078311208\n",
            "\n",
            "Iteration 3787 , Loss = 0.12500000077929144\n",
            "\n",
            "Iteration 3788 , Loss = 0.12500000077548987\n",
            "\n",
            "Iteration 3789 , Loss = 0.12500000077170736\n",
            "\n",
            "Iteration 3790 , Loss = 0.1250000007679437\n",
            "\n",
            "Iteration 3791 , Loss = 0.1250000007641989\n",
            "\n",
            "Iteration 3792 , Loss = 0.1250000007604728\n",
            "\n",
            "Iteration 3793 , Loss = 0.12500000075676532\n",
            "\n",
            "Iteration 3794 , Loss = 0.12500000075307638\n",
            "\n",
            "Iteration 3795 , Loss = 0.1250000007494059\n",
            "\n",
            "Iteration 3796 , Loss = 0.12500000074575376\n",
            "\n",
            "Iteration 3797 , Loss = 0.1250000007421199\n",
            "\n",
            "Iteration 3798 , Loss = 0.12500000073850417\n",
            "\n",
            "Iteration 3799 , Loss = 0.12500000073490652\n",
            "\n",
            "Iteration 3800 , Loss = 0.1250000007313269\n",
            "\n",
            "Iteration 3801 , Loss = 0.12500000072776515\n",
            "\n",
            "Iteration 3802 , Loss = 0.12500000072422116\n",
            "\n",
            "Iteration 3803 , Loss = 0.12500000072069495\n",
            "\n",
            "Iteration 3804 , Loss = 0.12500000071718634\n",
            "\n",
            "Iteration 3805 , Loss = 0.12500000071369527\n",
            "\n",
            "Iteration 3806 , Loss = 0.12500000071022166\n",
            "\n",
            "Iteration 3807 , Loss = 0.12500000070676542\n",
            "\n",
            "Iteration 3808 , Loss = 0.12500000070332645\n",
            "\n",
            "Iteration 3809 , Loss = 0.1250000006999047\n",
            "\n",
            "Iteration 3810 , Loss = 0.12500000069650002\n",
            "\n",
            "Iteration 3811 , Loss = 0.12500000069311237\n",
            "\n",
            "Iteration 3812 , Loss = 0.12500000068974165\n",
            "\n",
            "Iteration 3813 , Loss = 0.12500000068638778\n",
            "\n",
            "Iteration 3814 , Loss = 0.12500000068305073\n",
            "\n",
            "Iteration 3815 , Loss = 0.12500000067973033\n",
            "\n",
            "Iteration 3816 , Loss = 0.1250000006764265\n",
            "\n",
            "Iteration 3817 , Loss = 0.1250000006731392\n",
            "\n",
            "Iteration 3818 , Loss = 0.12500000066986838\n",
            "\n",
            "Iteration 3819 , Loss = 0.12500000066661388\n",
            "\n",
            "Iteration 3820 , Loss = 0.12500000066337563\n",
            "\n",
            "Iteration 3821 , Loss = 0.1250000006601536\n",
            "\n",
            "Iteration 3822 , Loss = 0.12500000065694766\n",
            "\n",
            "Iteration 3823 , Loss = 0.12500000065375777\n",
            "\n",
            "Iteration 3824 , Loss = 0.1250000006505838\n",
            "\n",
            "Iteration 3825 , Loss = 0.1250000006474257\n",
            "\n",
            "Iteration 3826 , Loss = 0.12500000064428338\n",
            "\n",
            "Iteration 3827 , Loss = 0.12500000064115682\n",
            "\n",
            "Iteration 3828 , Loss = 0.1250000006380459\n",
            "\n",
            "Iteration 3829 , Loss = 0.12500000063495048\n",
            "\n",
            "Iteration 3830 , Loss = 0.12500000063187053\n",
            "\n",
            "Iteration 3831 , Loss = 0.125000000628806\n",
            "\n",
            "Iteration 3832 , Loss = 0.12500000062575678\n",
            "\n",
            "Iteration 3833 , Loss = 0.12500000062272282\n",
            "\n",
            "Iteration 3834 , Loss = 0.125000000619704\n",
            "\n",
            "Iteration 3835 , Loss = 0.1250000006167003\n",
            "\n",
            "Iteration 3836 , Loss = 0.12500000061371164\n",
            "\n",
            "Iteration 3837 , Loss = 0.12500000061073785\n",
            "\n",
            "Iteration 3838 , Loss = 0.125000000607779\n",
            "\n",
            "Iteration 3839 , Loss = 0.1250000006048349\n",
            "\n",
            "Iteration 3840 , Loss = 0.12500000060190553\n",
            "\n",
            "Iteration 3841 , Loss = 0.1250000005989908\n",
            "\n",
            "Iteration 3842 , Loss = 0.12500000059609065\n",
            "\n",
            "Iteration 3843 , Loss = 0.12500000059320504\n",
            "\n",
            "Iteration 3844 , Loss = 0.12500000059033378\n",
            "\n",
            "Iteration 3845 , Loss = 0.12500000058747693\n",
            "\n",
            "Iteration 3846 , Loss = 0.1250000005846343\n",
            "\n",
            "Iteration 3847 , Loss = 0.12500000058180594\n",
            "\n",
            "Iteration 3848 , Loss = 0.12500000057899172\n",
            "\n",
            "Iteration 3849 , Loss = 0.12500000057619154\n",
            "\n",
            "Iteration 3850 , Loss = 0.12500000057340538\n",
            "\n",
            "Iteration 3851 , Loss = 0.12500000057063315\n",
            "\n",
            "Iteration 3852 , Loss = 0.12500000056787477\n",
            "\n",
            "Iteration 3853 , Loss = 0.1250000005651302\n",
            "\n",
            "Iteration 3854 , Loss = 0.12500000056239932\n",
            "\n",
            "Iteration 3855 , Loss = 0.12500000055968213\n",
            "\n",
            "Iteration 3856 , Loss = 0.1250000005569785\n",
            "\n",
            "Iteration 3857 , Loss = 0.12500000055428837\n",
            "\n",
            "Iteration 3858 , Loss = 0.12500000055161173\n",
            "\n",
            "Iteration 3859 , Loss = 0.12500000054894844\n",
            "\n",
            "Iteration 3860 , Loss = 0.1250000005462985\n",
            "\n",
            "Iteration 3861 , Loss = 0.12500000054366178\n",
            "\n",
            "Iteration 3862 , Loss = 0.12500000054103827\n",
            "\n",
            "Iteration 3863 , Loss = 0.12500000053842786\n",
            "\n",
            "Iteration 3864 , Loss = 0.12500000053583046\n",
            "\n",
            "Iteration 3865 , Loss = 0.12500000053324611\n",
            "\n",
            "Iteration 3866 , Loss = 0.12500000053067467\n",
            "\n",
            "Iteration 3867 , Loss = 0.12500000052811605\n",
            "\n",
            "Iteration 3868 , Loss = 0.12500000052557025\n",
            "\n",
            "Iteration 3869 , Loss = 0.1250000005230372\n",
            "\n",
            "Iteration 3870 , Loss = 0.12500000052051674\n",
            "\n",
            "Iteration 3871 , Loss = 0.12500000051800894\n",
            "\n",
            "Iteration 3872 , Loss = 0.12500000051551366\n",
            "\n",
            "Iteration 3873 , Loss = 0.12500000051303084\n",
            "\n",
            "Iteration 3874 , Loss = 0.12500000051056048\n",
            "\n",
            "Iteration 3875 , Loss = 0.12500000050810245\n",
            "\n",
            "Iteration 3876 , Loss = 0.1250000005056567\n",
            "\n",
            "Iteration 3877 , Loss = 0.12500000050322319\n",
            "\n",
            "Iteration 3878 , Loss = 0.12500000050080184\n",
            "\n",
            "Iteration 3879 , Loss = 0.12500000049839258\n",
            "\n",
            "Iteration 3880 , Loss = 0.1250000004959954\n",
            "\n",
            "Iteration 3881 , Loss = 0.12500000049361018\n",
            "\n",
            "Iteration 3882 , Loss = 0.12500000049123688\n",
            "\n",
            "Iteration 3883 , Loss = 0.12500000048887547\n",
            "\n",
            "Iteration 3884 , Loss = 0.12500000048652585\n",
            "\n",
            "Iteration 3885 , Loss = 0.125000000484188\n",
            "\n",
            "Iteration 3886 , Loss = 0.1250000004818618\n",
            "\n",
            "Iteration 3887 , Loss = 0.12500000047954724\n",
            "\n",
            "Iteration 3888 , Loss = 0.12500000047724427\n",
            "\n",
            "Iteration 3889 , Loss = 0.1250000004749528\n",
            "\n",
            "Iteration 3890 , Loss = 0.1250000004726728\n",
            "\n",
            "Iteration 3891 , Loss = 0.12500000047040422\n",
            "\n",
            "Iteration 3892 , Loss = 0.1250000004681469\n",
            "\n",
            "Iteration 3893 , Loss = 0.12500000046590096\n",
            "\n",
            "Iteration 3894 , Loss = 0.12500000046366622\n",
            "\n",
            "Iteration 3895 , Loss = 0.1250000004614426\n",
            "\n",
            "Iteration 3896 , Loss = 0.12500000045923018\n",
            "\n",
            "Iteration 3897 , Loss = 0.12500000045702878\n",
            "\n",
            "Iteration 3898 , Loss = 0.1250000004548384\n",
            "\n",
            "Iteration 3899 , Loss = 0.12500000045265897\n",
            "\n",
            "Iteration 3900 , Loss = 0.12500000045049042\n",
            "\n",
            "Iteration 3901 , Loss = 0.1250000004483327\n",
            "\n",
            "Iteration 3902 , Loss = 0.1250000004461858\n",
            "\n",
            "Iteration 3903 , Loss = 0.1250000004440496\n",
            "\n",
            "Iteration 3904 , Loss = 0.1250000004419241\n",
            "\n",
            "Iteration 3905 , Loss = 0.12500000043980924\n",
            "\n",
            "Iteration 3906 , Loss = 0.12500000043770496\n",
            "\n",
            "Iteration 3907 , Loss = 0.12500000043561119\n",
            "\n",
            "Iteration 3908 , Loss = 0.12500000043352788\n",
            "\n",
            "Iteration 3909 , Loss = 0.12500000043145498\n",
            "\n",
            "Iteration 3910 , Loss = 0.12500000042939247\n",
            "\n",
            "Iteration 3911 , Loss = 0.12500000042734027\n",
            "\n",
            "Iteration 3912 , Loss = 0.1250000004252983\n",
            "\n",
            "Iteration 3913 , Loss = 0.12500000042326656\n",
            "\n",
            "Iteration 3914 , Loss = 0.12500000042124498\n",
            "\n",
            "Iteration 3915 , Loss = 0.1250000004192335\n",
            "\n",
            "Iteration 3916 , Loss = 0.12500000041723208\n",
            "\n",
            "Iteration 3917 , Loss = 0.12500000041524068\n",
            "\n",
            "Iteration 3918 , Loss = 0.1250000004132592\n",
            "\n",
            "Iteration 3919 , Loss = 0.12500000041128767\n",
            "\n",
            "Iteration 3920 , Loss = 0.125000000409326\n",
            "\n",
            "Iteration 3921 , Loss = 0.1250000004073741\n",
            "\n",
            "Iteration 3922 , Loss = 0.12500000040543197\n",
            "\n",
            "Iteration 3923 , Loss = 0.12500000040349954\n",
            "\n",
            "Iteration 3924 , Loss = 0.1250000004015768\n",
            "\n",
            "Iteration 3925 , Loss = 0.12500000039966366\n",
            "\n",
            "Iteration 3926 , Loss = 0.1250000003977601\n",
            "\n",
            "Iteration 3927 , Loss = 0.12500000039586606\n",
            "\n",
            "Iteration 3928 , Loss = 0.12500000039398146\n",
            "\n",
            "Iteration 3929 , Loss = 0.1250000003921063\n",
            "\n",
            "Iteration 3930 , Loss = 0.1250000003902405\n",
            "\n",
            "Iteration 3931 , Loss = 0.12500000038838405\n",
            "\n",
            "Iteration 3932 , Loss = 0.12500000038653689\n",
            "\n",
            "Iteration 3933 , Loss = 0.12500000038469894\n",
            "\n",
            "Iteration 3934 , Loss = 0.12500000038287018\n",
            "\n",
            "Iteration 3935 , Loss = 0.12500000038105058\n",
            "\n",
            "Iteration 3936 , Loss = 0.12500000037924008\n",
            "\n",
            "Iteration 3937 , Loss = 0.12500000037743864\n",
            "\n",
            "Iteration 3938 , Loss = 0.1250000003756462\n",
            "\n",
            "Iteration 3939 , Loss = 0.12500000037386272\n",
            "\n",
            "Iteration 3940 , Loss = 0.12500000037208814\n",
            "\n",
            "Iteration 3941 , Loss = 0.12500000037032247\n",
            "\n",
            "Iteration 3942 , Loss = 0.12500000036856557\n",
            "\n",
            "Iteration 3943 , Loss = 0.12500000036681752\n",
            "\n",
            "Iteration 3944 , Loss = 0.12500000036507816\n",
            "\n",
            "Iteration 3945 , Loss = 0.12500000036334752\n",
            "\n",
            "Iteration 3946 , Loss = 0.12500000036162554\n",
            "\n",
            "Iteration 3947 , Loss = 0.12500000035991216\n",
            "\n",
            "Iteration 3948 , Loss = 0.12500000035820735\n",
            "\n",
            "Iteration 3949 , Loss = 0.12500000035651104\n",
            "\n",
            "Iteration 3950 , Loss = 0.12500000035482323\n",
            "\n",
            "Iteration 3951 , Loss = 0.12500000035314388\n",
            "\n",
            "Iteration 3952 , Loss = 0.1250000003514729\n",
            "\n",
            "Iteration 3953 , Loss = 0.12500000034981032\n",
            "\n",
            "Iteration 3954 , Loss = 0.125000000348156\n",
            "\n",
            "Iteration 3955 , Loss = 0.12500000034650996\n",
            "\n",
            "Iteration 3956 , Loss = 0.12500000034487216\n",
            "\n",
            "Iteration 3957 , Loss = 0.12500000034324255\n",
            "\n",
            "Iteration 3958 , Loss = 0.1250000003416211\n",
            "\n",
            "Iteration 3959 , Loss = 0.12500000034000774\n",
            "\n",
            "Iteration 3960 , Loss = 0.12500000033840247\n",
            "\n",
            "Iteration 3961 , Loss = 0.1250000003368052\n",
            "\n",
            "Iteration 3962 , Loss = 0.12500000033521594\n",
            "\n",
            "Iteration 3963 , Loss = 0.1250000003336346\n",
            "\n",
            "Iteration 3964 , Loss = 0.12500000033206116\n",
            "\n",
            "Iteration 3965 , Loss = 0.12500000033049563\n",
            "\n",
            "Iteration 3966 , Loss = 0.1250000003289379\n",
            "\n",
            "Iteration 3967 , Loss = 0.12500000032738798\n",
            "\n",
            "Iteration 3968 , Loss = 0.12500000032584577\n",
            "\n",
            "Iteration 3969 , Loss = 0.1250000003243113\n",
            "\n",
            "Iteration 3970 , Loss = 0.1250000003227845\n",
            "\n",
            "Iteration 3971 , Loss = 0.12500000032126535\n",
            "\n",
            "Iteration 3972 , Loss = 0.12500000031975378\n",
            "\n",
            "Iteration 3973 , Loss = 0.12500000031824976\n",
            "\n",
            "Iteration 3974 , Loss = 0.12500000031675326\n",
            "\n",
            "Iteration 3975 , Loss = 0.12500000031526426\n",
            "\n",
            "Iteration 3976 , Loss = 0.12500000031378267\n",
            "\n",
            "Iteration 3977 , Loss = 0.12500000031230854\n",
            "\n",
            "Iteration 3978 , Loss = 0.12500000031084174\n",
            "\n",
            "Iteration 3979 , Loss = 0.12500000030938227\n",
            "\n",
            "Iteration 3980 , Loss = 0.12500000030793015\n",
            "\n",
            "Iteration 3981 , Loss = 0.12500000030648525\n",
            "\n",
            "Iteration 3982 , Loss = 0.1250000003050476\n",
            "\n",
            "Iteration 3983 , Loss = 0.1250000003036171\n",
            "\n",
            "Iteration 3984 , Loss = 0.1250000003021938\n",
            "\n",
            "Iteration 3985 , Loss = 0.12500000030077757\n",
            "\n",
            "Iteration 3986 , Loss = 0.12500000029936845\n",
            "\n",
            "Iteration 3987 , Loss = 0.12500000029796637\n",
            "\n",
            "Iteration 3988 , Loss = 0.1250000002965713\n",
            "\n",
            "Iteration 3989 , Loss = 0.1250000002951832\n",
            "\n",
            "Iteration 3990 , Loss = 0.12500000029380207\n",
            "\n",
            "Iteration 3991 , Loss = 0.12500000029242783\n",
            "\n",
            "Iteration 3992 , Loss = 0.12500000029106045\n",
            "\n",
            "Iteration 3993 , Loss = 0.1250000002896999\n",
            "\n",
            "Iteration 3994 , Loss = 0.12500000028834618\n",
            "\n",
            "Iteration 3995 , Loss = 0.1250000002869992\n",
            "\n",
            "Iteration 3996 , Loss = 0.125000000285659\n",
            "\n",
            "Iteration 3997 , Loss = 0.12500000028432545\n",
            "\n",
            "Iteration 3998 , Loss = 0.12500000028299862\n",
            "\n",
            "Iteration 3999 , Loss = 0.1250000002816784\n",
            "\n",
            "Iteration 4000 , Loss = 0.12500000028036476\n",
            "\n",
            "Iteration 4001 , Loss = 0.1250000002790577\n",
            "\n",
            "Iteration 4002 , Loss = 0.1250000002777572\n",
            "\n",
            "Iteration 4003 , Loss = 0.12500000027646316\n",
            "\n",
            "Iteration 4004 , Loss = 0.12500000027517563\n",
            "\n",
            "Iteration 4005 , Loss = 0.12500000027389455\n",
            "\n",
            "Iteration 4006 , Loss = 0.12500000027261984\n",
            "\n",
            "Iteration 4007 , Loss = 0.12500000027135152\n",
            "\n",
            "Iteration 4008 , Loss = 0.12500000027008956\n",
            "\n",
            "Iteration 4009 , Loss = 0.12500000026883387\n",
            "\n",
            "Iteration 4010 , Loss = 0.12500000026758448\n",
            "\n",
            "Iteration 4011 , Loss = 0.12500000026634134\n",
            "\n",
            "Iteration 4012 , Loss = 0.1250000002651044\n",
            "\n",
            "Iteration 4013 , Loss = 0.12500000026387367\n",
            "\n",
            "Iteration 4014 , Loss = 0.12500000026264907\n",
            "\n",
            "Iteration 4015 , Loss = 0.12500000026143063\n",
            "\n",
            "Iteration 4016 , Loss = 0.12500000026021824\n",
            "\n",
            "Iteration 4017 , Loss = 0.12500000025901192\n",
            "\n",
            "Iteration 4018 , Loss = 0.12500000025781166\n",
            "\n",
            "Iteration 4019 , Loss = 0.12500000025661737\n",
            "\n",
            "Iteration 4020 , Loss = 0.1250000002554291\n",
            "\n",
            "Iteration 4021 , Loss = 0.12500000025424673\n",
            "\n",
            "Iteration 4022 , Loss = 0.1250000002530703\n",
            "\n",
            "Iteration 4023 , Loss = 0.12500000025189972\n",
            "\n",
            "Iteration 4024 , Loss = 0.12500000025073502\n",
            "\n",
            "Iteration 4025 , Loss = 0.12500000024957614\n",
            "\n",
            "Iteration 4026 , Loss = 0.12500000024842303\n",
            "\n",
            "Iteration 4027 , Loss = 0.1250000002472757\n",
            "\n",
            "Iteration 4028 , Loss = 0.1250000002461341\n",
            "\n",
            "Iteration 4029 , Loss = 0.12500000024499824\n",
            "\n",
            "Iteration 4030 , Loss = 0.12500000024386804\n",
            "\n",
            "Iteration 4031 , Loss = 0.12500000024274352\n",
            "\n",
            "Iteration 4032 , Loss = 0.12500000024162458\n",
            "\n",
            "Iteration 4033 , Loss = 0.12500000024051125\n",
            "\n",
            "Iteration 4034 , Loss = 0.1250000002394035\n",
            "\n",
            "Iteration 4035 , Loss = 0.12500000023830127\n",
            "\n",
            "Iteration 4036 , Loss = 0.12500000023720456\n",
            "\n",
            "Iteration 4037 , Loss = 0.12500000023611332\n",
            "\n",
            "Iteration 4038 , Loss = 0.12500000023502758\n",
            "\n",
            "Iteration 4039 , Loss = 0.12500000023394725\n",
            "\n",
            "Iteration 4040 , Loss = 0.1250000002328723\n",
            "\n",
            "Iteration 4041 , Loss = 0.12500000023180274\n",
            "\n",
            "Iteration 4042 , Loss = 0.12500000023073857\n",
            "\n",
            "Iteration 4043 , Loss = 0.12500000022967966\n",
            "\n",
            "Iteration 4044 , Loss = 0.1250000002286261\n",
            "\n",
            "Iteration 4045 , Loss = 0.12500000022757773\n",
            "\n",
            "Iteration 4046 , Loss = 0.12500000022653468\n",
            "\n",
            "Iteration 4047 , Loss = 0.12500000022549682\n",
            "\n",
            "Iteration 4048 , Loss = 0.12500000022446414\n",
            "\n",
            "Iteration 4049 , Loss = 0.12500000022343663\n",
            "\n",
            "Iteration 4050 , Loss = 0.12500000022241423\n",
            "\n",
            "Iteration 4051 , Loss = 0.12500000022139698\n",
            "\n",
            "Iteration 4052 , Loss = 0.12500000022038482\n",
            "\n",
            "Iteration 4053 , Loss = 0.1250000002193777\n",
            "\n",
            "Iteration 4054 , Loss = 0.12500000021837565\n",
            "\n",
            "Iteration 4055 , Loss = 0.12500000021737856\n",
            "\n",
            "Iteration 4056 , Loss = 0.12500000021638652\n",
            "\n",
            "Iteration 4057 , Loss = 0.1250000002153994\n",
            "\n",
            "Iteration 4058 , Loss = 0.1250000002144172\n",
            "\n",
            "Iteration 4059 , Loss = 0.12500000021343996\n",
            "\n",
            "Iteration 4060 , Loss = 0.12500000021246757\n",
            "\n",
            "Iteration 4061 , Loss = 0.12500000021150007\n",
            "\n",
            "Iteration 4062 , Loss = 0.1250000002105374\n",
            "\n",
            "Iteration 4063 , Loss = 0.12500000020957955\n",
            "\n",
            "Iteration 4064 , Loss = 0.12500000020862648\n",
            "\n",
            "Iteration 4065 , Loss = 0.12500000020767815\n",
            "\n",
            "Iteration 4066 , Loss = 0.1250000002067346\n",
            "\n",
            "Iteration 4067 , Loss = 0.12500000020579574\n",
            "\n",
            "Iteration 4068 , Loss = 0.12500000020486163\n",
            "\n",
            "Iteration 4069 , Loss = 0.12500000020393215\n",
            "\n",
            "Iteration 4070 , Loss = 0.1250000002030073\n",
            "\n",
            "Iteration 4071 , Loss = 0.12500000020208712\n",
            "\n",
            "Iteration 4072 , Loss = 0.12500000020117152\n",
            "\n",
            "Iteration 4073 , Loss = 0.12500000020026047\n",
            "\n",
            "Iteration 4074 , Loss = 0.12500000019935403\n",
            "\n",
            "Iteration 4075 , Loss = 0.1250000001984521\n",
            "\n",
            "Iteration 4076 , Loss = 0.12500000019755467\n",
            "\n",
            "Iteration 4077 , Loss = 0.12500000019666174\n",
            "\n",
            "Iteration 4078 , Loss = 0.12500000019577326\n",
            "\n",
            "Iteration 4079 , Loss = 0.12500000019488924\n",
            "\n",
            "Iteration 4080 , Loss = 0.12500000019400967\n",
            "\n",
            "Iteration 4081 , Loss = 0.12500000019313445\n",
            "\n",
            "Iteration 4082 , Loss = 0.12500000019226362\n",
            "\n",
            "Iteration 4083 , Loss = 0.12500000019139715\n",
            "\n",
            "Iteration 4084 , Loss = 0.12500000019053503\n",
            "\n",
            "Iteration 4085 , Loss = 0.1250000001896772\n",
            "\n",
            "Iteration 4086 , Loss = 0.12500000018882365\n",
            "\n",
            "Iteration 4087 , Loss = 0.12500000018797444\n",
            "\n",
            "Iteration 4088 , Loss = 0.1250000001871294\n",
            "\n",
            "Iteration 4089 , Loss = 0.1250000001862886\n",
            "\n",
            "Iteration 4090 , Loss = 0.12500000018545201\n",
            "\n",
            "Iteration 4091 , Loss = 0.1250000001846196\n",
            "\n",
            "Iteration 4092 , Loss = 0.12500000018379137\n",
            "\n",
            "Iteration 4093 , Loss = 0.12500000018296725\n",
            "\n",
            "Iteration 4094 , Loss = 0.1250000001821473\n",
            "\n",
            "Iteration 4095 , Loss = 0.12500000018133145\n",
            "\n",
            "Iteration 4096 , Loss = 0.12500000018051963\n",
            "\n",
            "Iteration 4097 , Loss = 0.12500000017971188\n",
            "\n",
            "Iteration 4098 , Loss = 0.1250000001789082\n",
            "\n",
            "Iteration 4099 , Loss = 0.12500000017810853\n",
            "\n",
            "Iteration 4100 , Loss = 0.12500000017731286\n",
            "\n",
            "Iteration 4101 , Loss = 0.12500000017652116\n",
            "\n",
            "Iteration 4102 , Loss = 0.12500000017573343\n",
            "\n",
            "Iteration 4103 , Loss = 0.12500000017494964\n",
            "\n",
            "Iteration 4104 , Loss = 0.12500000017416976\n",
            "\n",
            "Iteration 4105 , Loss = 0.1250000001733938\n",
            "\n",
            "Iteration 4106 , Loss = 0.1250000001726217\n",
            "\n",
            "Iteration 4107 , Loss = 0.12500000017185348\n",
            "\n",
            "Iteration 4108 , Loss = 0.1250000001710891\n",
            "\n",
            "Iteration 4109 , Loss = 0.12500000017032853\n",
            "\n",
            "Iteration 4110 , Loss = 0.12500000016957175\n",
            "\n",
            "Iteration 4111 , Loss = 0.1250000001688188\n",
            "\n",
            "Iteration 4112 , Loss = 0.12500000016806956\n",
            "\n",
            "Iteration 4113 , Loss = 0.1250000001673241\n",
            "\n",
            "Iteration 4114 , Loss = 0.12500000016658236\n",
            "\n",
            "Iteration 4115 , Loss = 0.12500000016584434\n",
            "\n",
            "Iteration 4116 , Loss = 0.12500000016511004\n",
            "\n",
            "Iteration 4117 , Loss = 0.12500000016437934\n",
            "\n",
            "Iteration 4118 , Loss = 0.12500000016365234\n",
            "\n",
            "Iteration 4119 , Loss = 0.12500000016292898\n",
            "\n",
            "Iteration 4120 , Loss = 0.12500000016220925\n",
            "\n",
            "Iteration 4121 , Loss = 0.1250000001614931\n",
            "\n",
            "Iteration 4122 , Loss = 0.1250000001607805\n",
            "\n",
            "Iteration 4123 , Loss = 0.12500000016007154\n",
            "\n",
            "Iteration 4124 , Loss = 0.12500000015936605\n",
            "\n",
            "Iteration 4125 , Loss = 0.1250000001586641\n",
            "\n",
            "Iteration 4126 , Loss = 0.12500000015796572\n",
            "\n",
            "Iteration 4127 , Loss = 0.12500000015727078\n",
            "\n",
            "Iteration 4128 , Loss = 0.12500000015657933\n",
            "\n",
            "Iteration 4129 , Loss = 0.12500000015589136\n",
            "\n",
            "Iteration 4130 , Loss = 0.1250000001552068\n",
            "\n",
            "Iteration 4131 , Loss = 0.12500000015452567\n",
            "\n",
            "Iteration 4132 , Loss = 0.12500000015384793\n",
            "\n",
            "Iteration 4133 , Loss = 0.1250000001531736\n",
            "\n",
            "Iteration 4134 , Loss = 0.12500000015250268\n",
            "\n",
            "Iteration 4135 , Loss = 0.12500000015183507\n",
            "\n",
            "Iteration 4136 , Loss = 0.1250000001511708\n",
            "\n",
            "Iteration 4137 , Loss = 0.12500000015050988\n",
            "\n",
            "Iteration 4138 , Loss = 0.12500000014985227\n",
            "\n",
            "Iteration 4139 , Loss = 0.1250000001491979\n",
            "\n",
            "Iteration 4140 , Loss = 0.12500000014854684\n",
            "\n",
            "Iteration 4141 , Loss = 0.12500000014789903\n",
            "\n",
            "Iteration 4142 , Loss = 0.12500000014725446\n",
            "\n",
            "Iteration 4143 , Loss = 0.1250000001466131\n",
            "\n",
            "Iteration 4144 , Loss = 0.12500000014597498\n",
            "\n",
            "Iteration 4145 , Loss = 0.12500000014534002\n",
            "\n",
            "Iteration 4146 , Loss = 0.12500000014470825\n",
            "\n",
            "Iteration 4147 , Loss = 0.12500000014407966\n",
            "\n",
            "Iteration 4148 , Loss = 0.1250000001434542\n",
            "\n",
            "Iteration 4149 , Loss = 0.12500000014283183\n",
            "\n",
            "Iteration 4150 , Loss = 0.12500000014221263\n",
            "\n",
            "Iteration 4151 , Loss = 0.12500000014159648\n",
            "\n",
            "Iteration 4152 , Loss = 0.12500000014098345\n",
            "\n",
            "Iteration 4153 , Loss = 0.1250000001403735\n",
            "\n",
            "Iteration 4154 , Loss = 0.12500000013976653\n",
            "\n",
            "Iteration 4155 , Loss = 0.12500000013916268\n",
            "\n",
            "Iteration 4156 , Loss = 0.12500000013856177\n",
            "\n",
            "Iteration 4157 , Loss = 0.12500000013796392\n",
            "\n",
            "Iteration 4158 , Loss = 0.12500000013736903\n",
            "\n",
            "Iteration 4159 , Loss = 0.12500000013677715\n",
            "\n",
            "Iteration 4160 , Loss = 0.12500000013618823\n",
            "\n",
            "Iteration 4161 , Loss = 0.12500000013560222\n",
            "\n",
            "Iteration 4162 , Loss = 0.1250000001350192\n",
            "\n",
            "Iteration 4163 , Loss = 0.12500000013443902\n",
            "\n",
            "Iteration 4164 , Loss = 0.12500000013386178\n",
            "\n",
            "Iteration 4165 , Loss = 0.12500000013328744\n",
            "\n",
            "Iteration 4166 , Loss = 0.12500000013271598\n",
            "\n",
            "Iteration 4167 , Loss = 0.12500000013214732\n",
            "\n",
            "Iteration 4168 , Loss = 0.12500000013158155\n",
            "\n",
            "Iteration 4169 , Loss = 0.12500000013101864\n",
            "\n",
            "Iteration 4170 , Loss = 0.12500000013045848\n",
            "\n",
            "Iteration 4171 , Loss = 0.12500000012990117\n",
            "\n",
            "Iteration 4172 , Loss = 0.12500000012934662\n",
            "\n",
            "Iteration 4173 , Loss = 0.12500000012879486\n",
            "\n",
            "Iteration 4174 , Loss = 0.12500000012824586\n",
            "\n",
            "Iteration 4175 , Loss = 0.12500000012769963\n",
            "\n",
            "Iteration 4176 , Loss = 0.12500000012715606\n",
            "\n",
            "Iteration 4177 , Loss = 0.12500000012661527\n",
            "\n",
            "Iteration 4178 , Loss = 0.12500000012607718\n",
            "\n",
            "Iteration 4179 , Loss = 0.12500000012554177\n",
            "\n",
            "Iteration 4180 , Loss = 0.12500000012500903\n",
            "\n",
            "Iteration 4181 , Loss = 0.12500000012447895\n",
            "\n",
            "Iteration 4182 , Loss = 0.12500000012395154\n",
            "\n",
            "Iteration 4183 , Loss = 0.1250000001234268\n",
            "\n",
            "Iteration 4184 , Loss = 0.12500000012290463\n",
            "\n",
            "Iteration 4185 , Loss = 0.1250000001223851\n",
            "\n",
            "Iteration 4186 , Loss = 0.12500000012186813\n",
            "\n",
            "Iteration 4187 , Loss = 0.12500000012135382\n",
            "\n",
            "Iteration 4188 , Loss = 0.12500000012084203\n",
            "\n",
            "Iteration 4189 , Loss = 0.1250000001203328\n",
            "\n",
            "Iteration 4190 , Loss = 0.12500000011982615\n",
            "\n",
            "Iteration 4191 , Loss = 0.125000000119322\n",
            "\n",
            "Iteration 4192 , Loss = 0.1250000001188204\n",
            "\n",
            "Iteration 4193 , Loss = 0.1250000001183213\n",
            "\n",
            "Iteration 4194 , Loss = 0.1250000001178247\n",
            "\n",
            "Iteration 4195 , Loss = 0.12500000011733056\n",
            "\n",
            "Iteration 4196 , Loss = 0.1250000001168389\n",
            "\n",
            "Iteration 4197 , Loss = 0.12500000011634974\n",
            "\n",
            "Iteration 4198 , Loss = 0.12500000011586299\n",
            "\n",
            "Iteration 4199 , Loss = 0.1250000001153787\n",
            "\n",
            "Iteration 4200 , Loss = 0.12500000011489681\n",
            "\n",
            "Iteration 4201 , Loss = 0.1250000001144173\n",
            "\n",
            "Iteration 4202 , Loss = 0.12500000011394025\n",
            "\n",
            "Iteration 4203 , Loss = 0.12500000011346554\n",
            "\n",
            "Iteration 4204 , Loss = 0.12500000011299325\n",
            "\n",
            "Iteration 4205 , Loss = 0.1250000001125233\n",
            "\n",
            "Iteration 4206 , Loss = 0.1250000001120557\n",
            "\n",
            "Iteration 4207 , Loss = 0.12500000011159043\n",
            "\n",
            "Iteration 4208 , Loss = 0.12500000011112752\n",
            "\n",
            "Iteration 4209 , Loss = 0.1250000001106669\n",
            "\n",
            "Iteration 4210 , Loss = 0.1250000001102086\n",
            "\n",
            "Iteration 4211 , Loss = 0.12500000010975257\n",
            "\n",
            "Iteration 4212 , Loss = 0.12500000010929885\n",
            "\n",
            "Iteration 4213 , Loss = 0.12500000010884738\n",
            "\n",
            "Iteration 4214 , Loss = 0.12500000010839818\n",
            "\n",
            "Iteration 4215 , Loss = 0.1250000001079512\n",
            "\n",
            "Iteration 4216 , Loss = 0.1250000001075065\n",
            "\n",
            "Iteration 4217 , Loss = 0.125000000107064\n",
            "\n",
            "Iteration 4218 , Loss = 0.1250000001066237\n",
            "\n",
            "Iteration 4219 , Loss = 0.12500000010618562\n",
            "\n",
            "Iteration 4220 , Loss = 0.12500000010574971\n",
            "\n",
            "Iteration 4221 , Loss = 0.12500000010531603\n",
            "\n",
            "Iteration 4222 , Loss = 0.1250000001048845\n",
            "\n",
            "Iteration 4223 , Loss = 0.1250000001044551\n",
            "\n",
            "Iteration 4224 , Loss = 0.1250000001040279\n",
            "\n",
            "Iteration 4225 , Loss = 0.12500000010360282\n",
            "\n",
            "Iteration 4226 , Loss = 0.12500000010317983\n",
            "\n",
            "Iteration 4227 , Loss = 0.125000000102759\n",
            "\n",
            "Iteration 4228 , Loss = 0.12500000010234025\n",
            "\n",
            "Iteration 4229 , Loss = 0.12500000010192358\n",
            "\n",
            "Iteration 4230 , Loss = 0.12500000010150902\n",
            "\n",
            "Iteration 4231 , Loss = 0.12500000010109658\n",
            "\n",
            "Iteration 4232 , Loss = 0.12500000010068613\n",
            "\n",
            "Iteration 4233 , Loss = 0.12500000010027779\n",
            "\n",
            "Iteration 4234 , Loss = 0.12500000009987144\n",
            "\n",
            "Iteration 4235 , Loss = 0.12500000009946716\n",
            "\n",
            "Iteration 4236 , Loss = 0.12500000009906487\n",
            "\n",
            "Iteration 4237 , Loss = 0.1250000000986646\n",
            "\n",
            "Iteration 4238 , Loss = 0.12500000009826634\n",
            "\n",
            "Iteration 4239 , Loss = 0.1250000000978701\n",
            "\n",
            "Iteration 4240 , Loss = 0.1250000000974758\n",
            "\n",
            "Iteration 4241 , Loss = 0.12500000009708354\n",
            "\n",
            "Iteration 4242 , Loss = 0.12500000009669315\n",
            "\n",
            "Iteration 4243 , Loss = 0.1250000000963048\n",
            "\n",
            "Iteration 4244 , Loss = 0.12500000009591833\n",
            "\n",
            "Iteration 4245 , Loss = 0.12500000009553383\n",
            "\n",
            "Iteration 4246 , Loss = 0.12500000009515125\n",
            "\n",
            "Iteration 4247 , Loss = 0.12500000009477058\n",
            "\n",
            "Iteration 4248 , Loss = 0.12500000009439183\n",
            "\n",
            "Iteration 4249 , Loss = 0.12500000009401493\n",
            "\n",
            "Iteration 4250 , Loss = 0.12500000009363993\n",
            "\n",
            "Iteration 4251 , Loss = 0.12500000009326684\n",
            "\n",
            "Iteration 4252 , Loss = 0.1250000000928956\n",
            "\n",
            "Iteration 4253 , Loss = 0.1250000000925262\n",
            "\n",
            "Iteration 4254 , Loss = 0.12500000009215867\n",
            "\n",
            "Iteration 4255 , Loss = 0.12500000009179302\n",
            "\n",
            "Iteration 4256 , Loss = 0.12500000009142911\n",
            "\n",
            "Iteration 4257 , Loss = 0.12500000009106704\n",
            "\n",
            "Iteration 4258 , Loss = 0.12500000009070683\n",
            "\n",
            "Iteration 4259 , Loss = 0.1250000000903484\n",
            "\n",
            "Iteration 4260 , Loss = 0.12500000008999176\n",
            "\n",
            "Iteration 4261 , Loss = 0.1250000000896369\n",
            "\n",
            "Iteration 4262 , Loss = 0.12500000008928383\n",
            "\n",
            "Iteration 4263 , Loss = 0.12500000008893253\n",
            "\n",
            "Iteration 4264 , Loss = 0.12500000008858297\n",
            "\n",
            "Iteration 4265 , Loss = 0.12500000008823517\n",
            "\n",
            "Iteration 4266 , Loss = 0.1250000000878891\n",
            "\n",
            "Iteration 4267 , Loss = 0.12500000008754478\n",
            "\n",
            "Iteration 4268 , Loss = 0.1250000000872022\n",
            "\n",
            "Iteration 4269 , Loss = 0.1250000000868613\n",
            "\n",
            "Iteration 4270 , Loss = 0.1250000000865221\n",
            "\n",
            "Iteration 4271 , Loss = 0.1250000000861846\n",
            "\n",
            "Iteration 4272 , Loss = 0.1250000000858488\n",
            "\n",
            "Iteration 4273 , Loss = 0.1250000000855147\n",
            "\n",
            "Iteration 4274 , Loss = 0.12500000008518225\n",
            "\n",
            "Iteration 4275 , Loss = 0.12500000008485146\n",
            "\n",
            "Iteration 4276 , Loss = 0.12500000008452233\n",
            "\n",
            "Iteration 4277 , Loss = 0.12500000008419485\n",
            "\n",
            "Iteration 4278 , Loss = 0.12500000008386902\n",
            "\n",
            "Iteration 4279 , Loss = 0.1250000000835448\n",
            "\n",
            "Iteration 4280 , Loss = 0.12500000008322223\n",
            "\n",
            "Iteration 4281 , Loss = 0.12500000008290124\n",
            "\n",
            "Iteration 4282 , Loss = 0.12500000008258189\n",
            "\n",
            "Iteration 4283 , Loss = 0.1250000000822641\n",
            "\n",
            "Iteration 4284 , Loss = 0.12500000008194795\n",
            "\n",
            "Iteration 4285 , Loss = 0.12500000008163337\n",
            "\n",
            "Iteration 4286 , Loss = 0.12500000008132034\n",
            "\n",
            "Iteration 4287 , Loss = 0.12500000008100887\n",
            "\n",
            "Iteration 4288 , Loss = 0.12500000008069898\n",
            "\n",
            "Iteration 4289 , Loss = 0.12500000008039064\n",
            "\n",
            "Iteration 4290 , Loss = 0.12500000008008386\n",
            "\n",
            "Iteration 4291 , Loss = 0.1250000000797786\n",
            "\n",
            "Iteration 4292 , Loss = 0.12500000007947484\n",
            "\n",
            "Iteration 4293 , Loss = 0.12500000007917264\n",
            "\n",
            "Iteration 4294 , Loss = 0.12500000007887194\n",
            "\n",
            "Iteration 4295 , Loss = 0.12500000007857276\n",
            "\n",
            "Iteration 4296 , Loss = 0.12500000007827505\n",
            "\n",
            "Iteration 4297 , Loss = 0.12500000007797882\n",
            "\n",
            "Iteration 4298 , Loss = 0.12500000007768408\n",
            "\n",
            "Iteration 4299 , Loss = 0.12500000007739082\n",
            "\n",
            "Iteration 4300 , Loss = 0.12500000007709908\n",
            "\n",
            "Iteration 4301 , Loss = 0.12500000007680875\n",
            "\n",
            "Iteration 4302 , Loss = 0.12500000007651987\n",
            "\n",
            "Iteration 4303 , Loss = 0.12500000007623244\n",
            "\n",
            "Iteration 4304 , Loss = 0.12500000007594647\n",
            "\n",
            "Iteration 4305 , Loss = 0.12500000007566192\n",
            "\n",
            "Iteration 4306 , Loss = 0.12500000007537876\n",
            "\n",
            "Iteration 4307 , Loss = 0.12500000007509707\n",
            "\n",
            "Iteration 4308 , Loss = 0.1250000000748168\n",
            "\n",
            "Iteration 4309 , Loss = 0.12500000007453788\n",
            "\n",
            "Iteration 4310 , Loss = 0.12500000007426038\n",
            "\n",
            "Iteration 4311 , Loss = 0.12500000007398426\n",
            "\n",
            "Iteration 4312 , Loss = 0.12500000007370954\n",
            "\n",
            "Iteration 4313 , Loss = 0.12500000007343617\n",
            "\n",
            "Iteration 4314 , Loss = 0.1250000000731642\n",
            "\n",
            "Iteration 4315 , Loss = 0.12500000007289355\n",
            "\n",
            "Iteration 4316 , Loss = 0.1250000000726243\n",
            "\n",
            "Iteration 4317 , Loss = 0.12500000007235637\n",
            "\n",
            "Iteration 4318 , Loss = 0.1250000000720898\n",
            "\n",
            "Iteration 4319 , Loss = 0.12500000007182455\n",
            "\n",
            "Iteration 4320 , Loss = 0.12500000007156062\n",
            "\n",
            "Iteration 4321 , Loss = 0.12500000007129802\n",
            "\n",
            "Iteration 4322 , Loss = 0.12500000007103673\n",
            "\n",
            "Iteration 4323 , Loss = 0.12500000007077675\n",
            "\n",
            "Iteration 4324 , Loss = 0.1250000000705181\n",
            "\n",
            "Iteration 4325 , Loss = 0.12500000007026071\n",
            "\n",
            "Iteration 4326 , Loss = 0.1250000000700046\n",
            "\n",
            "Iteration 4327 , Loss = 0.12500000006974982\n",
            "\n",
            "Iteration 4328 , Loss = 0.12500000006949627\n",
            "\n",
            "Iteration 4329 , Loss = 0.125000000069244\n",
            "\n",
            "Iteration 4330 , Loss = 0.12500000006899303\n",
            "\n",
            "Iteration 4331 , Loss = 0.1250000000687433\n",
            "\n",
            "Iteration 4332 , Loss = 0.12500000006849482\n",
            "\n",
            "Iteration 4333 , Loss = 0.12500000006824757\n",
            "\n",
            "Iteration 4334 , Loss = 0.12500000006800155\n",
            "\n",
            "Iteration 4335 , Loss = 0.12500000006775674\n",
            "\n",
            "Iteration 4336 , Loss = 0.12500000006751322\n",
            "\n",
            "Iteration 4337 , Loss = 0.12500000006727088\n",
            "\n",
            "Iteration 4338 , Loss = 0.12500000006702974\n",
            "\n",
            "Iteration 4339 , Loss = 0.12500000006678982\n",
            "\n",
            "Iteration 4340 , Loss = 0.12500000006655113\n",
            "\n",
            "Iteration 4341 , Loss = 0.12500000006631362\n",
            "\n",
            "Iteration 4342 , Loss = 0.12500000006607728\n",
            "\n",
            "Iteration 4343 , Loss = 0.12500000006584217\n",
            "\n",
            "Iteration 4344 , Loss = 0.12500000006560819\n",
            "\n",
            "Iteration 4345 , Loss = 0.12500000006537537\n",
            "\n",
            "Iteration 4346 , Loss = 0.12500000006514378\n",
            "\n",
            "Iteration 4347 , Loss = 0.12500000006491327\n",
            "\n",
            "Iteration 4348 , Loss = 0.12500000006468398\n",
            "\n",
            "Iteration 4349 , Loss = 0.1250000000644558\n",
            "\n",
            "Iteration 4350 , Loss = 0.1250000000642288\n",
            "\n",
            "Iteration 4351 , Loss = 0.1250000000640029\n",
            "\n",
            "Iteration 4352 , Loss = 0.12500000006377815\n",
            "\n",
            "Iteration 4353 , Loss = 0.12500000006355452\n",
            "\n",
            "Iteration 4354 , Loss = 0.12500000006333198\n",
            "\n",
            "Iteration 4355 , Loss = 0.12500000006311063\n",
            "\n",
            "Iteration 4356 , Loss = 0.12500000006289033\n",
            "\n",
            "Iteration 4357 , Loss = 0.12500000006267115\n",
            "\n",
            "Iteration 4358 , Loss = 0.12500000006245304\n",
            "\n",
            "Iteration 4359 , Loss = 0.12500000006223605\n",
            "\n",
            "Iteration 4360 , Loss = 0.12500000006202014\n",
            "\n",
            "Iteration 4361 , Loss = 0.12500000006180534\n",
            "\n",
            "Iteration 4362 , Loss = 0.12500000006159157\n",
            "\n",
            "Iteration 4363 , Loss = 0.1250000000613789\n",
            "\n",
            "Iteration 4364 , Loss = 0.12500000006116727\n",
            "\n",
            "Iteration 4365 , Loss = 0.1250000000609567\n",
            "\n",
            "Iteration 4366 , Loss = 0.1250000000607472\n",
            "\n",
            "Iteration 4367 , Loss = 0.1250000000605388\n",
            "\n",
            "Iteration 4368 , Loss = 0.12500000006033138\n",
            "\n",
            "Iteration 4369 , Loss = 0.12500000006012496\n",
            "\n",
            "Iteration 4370 , Loss = 0.12500000005991965\n",
            "\n",
            "Iteration 4371 , Loss = 0.12500000005971534\n",
            "\n",
            "Iteration 4372 , Loss = 0.12500000005951206\n",
            "\n",
            "Iteration 4373 , Loss = 0.1250000000593098\n",
            "\n",
            "Iteration 4374 , Loss = 0.12500000005910855\n",
            "\n",
            "Iteration 4375 , Loss = 0.1250000000589083\n",
            "\n",
            "Iteration 4376 , Loss = 0.12500000005870907\n",
            "\n",
            "Iteration 4377 , Loss = 0.1250000000585108\n",
            "\n",
            "Iteration 4378 , Loss = 0.12500000005831358\n",
            "\n",
            "Iteration 4379 , Loss = 0.12500000005811732\n",
            "\n",
            "Iteration 4380 , Loss = 0.125000000057922\n",
            "\n",
            "Iteration 4381 , Loss = 0.12500000005772774\n",
            "\n",
            "Iteration 4382 , Loss = 0.1250000000575344\n",
            "\n",
            "Iteration 4383 , Loss = 0.12500000005734202\n",
            "\n",
            "Iteration 4384 , Loss = 0.12500000005715062\n",
            "\n",
            "Iteration 4385 , Loss = 0.1250000000569602\n",
            "\n",
            "Iteration 4386 , Loss = 0.1250000000567707\n",
            "\n",
            "Iteration 4387 , Loss = 0.12500000005658216\n",
            "\n",
            "Iteration 4388 , Loss = 0.1250000000563946\n",
            "\n",
            "Iteration 4389 , Loss = 0.12500000005620793\n",
            "\n",
            "Iteration 4390 , Loss = 0.12500000005602221\n",
            "\n",
            "Iteration 4391 , Loss = 0.12500000005583745\n",
            "\n",
            "Iteration 4392 , Loss = 0.1250000000556536\n",
            "\n",
            "Iteration 4393 , Loss = 0.12500000005547063\n",
            "\n",
            "Iteration 4394 , Loss = 0.1250000000552886\n",
            "\n",
            "Iteration 4395 , Loss = 0.1250000000551075\n",
            "\n",
            "Iteration 4396 , Loss = 0.12500000005492728\n",
            "\n",
            "Iteration 4397 , Loss = 0.12500000005474798\n",
            "\n",
            "Iteration 4398 , Loss = 0.1250000000545696\n",
            "\n",
            "Iteration 4399 , Loss = 0.1250000000543921\n",
            "\n",
            "Iteration 4400 , Loss = 0.12500000005421547\n",
            "\n",
            "Iteration 4401 , Loss = 0.12500000005403974\n",
            "\n",
            "Iteration 4402 , Loss = 0.12500000005386488\n",
            "\n",
            "Iteration 4403 , Loss = 0.1250000000536909\n",
            "\n",
            "Iteration 4404 , Loss = 0.1250000000535178\n",
            "\n",
            "Iteration 4405 , Loss = 0.12500000005334555\n",
            "\n",
            "Iteration 4406 , Loss = 0.1250000000531742\n",
            "\n",
            "Iteration 4407 , Loss = 0.12500000005300366\n",
            "\n",
            "Iteration 4408 , Loss = 0.12500000005283401\n",
            "\n",
            "Iteration 4409 , Loss = 0.12500000005266518\n",
            "\n",
            "Iteration 4410 , Loss = 0.12500000005249723\n",
            "\n",
            "Iteration 4411 , Loss = 0.12500000005233008\n",
            "\n",
            "Iteration 4412 , Loss = 0.1250000000521638\n",
            "\n",
            "Iteration 4413 , Loss = 0.12500000005199832\n",
            "\n",
            "Iteration 4414 , Loss = 0.1250000000518337\n",
            "\n",
            "Iteration 4415 , Loss = 0.12500000005166992\n",
            "\n",
            "Iteration 4416 , Loss = 0.12500000005150694\n",
            "\n",
            "Iteration 4417 , Loss = 0.12500000005134476\n",
            "\n",
            "Iteration 4418 , Loss = 0.12500000005118342\n",
            "\n",
            "Iteration 4419 , Loss = 0.12500000005102288\n",
            "\n",
            "Iteration 4420 , Loss = 0.12500000005086315\n",
            "\n",
            "Iteration 4421 , Loss = 0.12500000005070422\n",
            "\n",
            "Iteration 4422 , Loss = 0.12500000005054607\n",
            "\n",
            "Iteration 4423 , Loss = 0.12500000005038872\n",
            "\n",
            "Iteration 4424 , Loss = 0.12500000005023215\n",
            "\n",
            "Iteration 4425 , Loss = 0.1250000000500764\n",
            "\n",
            "Iteration 4426 , Loss = 0.12500000004992137\n",
            "\n",
            "Iteration 4427 , Loss = 0.12500000004976713\n",
            "\n",
            "Iteration 4428 , Loss = 0.12500000004961373\n",
            "\n",
            "Iteration 4429 , Loss = 0.12500000004946102\n",
            "\n",
            "Iteration 4430 , Loss = 0.1250000000493091\n",
            "\n",
            "Iteration 4431 , Loss = 0.12500000004915796\n",
            "\n",
            "Iteration 4432 , Loss = 0.12500000004900758\n",
            "\n",
            "Iteration 4433 , Loss = 0.12500000004885797\n",
            "\n",
            "Iteration 4434 , Loss = 0.12500000004870904\n",
            "\n",
            "Iteration 4435 , Loss = 0.1250000000485609\n",
            "\n",
            "Iteration 4436 , Loss = 0.1250000000484135\n",
            "\n",
            "Iteration 4437 , Loss = 0.12500000004826683\n",
            "\n",
            "Iteration 4438 , Loss = 0.1250000000481209\n",
            "\n",
            "Iteration 4439 , Loss = 0.1250000000479757\n",
            "\n",
            "Iteration 4440 , Loss = 0.12500000004783124\n",
            "\n",
            "Iteration 4441 , Loss = 0.1250000000476875\n",
            "\n",
            "Iteration 4442 , Loss = 0.12500000004754447\n",
            "\n",
            "Iteration 4443 , Loss = 0.12500000004740214\n",
            "\n",
            "Iteration 4444 , Loss = 0.12500000004726058\n",
            "\n",
            "Iteration 4445 , Loss = 0.1250000000471197\n",
            "\n",
            "Iteration 4446 , Loss = 0.12500000004697953\n",
            "\n",
            "Iteration 4447 , Loss = 0.12500000004684003\n",
            "\n",
            "Iteration 4448 , Loss = 0.12500000004670125\n",
            "\n",
            "Iteration 4449 , Loss = 0.12500000004656317\n",
            "\n",
            "Iteration 4450 , Loss = 0.1250000000464258\n",
            "\n",
            "Iteration 4451 , Loss = 0.12500000004628908\n",
            "\n",
            "Iteration 4452 , Loss = 0.12500000004615308\n",
            "\n",
            "Iteration 4453 , Loss = 0.12500000004601775\n",
            "\n",
            "Iteration 4454 , Loss = 0.1250000000458831\n",
            "\n",
            "Iteration 4455 , Loss = 0.12500000004574913\n",
            "\n",
            "Iteration 4456 , Loss = 0.1250000000456158\n",
            "\n",
            "Iteration 4457 , Loss = 0.12500000004548315\n",
            "\n",
            "Iteration 4458 , Loss = 0.12500000004535117\n",
            "\n",
            "Iteration 4459 , Loss = 0.12500000004521986\n",
            "\n",
            "Iteration 4460 , Loss = 0.1250000000450892\n",
            "\n",
            "Iteration 4461 , Loss = 0.12500000004495923\n",
            "\n",
            "Iteration 4462 , Loss = 0.12500000004482986\n",
            "\n",
            "Iteration 4463 , Loss = 0.1250000000447012\n",
            "\n",
            "Iteration 4464 , Loss = 0.12500000004457312\n",
            "\n",
            "Iteration 4465 , Loss = 0.1250000000444457\n",
            "\n",
            "Iteration 4466 , Loss = 0.12500000004431894\n",
            "\n",
            "Iteration 4467 , Loss = 0.1250000000441928\n",
            "\n",
            "Iteration 4468 , Loss = 0.12500000004406728\n",
            "\n",
            "Iteration 4469 , Loss = 0.1250000000439424\n",
            "\n",
            "Iteration 4470 , Loss = 0.12500000004381817\n",
            "\n",
            "Iteration 4471 , Loss = 0.12500000004369455\n",
            "\n",
            "Iteration 4472 , Loss = 0.12500000004357154\n",
            "\n",
            "Iteration 4473 , Loss = 0.12500000004344913\n",
            "\n",
            "Iteration 4474 , Loss = 0.12500000004332737\n",
            "\n",
            "Iteration 4475 , Loss = 0.12500000004320622\n",
            "\n",
            "Iteration 4476 , Loss = 0.12500000004308565\n",
            "\n",
            "Iteration 4477 , Loss = 0.12500000004296566\n",
            "\n",
            "Iteration 4478 , Loss = 0.12500000004284634\n",
            "\n",
            "Iteration 4479 , Loss = 0.12500000004272757\n",
            "\n",
            "Iteration 4480 , Loss = 0.12500000004260942\n",
            "\n",
            "Iteration 4481 , Loss = 0.12500000004249184\n",
            "\n",
            "Iteration 4482 , Loss = 0.12500000004237488\n",
            "\n",
            "Iteration 4483 , Loss = 0.12500000004225847\n",
            "\n",
            "Iteration 4484 , Loss = 0.12500000004214268\n",
            "\n",
            "Iteration 4485 , Loss = 0.1250000000420274\n",
            "\n",
            "Iteration 4486 , Loss = 0.1250000000419128\n",
            "\n",
            "Iteration 4487 , Loss = 0.1250000000417987\n",
            "\n",
            "Iteration 4488 , Loss = 0.1250000000416852\n",
            "\n",
            "Iteration 4489 , Loss = 0.1250000000415723\n",
            "\n",
            "Iteration 4490 , Loss = 0.12500000004145995\n",
            "\n",
            "Iteration 4491 , Loss = 0.12500000004134815\n",
            "\n",
            "Iteration 4492 , Loss = 0.12500000004123688\n",
            "\n",
            "Iteration 4493 , Loss = 0.12500000004112619\n",
            "\n",
            "Iteration 4494 , Loss = 0.12500000004101608\n",
            "\n",
            "Iteration 4495 , Loss = 0.1250000000409065\n",
            "\n",
            "Iteration 4496 , Loss = 0.12500000004079745\n",
            "\n",
            "Iteration 4497 , Loss = 0.125000000040689\n",
            "\n",
            "Iteration 4498 , Loss = 0.12500000004058104\n",
            "\n",
            "Iteration 4499 , Loss = 0.12500000004047365\n",
            "\n",
            "Iteration 4500 , Loss = 0.1250000000403668\n",
            "\n",
            "Iteration 4501 , Loss = 0.1250000000402605\n",
            "\n",
            "Iteration 4502 , Loss = 0.12500000004015469\n",
            "\n",
            "Iteration 4503 , Loss = 0.12500000004004946\n",
            "\n",
            "Iteration 4504 , Loss = 0.12500000003994471\n",
            "\n",
            "Iteration 4505 , Loss = 0.12500000003984052\n",
            "\n",
            "Iteration 4506 , Loss = 0.12500000003973685\n",
            "\n",
            "Iteration 4507 , Loss = 0.12500000003963369\n",
            "\n",
            "Iteration 4508 , Loss = 0.12500000003953105\n",
            "\n",
            "Iteration 4509 , Loss = 0.1250000000394289\n",
            "\n",
            "Iteration 4510 , Loss = 0.1250000000393273\n",
            "\n",
            "Iteration 4511 , Loss = 0.12500000003922618\n",
            "\n",
            "Iteration 4512 , Loss = 0.12500000003912562\n",
            "\n",
            "Iteration 4513 , Loss = 0.12500000003902553\n",
            "\n",
            "Iteration 4514 , Loss = 0.12500000003892595\n",
            "\n",
            "Iteration 4515 , Loss = 0.12500000003882683\n",
            "\n",
            "Iteration 4516 , Loss = 0.12500000003872824\n",
            "\n",
            "Iteration 4517 , Loss = 0.12500000003863013\n",
            "\n",
            "Iteration 4518 , Loss = 0.12500000003853254\n",
            "\n",
            "Iteration 4519 , Loss = 0.12500000003843542\n",
            "\n",
            "Iteration 4520 , Loss = 0.12500000003833878\n",
            "\n",
            "Iteration 4521 , Loss = 0.12500000003824263\n",
            "\n",
            "Iteration 4522 , Loss = 0.12500000003814699\n",
            "\n",
            "Iteration 4523 , Loss = 0.12500000003805178\n",
            "\n",
            "Iteration 4524 , Loss = 0.1250000000379571\n",
            "\n",
            "Iteration 4525 , Loss = 0.12500000003786288\n",
            "\n",
            "Iteration 4526 , Loss = 0.12500000003776912\n",
            "\n",
            "Iteration 4527 , Loss = 0.12500000003767583\n",
            "\n",
            "Iteration 4528 , Loss = 0.12500000003758302\n",
            "\n",
            "Iteration 4529 , Loss = 0.12500000003749065\n",
            "\n",
            "Iteration 4530 , Loss = 0.12500000003739875\n",
            "\n",
            "Iteration 4531 , Loss = 0.12500000003730735\n",
            "\n",
            "Iteration 4532 , Loss = 0.12500000003721637\n",
            "\n",
            "Iteration 4533 , Loss = 0.12500000003712586\n",
            "\n",
            "Iteration 4534 , Loss = 0.12500000003703582\n",
            "\n",
            "Iteration 4535 , Loss = 0.1250000000369462\n",
            "\n",
            "Iteration 4536 , Loss = 0.12500000003685707\n",
            "\n",
            "Iteration 4537 , Loss = 0.12500000003676834\n",
            "\n",
            "Iteration 4538 , Loss = 0.12500000003668008\n",
            "\n",
            "Iteration 4539 , Loss = 0.12500000003659226\n",
            "\n",
            "Iteration 4540 , Loss = 0.12500000003650488\n",
            "\n",
            "Iteration 4541 , Loss = 0.12500000003641792\n",
            "\n",
            "Iteration 4542 , Loss = 0.12500000003633144\n",
            "\n",
            "Iteration 4543 , Loss = 0.12500000003624537\n",
            "\n",
            "Iteration 4544 , Loss = 0.12500000003615974\n",
            "\n",
            "Iteration 4545 , Loss = 0.12500000003607453\n",
            "\n",
            "Iteration 4546 , Loss = 0.12500000003598974\n",
            "\n",
            "Iteration 4547 , Loss = 0.1250000000359054\n",
            "\n",
            "Iteration 4548 , Loss = 0.12500000003582146\n",
            "\n",
            "Iteration 4549 , Loss = 0.12500000003573794\n",
            "\n",
            "Iteration 4550 , Loss = 0.12500000003565484\n",
            "\n",
            "Iteration 4551 , Loss = 0.12500000003557218\n",
            "\n",
            "Iteration 4552 , Loss = 0.12500000003548992\n",
            "\n",
            "Iteration 4553 , Loss = 0.12500000003540807\n",
            "\n",
            "Iteration 4554 , Loss = 0.12500000003532663\n",
            "\n",
            "Iteration 4555 , Loss = 0.1250000000352456\n",
            "\n",
            "Iteration 4556 , Loss = 0.125000000035165\n",
            "\n",
            "Iteration 4557 , Loss = 0.1250000000350848\n",
            "\n",
            "Iteration 4558 , Loss = 0.12500000003500494\n",
            "\n",
            "Iteration 4559 , Loss = 0.12500000003492556\n",
            "\n",
            "Iteration 4560 , Loss = 0.12500000003484657\n",
            "\n",
            "Iteration 4561 , Loss = 0.12500000003476794\n",
            "\n",
            "Iteration 4562 , Loss = 0.12500000003468972\n",
            "\n",
            "Iteration 4563 , Loss = 0.1250000000346119\n",
            "\n",
            "Iteration 4564 , Loss = 0.1250000000345345\n",
            "\n",
            "Iteration 4565 , Loss = 0.1250000000344574\n",
            "\n",
            "Iteration 4566 , Loss = 0.12500000003438075\n",
            "\n",
            "Iteration 4567 , Loss = 0.12500000003430448\n",
            "\n",
            "Iteration 4568 , Loss = 0.1250000000342286\n",
            "\n",
            "Iteration 4569 , Loss = 0.12500000003415307\n",
            "\n",
            "Iteration 4570 , Loss = 0.12500000003407793\n",
            "\n",
            "Iteration 4571 , Loss = 0.1250000000340032\n",
            "\n",
            "Iteration 4572 , Loss = 0.1250000000339288\n",
            "\n",
            "Iteration 4573 , Loss = 0.1250000000338548\n",
            "\n",
            "Iteration 4574 , Loss = 0.12500000003378117\n",
            "\n",
            "Iteration 4575 , Loss = 0.12500000003370793\n",
            "\n",
            "Iteration 4576 , Loss = 0.125000000033635\n",
            "\n",
            "Iteration 4577 , Loss = 0.1250000000335625\n",
            "\n",
            "Iteration 4578 , Loss = 0.12500000003349032\n",
            "\n",
            "Iteration 4579 , Loss = 0.12500000003341852\n",
            "\n",
            "Iteration 4580 , Loss = 0.12500000003334708\n",
            "\n",
            "Iteration 4581 , Loss = 0.125000000033276\n",
            "\n",
            "Iteration 4582 , Loss = 0.12500000003320527\n",
            "\n",
            "Iteration 4583 , Loss = 0.12500000003313488\n",
            "\n",
            "Iteration 4584 , Loss = 0.12500000003306488\n",
            "\n",
            "Iteration 4585 , Loss = 0.12500000003299522\n",
            "\n",
            "Iteration 4586 , Loss = 0.12500000003292588\n",
            "\n",
            "Iteration 4587 , Loss = 0.12500000003285694\n",
            "\n",
            "Iteration 4588 , Loss = 0.1250000000327883\n",
            "\n",
            "Iteration 4589 , Loss = 0.12500000003272002\n",
            "\n",
            "Iteration 4590 , Loss = 0.1250000000326521\n",
            "\n",
            "Iteration 4591 , Loss = 0.12500000003258452\n",
            "\n",
            "Iteration 4592 , Loss = 0.12500000003251727\n",
            "\n",
            "Iteration 4593 , Loss = 0.12500000003245038\n",
            "\n",
            "Iteration 4594 , Loss = 0.1250000000323838\n",
            "\n",
            "Iteration 4595 , Loss = 0.12500000003231754\n",
            "\n",
            "Iteration 4596 , Loss = 0.12500000003225165\n",
            "\n",
            "Iteration 4597 , Loss = 0.12500000003218603\n",
            "\n",
            "Iteration 4598 , Loss = 0.1250000000321208\n",
            "\n",
            "Iteration 4599 , Loss = 0.12500000003205586\n",
            "\n",
            "Iteration 4600 , Loss = 0.1250000000319913\n",
            "\n",
            "Iteration 4601 , Loss = 0.12500000003192702\n",
            "\n",
            "Iteration 4602 , Loss = 0.12500000003186307\n",
            "\n",
            "Iteration 4603 , Loss = 0.12500000003179945\n",
            "\n",
            "Iteration 4604 , Loss = 0.12500000003173614\n",
            "\n",
            "Iteration 4605 , Loss = 0.12500000003167316\n",
            "\n",
            "Iteration 4606 , Loss = 0.12500000003161046\n",
            "\n",
            "Iteration 4607 , Loss = 0.12500000003154813\n",
            "\n",
            "Iteration 4608 , Loss = 0.1250000000314861\n",
            "\n",
            "Iteration 4609 , Loss = 0.12500000003142436\n",
            "\n",
            "Iteration 4610 , Loss = 0.1250000000313629\n",
            "\n",
            "Iteration 4611 , Loss = 0.12500000003130182\n",
            "\n",
            "Iteration 4612 , Loss = 0.125000000031241\n",
            "\n",
            "Iteration 4613 , Loss = 0.1250000000311805\n",
            "\n",
            "Iteration 4614 , Loss = 0.12500000003112033\n",
            "\n",
            "Iteration 4615 , Loss = 0.12500000003106043\n",
            "\n",
            "Iteration 4616 , Loss = 0.12500000003100084\n",
            "\n",
            "Iteration 4617 , Loss = 0.12500000003094155\n",
            "\n",
            "Iteration 4618 , Loss = 0.12500000003088257\n",
            "\n",
            "Iteration 4619 , Loss = 0.12500000003082387\n",
            "\n",
            "Iteration 4620 , Loss = 0.12500000003076547\n",
            "\n",
            "Iteration 4621 , Loss = 0.12500000003070735\n",
            "\n",
            "Iteration 4622 , Loss = 0.12500000003064957\n",
            "\n",
            "Iteration 4623 , Loss = 0.125000000030592\n",
            "\n",
            "Iteration 4624 , Loss = 0.1250000000305348\n",
            "\n",
            "Iteration 4625 , Loss = 0.12500000003047784\n",
            "\n",
            "Iteration 4626 , Loss = 0.1250000000304212\n",
            "\n",
            "Iteration 4627 , Loss = 0.12500000003036482\n",
            "\n",
            "Iteration 4628 , Loss = 0.1250000000303087\n",
            "\n",
            "Iteration 4629 , Loss = 0.1250000000302529\n",
            "\n",
            "Iteration 4630 , Loss = 0.12500000003019737\n",
            "\n",
            "Iteration 4631 , Loss = 0.1250000000301421\n",
            "\n",
            "Iteration 4632 , Loss = 0.12500000003008715\n",
            "\n",
            "Iteration 4633 , Loss = 0.12500000003003245\n",
            "\n",
            "Iteration 4634 , Loss = 0.12500000002997805\n",
            "\n",
            "Iteration 4635 , Loss = 0.1250000000299239\n",
            "\n",
            "Iteration 4636 , Loss = 0.12500000002987002\n",
            "\n",
            "Iteration 4637 , Loss = 0.12500000002981643\n",
            "\n",
            "Iteration 4638 , Loss = 0.1250000000297631\n",
            "\n",
            "Iteration 4639 , Loss = 0.12500000002971004\n",
            "\n",
            "Iteration 4640 , Loss = 0.12500000002965722\n",
            "\n",
            "Iteration 4641 , Loss = 0.1250000000296047\n",
            "\n",
            "Iteration 4642 , Loss = 0.12500000002955242\n",
            "\n",
            "Iteration 4643 , Loss = 0.12500000002950046\n",
            "\n",
            "Iteration 4644 , Loss = 0.1250000000294487\n",
            "\n",
            "Iteration 4645 , Loss = 0.1250000000293972\n",
            "\n",
            "Iteration 4646 , Loss = 0.125000000029346\n",
            "\n",
            "Iteration 4647 , Loss = 0.125000000029295\n",
            "\n",
            "Iteration 4648 , Loss = 0.12500000002924427\n",
            "\n",
            "Iteration 4649 , Loss = 0.12500000002919384\n",
            "\n",
            "Iteration 4650 , Loss = 0.12500000002914366\n",
            "\n",
            "Iteration 4651 , Loss = 0.1250000000290937\n",
            "\n",
            "Iteration 4652 , Loss = 0.125000000029044\n",
            "\n",
            "Iteration 4653 , Loss = 0.12500000002899456\n",
            "\n",
            "Iteration 4654 , Loss = 0.1250000000289454\n",
            "\n",
            "Iteration 4655 , Loss = 0.1250000000288964\n",
            "\n",
            "Iteration 4656 , Loss = 0.1250000000288477\n",
            "\n",
            "Iteration 4657 , Loss = 0.12500000002879927\n",
            "\n",
            "Iteration 4658 , Loss = 0.12500000002875103\n",
            "\n",
            "Iteration 4659 , Loss = 0.1250000000287031\n",
            "\n",
            "Iteration 4660 , Loss = 0.12500000002865536\n",
            "\n",
            "Iteration 4661 , Loss = 0.12500000002860787\n",
            "\n",
            "Iteration 4662 , Loss = 0.12500000002856063\n",
            "\n",
            "Iteration 4663 , Loss = 0.1250000000285136\n",
            "\n",
            "Iteration 4664 , Loss = 0.12500000002846684\n",
            "\n",
            "Iteration 4665 , Loss = 0.1250000000284203\n",
            "\n",
            "Iteration 4666 , Loss = 0.12500000002837397\n",
            "\n",
            "Iteration 4667 , Loss = 0.12500000002832792\n",
            "\n",
            "Iteration 4668 , Loss = 0.12500000002828207\n",
            "\n",
            "Iteration 4669 , Loss = 0.12500000002823647\n",
            "\n",
            "Iteration 4670 , Loss = 0.12500000002819106\n",
            "\n",
            "Iteration 4671 , Loss = 0.12500000002814596\n",
            "\n",
            "Iteration 4672 , Loss = 0.12500000002810102\n",
            "\n",
            "Iteration 4673 , Loss = 0.12500000002805628\n",
            "\n",
            "Iteration 4674 , Loss = 0.12500000002801187\n",
            "\n",
            "Iteration 4675 , Loss = 0.1250000000279676\n",
            "\n",
            "Iteration 4676 , Loss = 0.12500000002792358\n",
            "\n",
            "Iteration 4677 , Loss = 0.12500000002787978\n",
            "\n",
            "Iteration 4678 , Loss = 0.12500000002783618\n",
            "\n",
            "Iteration 4679 , Loss = 0.12500000002779282\n",
            "\n",
            "Iteration 4680 , Loss = 0.1250000000277497\n",
            "\n",
            "Iteration 4681 , Loss = 0.12500000002770678\n",
            "\n",
            "Iteration 4682 , Loss = 0.12500000002766407\n",
            "\n",
            "Iteration 4683 , Loss = 0.12500000002762157\n",
            "\n",
            "Iteration 4684 , Loss = 0.12500000002757927\n",
            "\n",
            "Iteration 4685 , Loss = 0.1250000000275372\n",
            "\n",
            "Iteration 4686 , Loss = 0.1250000000274954\n",
            "\n",
            "Iteration 4687 , Loss = 0.12500000002745373\n",
            "\n",
            "Iteration 4688 , Loss = 0.1250000000274123\n",
            "\n",
            "Iteration 4689 , Loss = 0.12500000002737108\n",
            "\n",
            "Iteration 4690 , Loss = 0.12500000002733005\n",
            "\n",
            "Iteration 4691 , Loss = 0.12500000002728923\n",
            "\n",
            "Iteration 4692 , Loss = 0.1250000000272486\n",
            "\n",
            "Iteration 4693 , Loss = 0.1250000000272082\n",
            "\n",
            "Iteration 4694 , Loss = 0.12500000002716802\n",
            "\n",
            "Iteration 4695 , Loss = 0.12500000002712802\n",
            "\n",
            "Iteration 4696 , Loss = 0.12500000002708822\n",
            "\n",
            "Iteration 4697 , Loss = 0.12500000002704864\n",
            "\n",
            "Iteration 4698 , Loss = 0.12500000002700926\n",
            "\n",
            "Iteration 4699 , Loss = 0.12500000002697004\n",
            "\n",
            "Iteration 4700 , Loss = 0.12500000002693104\n",
            "\n",
            "Iteration 4701 , Loss = 0.12500000002689227\n",
            "\n",
            "Iteration 4702 , Loss = 0.12500000002685366\n",
            "\n",
            "Iteration 4703 , Loss = 0.12500000002681524\n",
            "\n",
            "Iteration 4704 , Loss = 0.12500000002677703\n",
            "\n",
            "Iteration 4705 , Loss = 0.125000000026739\n",
            "\n",
            "Iteration 4706 , Loss = 0.1250000000267012\n",
            "\n",
            "Iteration 4707 , Loss = 0.12500000002666356\n",
            "\n",
            "Iteration 4708 , Loss = 0.1250000000266261\n",
            "\n",
            "Iteration 4709 , Loss = 0.12500000002658881\n",
            "\n",
            "Iteration 4710 , Loss = 0.12500000002655176\n",
            "\n",
            "Iteration 4711 , Loss = 0.1250000000265149\n",
            "\n",
            "Iteration 4712 , Loss = 0.12500000002647815\n",
            "\n",
            "Iteration 4713 , Loss = 0.12500000002644168\n",
            "\n",
            "Iteration 4714 , Loss = 0.12500000002640532\n",
            "\n",
            "Iteration 4715 , Loss = 0.12500000002636918\n",
            "\n",
            "Iteration 4716 , Loss = 0.1250000000263332\n",
            "\n",
            "Iteration 4717 , Loss = 0.12500000002629744\n",
            "\n",
            "Iteration 4718 , Loss = 0.12500000002626183\n",
            "\n",
            "Iteration 4719 , Loss = 0.1250000000262264\n",
            "\n",
            "Iteration 4720 , Loss = 0.12500000002619116\n",
            "\n",
            "Iteration 4721 , Loss = 0.1250000000261561\n",
            "\n",
            "Iteration 4722 , Loss = 0.12500000002612122\n",
            "\n",
            "Iteration 4723 , Loss = 0.1250000000260865\n",
            "\n",
            "Iteration 4724 , Loss = 0.12500000002605194\n",
            "\n",
            "Iteration 4725 , Loss = 0.12500000002601758\n",
            "\n",
            "Iteration 4726 , Loss = 0.12500000002598338\n",
            "\n",
            "Iteration 4727 , Loss = 0.12500000002594935\n",
            "\n",
            "Iteration 4728 , Loss = 0.12500000002591555\n",
            "\n",
            "Iteration 4729 , Loss = 0.12500000002588185\n",
            "\n",
            "Iteration 4730 , Loss = 0.12500000002584835\n",
            "\n",
            "Iteration 4731 , Loss = 0.12500000002581504\n",
            "\n",
            "Iteration 4732 , Loss = 0.12500000002578185\n",
            "\n",
            "Iteration 4733 , Loss = 0.12500000002574885\n",
            "\n",
            "Iteration 4734 , Loss = 0.12500000002571604\n",
            "\n",
            "Iteration 4735 , Loss = 0.12500000002568334\n",
            "\n",
            "Iteration 4736 , Loss = 0.12500000002565084\n",
            "\n",
            "Iteration 4737 , Loss = 0.1250000000256185\n",
            "\n",
            "Iteration 4738 , Loss = 0.12500000002558634\n",
            "\n",
            "Iteration 4739 , Loss = 0.12500000002555434\n",
            "\n",
            "Iteration 4740 , Loss = 0.12500000002552247\n",
            "\n",
            "Iteration 4741 , Loss = 0.1250000000254908\n",
            "\n",
            "Iteration 4742 , Loss = 0.12500000002545925\n",
            "\n",
            "Iteration 4743 , Loss = 0.12500000002542788\n",
            "\n",
            "Iteration 4744 , Loss = 0.12500000002539668\n",
            "\n",
            "Iteration 4745 , Loss = 0.12500000002536563\n",
            "\n",
            "Iteration 4746 , Loss = 0.12500000002533473\n",
            "\n",
            "Iteration 4747 , Loss = 0.12500000002530398\n",
            "\n",
            "Iteration 4748 , Loss = 0.12500000002527337\n",
            "\n",
            "Iteration 4749 , Loss = 0.12500000002524297\n",
            "\n",
            "Iteration 4750 , Loss = 0.1250000000252127\n",
            "\n",
            "Iteration 4751 , Loss = 0.12500000002518255\n",
            "\n",
            "Iteration 4752 , Loss = 0.12500000002515257\n",
            "\n",
            "Iteration 4753 , Loss = 0.1250000000251228\n",
            "\n",
            "Iteration 4754 , Loss = 0.1250000000250931\n",
            "\n",
            "Iteration 4755 , Loss = 0.1250000000250636\n",
            "\n",
            "Iteration 4756 , Loss = 0.12500000002503422\n",
            "\n",
            "Iteration 4757 , Loss = 0.125000000025005\n",
            "\n",
            "Iteration 4758 , Loss = 0.1250000000249759\n",
            "\n",
            "Iteration 4759 , Loss = 0.125000000024947\n",
            "\n",
            "Iteration 4760 , Loss = 0.1250000000249182\n",
            "\n",
            "Iteration 4761 , Loss = 0.12500000002488956\n",
            "\n",
            "Iteration 4762 , Loss = 0.12500000002486109\n",
            "\n",
            "Iteration 4763 , Loss = 0.12500000002483275\n",
            "\n",
            "Iteration 4764 , Loss = 0.12500000002480452\n",
            "\n",
            "Iteration 4765 , Loss = 0.12500000002477646\n",
            "\n",
            "Iteration 4766 , Loss = 0.1250000000247486\n",
            "\n",
            "Iteration 4767 , Loss = 0.12500000002472078\n",
            "\n",
            "Iteration 4768 , Loss = 0.12500000002469316\n",
            "\n",
            "Iteration 4769 , Loss = 0.12500000002466566\n",
            "\n",
            "Iteration 4770 , Loss = 0.1250000000246383\n",
            "\n",
            "Iteration 4771 , Loss = 0.12500000002461106\n",
            "\n",
            "Iteration 4772 , Loss = 0.125000000024584\n",
            "\n",
            "Iteration 4773 , Loss = 0.12500000002455705\n",
            "\n",
            "Iteration 4774 , Loss = 0.12500000002453024\n",
            "\n",
            "Iteration 4775 , Loss = 0.12500000002450357\n",
            "\n",
            "Iteration 4776 , Loss = 0.12500000002447703\n",
            "\n",
            "Iteration 4777 , Loss = 0.12500000002445064\n",
            "\n",
            "Iteration 4778 , Loss = 0.12500000002442435\n",
            "\n",
            "Iteration 4779 , Loss = 0.1250000000243982\n",
            "\n",
            "Iteration 4780 , Loss = 0.12500000002437223\n",
            "\n",
            "Iteration 4781 , Loss = 0.12500000002434633\n",
            "\n",
            "Iteration 4782 , Loss = 0.12500000002432063\n",
            "\n",
            "Iteration 4783 , Loss = 0.125000000024295\n",
            "\n",
            "Iteration 4784 , Loss = 0.12500000002426953\n",
            "\n",
            "Iteration 4785 , Loss = 0.12500000002424416\n",
            "\n",
            "Iteration 4786 , Loss = 0.12500000002421893\n",
            "\n",
            "Iteration 4787 , Loss = 0.12500000002419384\n",
            "\n",
            "Iteration 4788 , Loss = 0.12500000002416886\n",
            "\n",
            "Iteration 4789 , Loss = 0.12500000002414405\n",
            "\n",
            "Iteration 4790 , Loss = 0.12500000002411932\n",
            "\n",
            "Iteration 4791 , Loss = 0.12500000002409473\n",
            "\n",
            "Iteration 4792 , Loss = 0.12500000002407025\n",
            "\n",
            "Iteration 4793 , Loss = 0.1250000000240459\n",
            "\n",
            "Iteration 4794 , Loss = 0.12500000002402167\n",
            "\n",
            "Iteration 4795 , Loss = 0.12500000002399758\n",
            "\n",
            "Iteration 4796 , Loss = 0.12500000002397363\n",
            "\n",
            "Iteration 4797 , Loss = 0.12500000002394976\n",
            "\n",
            "Iteration 4798 , Loss = 0.12500000002392603\n",
            "\n",
            "Iteration 4799 , Loss = 0.12500000002390244\n",
            "\n",
            "Iteration 4800 , Loss = 0.12500000002387893\n",
            "\n",
            "Iteration 4801 , Loss = 0.12500000002385556\n",
            "\n",
            "Iteration 4802 , Loss = 0.12500000002383233\n",
            "\n",
            "Iteration 4803 , Loss = 0.12500000002380918\n",
            "\n",
            "Iteration 4804 , Loss = 0.12500000002378617\n",
            "\n",
            "Iteration 4805 , Loss = 0.12500000002376324\n",
            "\n",
            "Iteration 4806 , Loss = 0.12500000002374045\n",
            "\n",
            "Iteration 4807 , Loss = 0.12500000002371778\n",
            "\n",
            "Iteration 4808 , Loss = 0.12500000002369524\n",
            "\n",
            "Iteration 4809 , Loss = 0.1250000000236728\n",
            "\n",
            "Iteration 4810 , Loss = 0.12500000002365047\n",
            "\n",
            "Iteration 4811 , Loss = 0.12500000002362827\n",
            "\n",
            "Iteration 4812 , Loss = 0.12500000002360614\n",
            "\n",
            "Iteration 4813 , Loss = 0.1250000000235842\n",
            "\n",
            "Iteration 4814 , Loss = 0.12500000002356232\n",
            "\n",
            "Iteration 4815 , Loss = 0.1250000000235405\n",
            "\n",
            "Iteration 4816 , Loss = 0.12500000002351885\n",
            "\n",
            "Iteration 4817 , Loss = 0.12500000002349732\n",
            "\n",
            "Iteration 4818 , Loss = 0.1250000000234759\n",
            "\n",
            "Iteration 4819 , Loss = 0.12500000002345457\n",
            "\n",
            "Iteration 4820 , Loss = 0.1250000000234333\n",
            "\n",
            "Iteration 4821 , Loss = 0.12500000002341224\n",
            "\n",
            "Iteration 4822 , Loss = 0.1250000000233912\n",
            "\n",
            "Iteration 4823 , Loss = 0.12500000002337033\n",
            "\n",
            "Iteration 4824 , Loss = 0.12500000002334954\n",
            "\n",
            "Iteration 4825 , Loss = 0.12500000002332884\n",
            "\n",
            "Iteration 4826 , Loss = 0.12500000002330827\n",
            "\n",
            "Iteration 4827 , Loss = 0.1250000000232878\n",
            "\n",
            "Iteration 4828 , Loss = 0.12500000002326742\n",
            "\n",
            "Iteration 4829 , Loss = 0.12500000002324715\n",
            "\n",
            "Iteration 4830 , Loss = 0.12500000002322698\n",
            "\n",
            "Iteration 4831 , Loss = 0.12500000002320694\n",
            "\n",
            "Iteration 4832 , Loss = 0.12500000002318695\n",
            "\n",
            "Iteration 4833 , Loss = 0.12500000002316708\n",
            "\n",
            "Iteration 4834 , Loss = 0.12500000002314735\n",
            "\n",
            "Iteration 4835 , Loss = 0.12500000002312767\n",
            "\n",
            "Iteration 4836 , Loss = 0.1250000000231081\n",
            "\n",
            "Iteration 4837 , Loss = 0.12500000002308867\n",
            "\n",
            "Iteration 4838 , Loss = 0.1250000000230693\n",
            "\n",
            "Iteration 4839 , Loss = 0.12500000002305003\n",
            "\n",
            "Iteration 4840 , Loss = 0.12500000002303086\n",
            "\n",
            "Iteration 4841 , Loss = 0.12500000002301181\n",
            "\n",
            "Iteration 4842 , Loss = 0.12500000002299283\n",
            "\n",
            "Iteration 4843 , Loss = 0.12500000002297393\n",
            "\n",
            "Iteration 4844 , Loss = 0.12500000002295517\n",
            "\n",
            "Iteration 4845 , Loss = 0.1250000000229365\n",
            "\n",
            "Iteration 4846 , Loss = 0.12500000002291792\n",
            "\n",
            "Iteration 4847 , Loss = 0.1250000000228994\n",
            "\n",
            "Iteration 4848 , Loss = 0.12500000002288103\n",
            "\n",
            "Iteration 4849 , Loss = 0.1250000000228627\n",
            "\n",
            "Iteration 4850 , Loss = 0.1250000000228445\n",
            "\n",
            "Iteration 4851 , Loss = 0.12500000002282638\n",
            "\n",
            "Iteration 4852 , Loss = 0.12500000002280834\n",
            "\n",
            "Iteration 4853 , Loss = 0.12500000002279044\n",
            "\n",
            "Iteration 4854 , Loss = 0.12500000002277256\n",
            "\n",
            "Iteration 4855 , Loss = 0.12500000002275483\n",
            "\n",
            "Iteration 4856 , Loss = 0.12500000002273717\n",
            "\n",
            "Iteration 4857 , Loss = 0.1250000000227196\n",
            "\n",
            "Iteration 4858 , Loss = 0.12500000002270212\n",
            "\n",
            "Iteration 4859 , Loss = 0.12500000002268472\n",
            "\n",
            "Iteration 4860 , Loss = 0.12500000002266742\n",
            "\n",
            "Iteration 4861 , Loss = 0.12500000002265022\n",
            "\n",
            "Iteration 4862 , Loss = 0.12500000002263306\n",
            "\n",
            "Iteration 4863 , Loss = 0.12500000002261602\n",
            "\n",
            "Iteration 4864 , Loss = 0.1250000000225991\n",
            "\n",
            "Iteration 4865 , Loss = 0.1250000000225822\n",
            "\n",
            "Iteration 4866 , Loss = 0.12500000002256542\n",
            "\n",
            "Iteration 4867 , Loss = 0.12500000002254874\n",
            "\n",
            "Iteration 4868 , Loss = 0.12500000002253212\n",
            "\n",
            "Iteration 4869 , Loss = 0.12500000002251557\n",
            "\n",
            "Iteration 4870 , Loss = 0.12500000002249914\n",
            "\n",
            "Iteration 4871 , Loss = 0.1250000000224828\n",
            "\n",
            "Iteration 4872 , Loss = 0.1250000000224665\n",
            "\n",
            "Iteration 4873 , Loss = 0.12500000002245032\n",
            "\n",
            "Iteration 4874 , Loss = 0.1250000000224342\n",
            "\n",
            "Iteration 4875 , Loss = 0.12500000002241818\n",
            "\n",
            "Iteration 4876 , Loss = 0.12500000002240225\n",
            "\n",
            "Iteration 4877 , Loss = 0.12500000002238637\n",
            "\n",
            "Iteration 4878 , Loss = 0.12500000002237058\n",
            "\n",
            "Iteration 4879 , Loss = 0.12500000002235487\n",
            "\n",
            "Iteration 4880 , Loss = 0.12500000002233924\n",
            "\n",
            "Iteration 4881 , Loss = 0.1250000000223237\n",
            "\n",
            "Iteration 4882 , Loss = 0.12500000002230824\n",
            "\n",
            "Iteration 4883 , Loss = 0.12500000002229286\n",
            "\n",
            "Iteration 4884 , Loss = 0.12500000002227754\n",
            "\n",
            "Iteration 4885 , Loss = 0.12500000002226233\n",
            "\n",
            "Iteration 4886 , Loss = 0.12500000002224718\n",
            "\n",
            "Iteration 4887 , Loss = 0.1250000000222321\n",
            "\n",
            "Iteration 4888 , Loss = 0.1250000000222171\n",
            "\n",
            "Iteration 4889 , Loss = 0.12500000002220218\n",
            "\n",
            "Iteration 4890 , Loss = 0.12500000002218734\n",
            "\n",
            "Iteration 4891 , Loss = 0.12500000002217257\n",
            "\n",
            "Iteration 4892 , Loss = 0.12500000002215786\n",
            "\n",
            "Iteration 4893 , Loss = 0.12500000002214326\n",
            "\n",
            "Iteration 4894 , Loss = 0.12500000002212872\n",
            "\n",
            "Iteration 4895 , Loss = 0.12500000002211425\n",
            "\n",
            "Iteration 4896 , Loss = 0.12500000002209988\n",
            "\n",
            "Iteration 4897 , Loss = 0.12500000002208556\n",
            "\n",
            "Iteration 4898 , Loss = 0.1250000000220713\n",
            "\n",
            "Iteration 4899 , Loss = 0.1250000000220571\n",
            "\n",
            "Iteration 4900 , Loss = 0.125000000022043\n",
            "\n",
            "Iteration 4901 , Loss = 0.12500000002202896\n",
            "\n",
            "Iteration 4902 , Loss = 0.12500000002201503\n",
            "\n",
            "Iteration 4903 , Loss = 0.12500000002200112\n",
            "\n",
            "Iteration 4904 , Loss = 0.12500000002198733\n",
            "\n",
            "Iteration 4905 , Loss = 0.1250000000219736\n",
            "\n",
            "Iteration 4906 , Loss = 0.1250000000219599\n",
            "\n",
            "Iteration 4907 , Loss = 0.12500000002194628\n",
            "\n",
            "Iteration 4908 , Loss = 0.12500000002193276\n",
            "\n",
            "Iteration 4909 , Loss = 0.1250000000219193\n",
            "\n",
            "Iteration 4910 , Loss = 0.1250000000219059\n",
            "\n",
            "Iteration 4911 , Loss = 0.12500000002189254\n",
            "\n",
            "Iteration 4912 , Loss = 0.1250000000218793\n",
            "\n",
            "Iteration 4913 , Loss = 0.1250000000218661\n",
            "\n",
            "Iteration 4914 , Loss = 0.12500000002185296\n",
            "\n",
            "Iteration 4915 , Loss = 0.12500000002183992\n",
            "\n",
            "Iteration 4916 , Loss = 0.12500000002182696\n",
            "\n",
            "Iteration 4917 , Loss = 0.12500000002181402\n",
            "\n",
            "Iteration 4918 , Loss = 0.12500000002180117\n",
            "\n",
            "Iteration 4919 , Loss = 0.12500000002178838\n",
            "\n",
            "Iteration 4920 , Loss = 0.12500000002177564\n",
            "\n",
            "Iteration 4921 , Loss = 0.12500000002176298\n",
            "\n",
            "Iteration 4922 , Loss = 0.12500000002175038\n",
            "\n",
            "Iteration 4923 , Loss = 0.12500000002173786\n",
            "\n",
            "Iteration 4924 , Loss = 0.1250000000217254\n",
            "\n",
            "Iteration 4925 , Loss = 0.125000000021713\n",
            "\n",
            "Iteration 4926 , Loss = 0.12500000002170067\n",
            "\n",
            "Iteration 4927 , Loss = 0.12500000002168837\n",
            "\n",
            "Iteration 4928 , Loss = 0.12500000002167616\n",
            "\n",
            "Iteration 4929 , Loss = 0.125000000021664\n",
            "\n",
            "Iteration 4930 , Loss = 0.1250000000216519\n",
            "\n",
            "Iteration 4931 , Loss = 0.12500000002163988\n",
            "\n",
            "Iteration 4932 , Loss = 0.12500000002162792\n",
            "\n",
            "Iteration 4933 , Loss = 0.12500000002161601\n",
            "\n",
            "Iteration 4934 , Loss = 0.12500000002160416\n",
            "\n",
            "Iteration 4935 , Loss = 0.12500000002159237\n",
            "\n",
            "Iteration 4936 , Loss = 0.1250000000215807\n",
            "\n",
            "Iteration 4937 , Loss = 0.12500000002156902\n",
            "\n",
            "Iteration 4938 , Loss = 0.12500000002155742\n",
            "\n",
            "Iteration 4939 , Loss = 0.12500000002154588\n",
            "\n",
            "Iteration 4940 , Loss = 0.12500000002153439\n",
            "\n",
            "Iteration 4941 , Loss = 0.12500000002152298\n",
            "\n",
            "Iteration 4942 , Loss = 0.1250000000215116\n",
            "\n",
            "Iteration 4943 , Loss = 0.12500000002150027\n",
            "\n",
            "Iteration 4944 , Loss = 0.12500000002148906\n",
            "\n",
            "Iteration 4945 , Loss = 0.12500000002147788\n",
            "\n",
            "Iteration 4946 , Loss = 0.12500000002146672\n",
            "\n",
            "Iteration 4947 , Loss = 0.12500000002145567\n",
            "\n",
            "Iteration 4948 , Loss = 0.12500000002144462\n",
            "\n",
            "Iteration 4949 , Loss = 0.12500000002143366\n",
            "\n",
            "Iteration 4950 , Loss = 0.12500000002142278\n",
            "\n",
            "Iteration 4951 , Loss = 0.12500000002141193\n",
            "\n",
            "Iteration 4952 , Loss = 0.12500000002140113\n",
            "\n",
            "Iteration 4953 , Loss = 0.1250000000213904\n",
            "\n",
            "Iteration 4954 , Loss = 0.1250000000213797\n",
            "\n",
            "Iteration 4955 , Loss = 0.12500000002136907\n",
            "\n",
            "Iteration 4956 , Loss = 0.12500000002135853\n",
            "\n",
            "Iteration 4957 , Loss = 0.12500000002134798\n",
            "\n",
            "Iteration 4958 , Loss = 0.12500000002133752\n",
            "\n",
            "Iteration 4959 , Loss = 0.1250000000213271\n",
            "\n",
            "Iteration 4960 , Loss = 0.12500000002131678\n",
            "\n",
            "Iteration 4961 , Loss = 0.12500000002130648\n",
            "\n",
            "Iteration 4962 , Loss = 0.12500000002129621\n",
            "\n",
            "Iteration 4963 , Loss = 0.12500000002128603\n",
            "\n",
            "Iteration 4964 , Loss = 0.12500000002127584\n",
            "\n",
            "Iteration 4965 , Loss = 0.1250000000212658\n",
            "\n",
            "Iteration 4966 , Loss = 0.12500000002125575\n",
            "\n",
            "Iteration 4967 , Loss = 0.12500000002124578\n",
            "\n",
            "Iteration 4968 , Loss = 0.12500000002123582\n",
            "\n",
            "Iteration 4969 , Loss = 0.12500000002122594\n",
            "\n",
            "Iteration 4970 , Loss = 0.12500000002121608\n",
            "\n",
            "Iteration 4971 , Loss = 0.12500000002120631\n",
            "\n",
            "Iteration 4972 , Loss = 0.1250000000211966\n",
            "\n",
            "Iteration 4973 , Loss = 0.12500000002118689\n",
            "\n",
            "Iteration 4974 , Loss = 0.12500000002117725\n",
            "\n",
            "Iteration 4975 , Loss = 0.12500000002116768\n",
            "\n",
            "Iteration 4976 , Loss = 0.12500000002115816\n",
            "\n",
            "Iteration 4977 , Loss = 0.12500000002114867\n",
            "\n",
            "Iteration 4978 , Loss = 0.12500000002113923\n",
            "\n",
            "Iteration 4979 , Loss = 0.12500000002112985\n",
            "\n",
            "Iteration 4980 , Loss = 0.12500000002112052\n",
            "\n",
            "Iteration 4981 , Loss = 0.12500000002111122\n",
            "\n",
            "Iteration 4982 , Loss = 0.12500000002110198\n",
            "\n",
            "Iteration 4983 , Loss = 0.12500000002109277\n",
            "\n",
            "Iteration 4984 , Loss = 0.12500000002108366\n",
            "\n",
            "Iteration 4985 , Loss = 0.12500000002107453\n",
            "\n",
            "Iteration 4986 , Loss = 0.12500000002106548\n",
            "\n",
            "Iteration 4987 , Loss = 0.1250000000210565\n",
            "\n",
            "Iteration 4988 , Loss = 0.1250000000210475\n",
            "\n",
            "Iteration 4989 , Loss = 0.12500000002103862\n",
            "\n",
            "Iteration 4990 , Loss = 0.12500000002102973\n",
            "\n",
            "Iteration 4991 , Loss = 0.12500000002102093\n",
            "\n",
            "Iteration 4992 , Loss = 0.12500000002101216\n",
            "\n",
            "Iteration 4993 , Loss = 0.12500000002100342\n",
            "\n",
            "Iteration 4994 , Loss = 0.12500000002099476\n",
            "\n",
            "Iteration 4995 , Loss = 0.1250000000209861\n",
            "\n",
            "Iteration 4996 , Loss = 0.12500000002097753\n",
            "\n",
            "Iteration 4997 , Loss = 0.12500000002096898\n",
            "\n",
            "Iteration 4998 , Loss = 0.12500000002096046\n",
            "\n",
            "Iteration 4999 , Loss = 0.125000000020952\n",
            "\n",
            "Iteration 5000 , Loss = 0.12500000002094358\n",
            "\n"
          ]
        }
      ]
    }
  ]
}